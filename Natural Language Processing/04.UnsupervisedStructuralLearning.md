# Unsupervised Structural Learning

## NLP as Structural Learning

> “结构化，但是没那么结构化”

- Machine learning methods are better suited for tasks on less "structural" data.
  - E.g., predicting the label of single-granularity data.
- During training, we need to decompose NL data into basic units that can be processed by ML models.
- During inference, we need to **decode** the predicted basic units and convert them back to the structured form.

## A Simple Task: Word/Subword Segmentation

- **Segmentation over sequence.** A special case of tokenization in natural language processing for many languages with no explicit word delimiters.
  - **Word segmentation.** E.g., Chinese and many other Asian languages.
  - **Subword segmentation & Collocation detection.** E.g., English.

### Subword Segmentation

- Strength
  - Reduce the size of the vocabulary
  - Alleviate out-of-vocabulary (OOV) problems

### Segmentation Algorithm

A generalized framework for a segmentation algorithm contains roughly

1. A model
2. A decoder

#### Model

A word candidate list

$$ W = \{ (w_i, g(w_i)) \} $$

where

- $w_i$ is an n-gram.
- $g()$ is a "goodness" measure.
  - Higher $g()$ means the word $w_i$ is more "independent", i.e., more likely to be a word.

#### Decoder

- Viterbi Decoding
- Greedy Maximum Matching Decoding

##### Viterbi Decoding

##### Greedy Maximum Matching Decoding

- Start from the beginning of a sequence.
- Find a candidate that maximizes the goodness score.
- (For segmentation) Segment at the place with the highest score.

### Goodness Measure

- An unsupervised segmentation algorithm requires a pre-defined criteria.
- The **goodness scores** are used for evaluating how *independent* an n-gram is.

#### Frequency

$$ g_{FRQ}(w) = \mathrm{count}(w) $$

- Two improvements.
  - **The longest wins.** If one n-gram is a substring of another longer n-gram, and both have the same frequency, then the shorter one could be discarded.
    - **Intuition.** The two n-grams have the same frequency implies that the shorter one never appears independently.
  - **Filtering.**
    - **Blacklisting.** Remove stopwords.
    - **Whitelisting.** E.g., filter by part-of-speech patterns and only retain meaningful phrases.

#### Accessor Variety

$$ g_{AV}(w) = \min\{ L_{av}(w), R_{av}(w) \} $$

where the left and right accessor variety are, respectively, the number of distinct predecessor and successor n-grams.

- Usually the logarithm is used instead.

#### Branching (Boundary) Entropy

Higher entropy of candidate word boundary implies higher uncertainty of this candidate word combined with other elements, which means a fixed, stable collocation.

$$ BE(w) = \min\{ L_{be}(w), R_{be}(w) \} $$

Where $L_{be}, R_{be}$ are the left and right boundary entropy w.r.t. distinct predecessor and sucessor grams.

BE and AV scores are closely related to each other.

#### Description Length Gain (DLG)

#### Pointwise Mutual Information (PMI)

#### Student's T-Test
