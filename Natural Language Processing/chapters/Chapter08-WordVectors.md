# 词向量

## 词的表示

### 单热点词向量 One-hot

- 对 $N$ 个词的词库构建 $N$ 维的向量，其中第 $i$ 个词对应的向量第 $i$ 位为 $1$ 其余均为 $0$
- 简单直观
- 天然适合词袋模型

#### 词袋模型

- 对语句和短语的建模方式之一
- 只记录词的出现次数，忽略词的顺序
- 将语句中出现的每个词的词向量相加
  - 词袋模型中每一维度都是该词在该语句中出现的次数

#### 问题

- 维度爆炸
  - 有多少词就有多少维度
- 不能反应词与词之间的联系
  - 任意两个词的内积都是零，欧氏距离全部相等，不能反映词与词的语义关系

### 分布式词向量

- 具有聚类特性，能够根据距离反应词与词的关系
  - 可能是欧氏距离，也可能是余弦距离
- 每个维度代表了某种不同范畴的语义
- 构造相对复杂
- 但是稠密、紧凑、可以降维
- 词的语义分布再不同维度，故称为分布式词向量

## 词向量

### 基于共现频度的词向量

- 共现
  - 不同单词在同一语料或同一上下文中出现，称为“共现”
- 建立共现矩阵
  - 实对称矩阵
  - 每一行、每一列对应词表中的一个词
  - 若词 $i$ 和 $j$ 出现在同一句句子中，则 $[i,j]$ 和 $[j,i]$ 均 +1
- 共现矩阵较为稀疏
  - 尤其是一些生僻词可能不会共同出现
  - 未达到降维目的
    - 矩阵维度仍然是词表长度

#### 特征值分解降维

- 类似PCA
- 对共现矩阵做特征值分解
  - 使用特征向量作为词的分布式表达
  - 可以根据特征值大小舍弃特征值较小的维度

对于实对称矩阵 $\Sigma$

$$ \Sigma = UDU^T = \sum_i \lambda_i u_iu_i^T $$

- 每个特征向量 $u_i$ 都代表了用于重建 $\Sigma$ 的一种特征，对应的特征值 $\lambda_i$ 的绝对值反映了对重建的重要程度

##### 共现矩阵特征值分解的不足

- 统计时空开销很大
  - 简单存储需要词表大小平方规模的空间
  - 稀疏矩阵方法的插入、更新都会引入额外时间开销
- 可扩展性差
  - 词表每加入一个新的词都需要重新计算
- 特征值分解计算复杂度较高

> 但是对于数据量小、不太会更新的数据集（可能数据量不足以充分训练一个神经网络模型），则仍然可以考虑使用共现矩阵方法

### Continuous Bag of Words CBOW

- 利用统计模型从上下文预测当前词的条件概率，在这个过程中得到词向量

从生成式模型的角度，CBOW根据前后 $k$ 个词预测当前词的概率

$$ p(w_i) = \mathbb{P}[w_i|w_{i-k},\dots,w_{i-1},w_{i+1},\dots,w{i+k}] $$

$$ \mathbf{p} = f(e_{i-k},\dots,e_{i-1},e_{i+1},\dots,e_{i+k}) $$

$$ \mathcal{L} = -\sum_{e\in\mathcal{C}} \log p(w_i) $$

- $w_i$ 是词
- $e_i$ 是对应的词向量
- $\mathcal{C}$ 是语料库
- $f$ 是某个可学习的语言模型
- $\mathcal{L}$ 是损失函数

#### CBOW的神经网络实现

$$ \mathbf{p} = softmax(\mathbf{W}\mathbf{e}) $$

- $\mathbf{W}$ 是神经网络的权重矩阵
  - 行数是输出维度
  - 列数是输入维度
  - $\mathbf{e}$ 是上下文的词向量计算得到的输入

构建 $\mathbf{e}$ 通常有两种构建方式

- 拼接
  - $\mathbf{e} = [e_{i-k}^T,\dots,e_{i-1}^T,\dots]$
  - 代价是会增加输入维度
    - 进而导致模型参数过多
- 词袋
  - $ \mathbf{e} = \sum_{-k\le j \le k, j\neq 0} e_{i+j} $
  - 代价是损失了词的序列信息

#### 输出端优化

- 由于实际应用中涉及的词表很大，softmax中存在大量指数运算，计算开销大
- 有两种降低输出维度的方法
  - 分层柔性最大化 Hierarchical Softmax
  - 负采样 Negative Sampling
  - 都是将多分类任务转换成二分类任务

##### 分层柔性最大化

- 将多分类问题转换成多个二分类问题，组成二叉树

##### 负采样

- $\mathbf{W}=[\bm{w}_1, \dots, \bm{w}_n]^T$
- 每个输出维度都是 $\bm{w}_i^T\bm{e}$
  - 实质上可以看作选取内积相似度最高的词作为结果
- 可以将CBOW的多分类问题转换为正例和负例的二分类问题
  - 希望最大化正确分类的相似度 $\bm{w}_i^T\bm{e}$，最小化其他错误分类的相似度 $\bm{w}_j^T\bm{e}(j\neq i)$
- 使用Sigmoid函数预测二分类中的分类概率，则可以得到新的损失函数

$$ \mathcal{L} = -\sum_{e\in\mathcal{C}} \left[ \log\sigma(\bm{w}_i^T \bm{e}) - \sum_{j\neq i}\log\sigma(\bm{w}_j^T\bm{e}) \right] $$

- 当词表很大时，遍历词表中的负例的代价很大，因此需要采样

$$ p(w_i|C) = \frac{\alpha_i^{3/4}}{\sum_j\alpha_j^{3/4}} $$

- $\alpha_i$ 是词 $i$ 在词表出现的频率
- 采用小于1的幂可以适当增加低频词被采样的概率

### Skip-Gram

- 利用统计语言模型预测当前词的上下文，在此训练过程中得到当前词的词向量

$$ p_j(w_j) = \hat{P}_j(w_j|w_i) \quad -k \le j \le k, j \neq 0 $$

### Global Vectors GloVe

$$ \mathcal{L} = -\sum_{i=1}^n\sum_{j=1}^n f(x_{ij})(e_i^Te_j + b_i + b_j -\log x_{ij})^2 $$

- 式中 $x_{ij}$ 表示 $w_j$ 在 $w_i$ 的领域内出现的次数

$$ f(x) = \begin{cases}
  \left(\frac{x}{x_{TH}}\right)^{3/4} &\quad x < x_{TH}\\
  1 &\quad o.w.
\end{cases} $$

- $f$ 是用于平衡样本数量的权重函数，抑制过大的 $x_{ij}$，适当提升较小的 $x_{ij}$

#### GloVe 与 Skip-Gram

Skip-Gram的损失函数

$$ \mathcal{L} = -\sum_{e\in\mathcal{C}} \sum_{-k \le j \le k, j\neq 0}\log p_j(w_j) $$

若将 $w_i$ 的领域扩展到语料库全局，则

$$ \mathcal{L} = -\sum_{i=1}^n\sum_{j=1}^n x_{ij}\log\hat{P}(w_j|w_i) $$

Skip-gram中，

#### GloVe的直觉

词义的相关性会更好地反映在共现概率的差异上

- 如果两个词相似，那么和它们共现的词应该也比较相似
- 反之如果两个词差异很大，那么和它们共现的词差异也应该较大

## 动态词向量

### 多义词与语境

- 静态词向量
- 动态词向量
  - 使用模型根据上下文

## 文本分类

### 任务定义

- 输入
  - 句子、段落、文章等不同规模的文本
- 输出
  - 分类标签
  - 有些任务的标签体系是分层的，例如情感、主题等，大类下还可以细分不同小类

### 句子段落分类

- 情感分类
- 意图识别

#### 情感分类

- 极性分析
  - 用于判断一句话或一段言论的情感极性
  - 用于分析、筛选用户评论、反馈以及网络舆论等
- 输入
  - 用户的评论或社交媒体发言，通常比较短，只包含若干句话
- 输出
  - 正负双极性分类，或正负中三级性分类

##### 简单模型

- 词袋模型
  - 统计正负极性关键词
- 更精细的模型
  - 建模文本的其他句式、结构特征，例如否定词、连接词、转折、让步、假设、虚拟语气
- 切合领域的极性词库与语料
  - 同一个词在不同领域、上下文中的极性可能不同

### 篇章分类

- 主题分类
- 文体分类

#### 主题分类

- 简单模型
  - 如果存在恰当的主题词表，则可以构建词袋模型
- 更精细的模型
  - 文章不是单一的，而可能涉及多个主题
  - 每个主题下的用词不一定互斥，而是有不同的概率分布
  - 采用概率模型进行更精细的分类

##### 概率潜在语义分析 Probablistic Latent Semantic Analysis pLSA

$$ P(w|d) = \sum_z P{w|d,z} = \sum_zP(z|d)P(w|z) $$

- $w$ 是一个词
- $d$ 是当前文档
- $z$ 是表示某个主题的隐变量

##### 隐狄利克雷分布 Latent Dirichlet Allocation LDA

### 匹配分类

- 文本匹配
  - 输入
    - 两段文本内容
  - 输出
    - 两段文本是否匹配
    - 或者两者的相似度度量

#### 应用

- 问答系统
- 机器阅读理解
- 文书、工单检索

#### 匹配深度

- 字面匹配
  - 字符串匹配、TF-IDF、词向量
- 主题匹配
  - 词向量、语言模型、主题模型
- 实体、事件匹配
  - 命名实体识别、事件检测技术
