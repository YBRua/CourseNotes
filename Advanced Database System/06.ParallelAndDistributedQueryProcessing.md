# Parallel and Distributed Query Processing

## Concurrency Control in Distributed Databases

- Assume each node participates in the execution of a commit protocol.
- Assume data items are not replicated.
- Do not consider multiversion techniques.

### Locking Protocols

#### The Single Lock-Manager Approach

The system maintains a *single* lock manager in a *single* chosen node $N_i$.

- When a transaction needs to lock a data item, it sends a lock request to $N_i$.
- The transaction could then
  - read the data item from any node having the a replica.
  - write the data item, in which case all nodes where a replica resides must be involved.
- Advantages
  - **Simple implementation.** Requires two messages for locking and one message for unlocking.
  - **Simple deadlock handling.** All methods discussed in a centralized system apply.
- Disadvantages
  - **Bottleneck.** $N_i$ becomes a bottleneck since all locking/unlocking requests are handled at this node.
  - **Vulnerability.** If $N_i$ fails, the entire concurrency controller would be lost.
    - Either the processing has to stop or a backup node needs to be selected to take over lock management

#### Distributed Lock Manager

> "各管一摊子。"

The lock manager function is distributed across mulitple nodes.

- Each node maintains a local lock manager for handling the locking/unlocking of data items stored on this node.

### Deadlock Handling

#### Deadlock Prevention

- Deadlock prevention based on lock ordering can be used in a distributed system without modifications.
- Techniques based on preemption and transaction rollback can be used in a distributed system without modifications.
- Techniques based on timeouts also works without further modifications.

#### Deadlock Detection

- The main problem is to maintain the wait-for graph.
- Common techniques for deadlock detection usually requires that each node keeps a **local wait-for graph**.
  - The nodes corresponds to any transactions (local and nonlocal) currently holding or requesting any data items local to this node.

#### Leases

- If a node holding a lock fails, it will not release the nodel, leaving the data item logically inaccessible.
- A **lease** is a lock that is granted for some time.
- If a transaction hopes to continuely hoding the lock beyond the specified period, 

#### Distributed Timestamp-based Methods

People slept啊！

#### Distributed Validation

We now consider how to adapt the validation-based protocol to distributed setting.

- Validation is done locally at each node.
- The validation timestamp can be assigned at any of the nodes, but it must be used at all nodes at which the validation is performed.
- Note that the centralized validation assumes that once a particular transaction enters the validation phase, no transactions with lower timestamps can enter the validation phase.
  - This cannot be assured in the distributed setting.
  - Can be fixed by rolling back $T_j$ if $T_j$ has an earlier timestamp but enters validation phase later.
- When used in conjunction with the 2PC protocol, a transaction must first be validated before enter the prepared phase.
  - Write must be delayed until the transaction enters the committed state.

!!!example Delayed read in distributed validation
    Suppose $T_j$ reads a data item updated by $T_i$ that is in the prepared state.

    - $T_j$ proceeds with the old value (since the value updated by $T_i$ has not yet been written to the database).
    - If $T_j$ attempts to validate, it will be serialized after $T_i$.
    - $T_j$ will surely fail if $T_i$ commits, because it read an old value.
    - Thus, $T_j$ may as well be delayed until $T_i$ commits and writes the database.

Note that full implementations of the validation-based protocol is not widely used in distributed systems. Optimistic concurrency control without read validation is widely used in distributed settings.

## Extended Concurrency Control Protocols

### Multiversion 2PL + Globally Consistent Timestamps

### Distributed Snapshot Isolation

- Independent snapshot isolation does not guarantee global isolation. If each node implements snapshot isolation independently, the resultant schedules can have anormalies that cannot occur in a centralized system.

!!!example Anornaly of local snapshot isolation
    Assume $T_1$ and $T_2$ executes concurrently on $N_1$, where $T_1$ writes $x$ and $T_2$ reads $x$. $T_2$ cannot read the updates of $T_1$ due to isolation.

    However, if $T_1$ performs update on $N_2$ and commits, $T_2$ can read $y$ and see the update of $T_1$.

    As a result, $T_2$ only reads part of the updates of $T_1$.

### Federated Database Systems

Transactions in a federated database systems can be classified as

- **Local transactions.** These transactions are executed by each local database system outside of the federated database system's control.
- **GLobal transactions.** These transactions are executed under the control of the federated database system's control.

In a FDBS, local serializability is not sufficient to ensure global serializability.

## Replications

- One of the goals of distributed DBS is **high availability**: the database must function almost all the time, even when there are various types of failures.
  - The ability to continue functioning even during failures is referred to as **robustness**.
- For a distributed DB to be robust, data must be *replicated*.
  - The locations of replica are tracked in the catalog.

### Consistency of Replicas

- The system should ideally ensure that the copies have the same value.
- However, practcally, it is impossible to ensure that all copies have the same value due to potential failures.
- The system therefore must ensure that even if some replicas do not have the latest value, reads of an item get to see the latest value.
- The implementations of read and write operations on the replicas of an item must follow a protocol that ensures **linearizability**: given a set of read and write operations on an item
  1. dd

### Concurrency Control with Replicas

- Assume the update are done on all replicas of a data item.
- If any node containing a replica fails or is disconnected, the replica cannot be updated.
  - We only need to ensure the replicas will *eventually* be consistent.

#### Primary Replica

- Choose one of the replicas as the **primary copy**.
- For each item $Q$, the primary copy of $Q$ must reside in exactly one node, called the **primary node** of $Q$.
- When a transaction needs to lock an item $Q$, it request a lock at the primary node of $Q$.

!!!note
    In a distributed system, we want all algorithms to be de-centralized. I.e., they work even if some nodes fail.

#### Majority Protocol

- If a data item $Q$ is replicated on $n$ nodes, the lock request must be sent to at least half of the $n$ nodes.
- The transaction does not operate on $Q$ until it obtains a lock on the majority of the replicas of $Q$.

#### Biased Protocol

- Requests for shared locks are given more favorable treatment than requests for exclusive locks.
  - **Shared locks.** It only needs to request a lock at one node containing a replica of $Q$.
  - **Exclusive locks.** It needs to request a lock at all nodes containing the replica of $Q$.

#### Quorum Consensus Protocol

The quorum consensus protocol is a generalization of the majority protocol.

- It assigns each node a nonnegative weight.
- It assigns each item $x$ two integers, a **read quorum** $Q_r$ and a **write quorum** $Q_w$ such that
  - $Q_r + Q_w > S$ and $2 * Q_w > S$
  - where $S$ is the total weight of all nodes at which $x$ resides.
- To execute a read/write operation, enough nodes must be locked s.t. the total weight is at least $Q_r$/$Q_w$.

### Dealing with Failures

#### Reconfiguration and Reintegration

In some cases a node may fail permanently, and the system must then be **reconfigured** to remove failed nodes.

If a failed node that is removed from the system eventually recovers, it must be **reintegrated** into the system.
