# Parallel and Distributed Query Processing

## Concurrency Control in Distributed Databases

- Assume each node participates in the execution of a commit protocol.
- Assume data items are not replicated.
- Do not consider multiversion techniques.

### Locking Protocols

#### The Single Lock-Manager Approach

The system maintains a *single* lock manager in a *single* chosen node $N_i$.

- When a transaction needs to lock a data item, it sends a lock request to $N_i$.
- The transaction could then
  - read the data item from any node having the a replica.
  - write the data item, in which case all nodes where a replica resides must be involved.
- Advantages
  - **Simple implementation.** Requires two messages for locking and one message for unlocking.
  - **Simple deadlock handling.** All methods discussed in a centralized system apply.
- Disadvantages
  - **Bottleneck.** $N_i$ becomes a bottleneck since all locking/unlocking requests are handled at this node.
  - **Vulnerability.** If $N_i$ fails, the entire concurrency controller would be lost.
    - Either the processing has to stop or a backup node needs to be selected to take over lock management

#### Distributed Lock Manager

> "各管一摊子。"

The lock manager function is distributed across mulitple nodes.

- Each node maintains a local lock manager for handling the locking/unlocking of data items stored on this node.

### Deadlock Handling

#### Deadlock Prevention

- Deadlock prevention based on lock ordering can be used in a distributed system without modifications.
- Techniques based on preemption and transaction rollback can be used in a distributed system without modifications.
- Techniques based on timeouts also works without further modifications.

#### Deadlock Detection

- The main problem is to maintain the wait-for graph.
- Common techniques for deadlock detection usually requires that each node keeps a **local wait-for graph**.
  - The nodes corresponds to any transactions (local and nonlocal) currently holding or requesting any data items local to this node.

#### Leases

- If a node holding a lock fails, it will not release the node, leaving the data item logically inaccessible.
- A **lease** is a lock that is granted for some time.
- If a transaction hopes to continuely hoding the lock beyond the specified period, 

#### Distributed Timestamp-based Methods

People slept啊！

#### Distributed Validation

We now consider how to adapt the validation-based protocol to distributed setting.

- Validation is done locally at each node.
- The validation timestamp can be assigned at any of the nodes, but it must be used at all nodes at which the validation is performed.
- Note that the centralized validation assumes that once a particular transaction enters the validation phase, no transactions with lower timestamps can enter the validation phase.
  - This cannot be assured in the distributed setting.
  - Can be fixed by rolling back $T_j$ if $T_j$ has an earlier timestamp but enters validation phase later.
- When used in conjunction with the 2PC protocol, a transaction must first be validated before enter the prepared phase.
  - Write must be delayed until the transaction enters the committed state.

!!!example Delayed read in distributed validation
    Suppose $T_j$ reads a data item updated by $T_i$ that is in the prepared state.

    - $T_j$ proceeds with the old value (since the value updated by $T_i$ has not yet been written to the database).
    - If $T_j$ attempts to validate, it will be serialized after $T_i$.
    - $T_j$ will surely fail if $T_i$ commits, because it read an old value.
    - Thus, $T_j$ may as well be delayed until $T_i$ commits and writes the database.

Note that full implementations of the validation-based protocol is not widely used in distributed systems. Optimistic concurrency control without read validation is widely used in distributed settings.

## Extended Concurrency Control Protocols

### Multiversion 2PL + Globally Consistent Timestamps

### Distributed Snapshot Isolation

- Independent snapshot isolation does not guarantee global isolation. If each node implements snapshot isolation independently, the resultant schedules can have anormalies that cannot occur in a centralized system.

!!!example Anornaly of local snapshot isolation
    Assume $T_1$ and $T_2$ executes concurrently on $N_1$, where $T_1$ writes $x$ and $T_2$ reads $x$. $T_2$ cannot read the updates of $T_1$ due to isolation.

    However, if $T_1$ performs update on $N_2$ and commits, $T_2$ can read $y$ and see the update of $T_1$.

    As a result, $T_2$ only reads part of the updates of $T_1$.

### Federated Database Systems

Transactions in a federated database systems can be classified as

- **Local transactions.** These transactions are executed by each local database system outside of the federated database system's control.
- **GLobal transactions.** These transactions are executed under the control of the federated database system's control.

In a FDBS, local serializability is not sufficient to ensure global serializability.

## Replications

- One of the goals of distributed DBS is **high availability**: the database must function almost all the time, even when there are various types of failures.
  - The ability to continue functioning even during failures is referred to as **robustness**.
- For a distributed DB to be robust, data must be *replicated*.
  - The locations of replica are tracked in the catalog.

### Consistency of Replicas

- The system should ideally ensure that the copies have the same value.
- However, practcally, it is impossible to ensure that all copies have the same value due to potential failures.
- The system therefore must ensure that even if some replicas do not have the latest value, reads of an item get to see the latest value.
- The implementations of read and write operations on the replicas of an item must follow a protocol that ensures **linearizability**: given a set of read and write operations on an item
  1. dd

### Concurrency Control with Replicas

- Assume the update are done on all replicas of a data item.
- If any node containing a replica fails or is disconnected, the replica cannot be updated.
  - We only need to ensure the replicas will *eventually* be consistent.

#### Primary Replica

- Choose one of the replicas as the **primary copy**.
- For each item $Q$, the primary copy of $Q$ must reside in exactly one node, called the **primary node** of $Q$.
- When a transaction needs to lock an item $Q$, it request a lock at the primary node of $Q$.

!!!note
    In a distributed system, we want all algorithms to be de-centralized. I.e., they work even if some nodes fail.

#### Majority Protocol

- If a data item $Q$ is replicated on $n$ nodes, the lock request must be sent to at least half of the $n$ nodes.
- The transaction does not operate on $Q$ until it obtains a lock on the majority of the replicas of $Q$.

#### Biased Protocol

- Requests for shared locks are given more favorable treatment than requests for exclusive locks.
  - **Shared locks.** It only needs to request a lock at one node containing a replica of $Q$.
  - **Exclusive locks.** It needs to request a lock at all nodes containing the replica of $Q$.

#### Quorum Consensus Protocol

The quorum consensus protocol is a generalization of the majority protocol.

- It assigns each node a nonnegative weight.
- It assigns each item $x$ two integers, a **read quorum** $Q_r$ and a **write quorum** $Q_w$ such that
  - $Q_r + Q_w > S$ and $2 * Q_w > S$
  - where $S$ is the total weight of all nodes at which $x$ resides.
- To execute a read/write operation, enough nodes must be locked s.t. the total weight is at least $Q_r$/$Q_w$.

### Dealing with Failures

#### Reconfiguration and Reintegration

In some cases a node may fail permanently, and the system must then be **reconfigured** to remove failed nodes.

If a failed node that is removed from the system eventually recovers, it must be **reintegrated** into the system.

## Replication with Weak Degrees of Consistency

### The CAP Theorem

The **CAP Theorem** states that any distributed database can have at most two of the following three properties

1. Consistency.
2. Availability. 
3. Partition-tolerance.

With replicated data, a set of operations (reads and writes) on replicated data is **consistent** if its results is the same as if the operations were executed on a single node.

In any large-scale distributed system, partitions cannot be prevented, and therefore either consistency or availability has to be sacrificed.

### The BASE Property

The **BASE** properties

- **Basicially available.** The primary requirement is availability, at the cost of consistency.
- **Soft state.** The state of the database may not be precisely defined, with each replica possibly having a different value due to partitioning.
- **Eventually consistent.** Inconsistent copies should be eventually identified and updated.

### Asynchronous Replication

- With asynchronous replication, the database allows updates at a **primary** / **master** node and propagate updates to replicas at other nodes.
- The transaction that performs the update can commit once the update is performed at the primary node, before replicas are updated.
  - Propagation of updates after commit is also known as **lazy propagation**.
  - In contrast, **synchronous replication** refers to the case where updates are propagated to replicas as part of a transaction.

#### Asynchronous View Maintenance

- Views and indicies can also be treated as replicas.

### Detecting Inconsistent Updates

#### Version-Vector Scheme

- Each node $i$ stores a **version vector**, which is a set of version numbers $\{ V[j] \}$ associated with a data item.
  - Whenever node $i$ updates data item $d$, it increments $V_d[i]$ by 1.
- When two nodes $i$ and $j$ connect with each other, they discover inconsistencies and exchange updates.
  - If $V_i[k] = V_j[k]$ for all $k$, then the two copies of data item $d$ are identical.
  - If $V_i[k] \le V_j[k]$ (or vice versa) for all $k$, then the data item at $i$ is older. $i$ can replace its data and version vector with those from $j$.
  - If there exist $k$ and $m$ s.t. $V_i[k] < V_j[k]$ and $V_i[m] > V_j[m]$, then the data item is inconsistent. Manual intervention might be required in this case.

## Coordinator Selection

### Backup coordinator

### Election

#### The Bully Algorithm

## Consensus in Distributed Systems

The most basic form of the **distributed consensus problem**: a set of $n$ nodes (participants) needs to agree on a decision by executing a protocol that

- All participants must "learn" the same value for the decision, even if some nodes fail, or messages are lost, or there are partitions.
- The protocol should not block, and must terminate, as long as the majority of participants remain alive and can communicate with each other.

### The Paxos Concensus Protocol

The basic Paxos protocol for making a single decision has the following participants,

1. **Proposers.** One or more nodes that can propose a value for the decision.
2. **Acceptors.** An acceptor might get proposals from different proposers and must choose one of the values.
3. **Learners.** Query an acceptor to find what value each acceptor voted for in a particular round.
   - The acceptors could also send the values they accepted to the learners.

The algorithm should avoid different values getting majorities in different proposal rounds. To do so, the Paxos algorithm use the following steps

1. Each proposal in Paxos has a number; different proposals have different numbers.
2. A proposer sends a *prepare* message to acceptors, with proposal number $n$.
3. An acceptor receiving the *prepare* message checks if it has already responded to some proposal $m > n$.
   1. If so, it ignores proposal $n$ (i.e., it never accepts any proposal $n < m$ if it has participated in $m$).
   2. Otherwise, it remembers $n$ and returns the maximum $m < n$ it has accepted, along with the proposed value $v$ of $m$ (if any).
4. The proposer checks if it receives response from a majority of the acceptors. If it does, it chooses a value by
   1. If none of the acceptors have accepted any value, it can choose any value.
   2. Otherwise, it chooses $v$ with the largest associated $m$.
5. When an acceptor receives the proposed value $v$ for $n$, it checks if it has responded to any $n_1 > n$
   1. If so, it ignores the request.
   2. Otherwise it accepts the value $v$ with number $n$.

### The Raft Consensus Protocol

The raft protocol is based on log replication. It allow each node to run a state machine with log entries used as commands to the state machine.

- The raft algorithm has a coordinator, called the **leader**.
- The other participating nodes are called the **followers**.
- Time is divided into **terms**, identified by an integer. Later terms have higher integer.
- Each term has a unique leader, although some terms might not have any associated leader.
