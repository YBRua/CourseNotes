# 图神经网络及应用

## 图的基本概念

- 图 $G = (V, E)$
  - $V$ 是节点集合
  - $X \in \mathbb{R}^{m\times|V|}$ 是特征矩阵，每一列对应一个节点的 $m$ 维特征向量
  - $E$ 是邻接矩阵
  - $N(v_i)$ 是节点 $v_i$ 的邻居节点，$N_+(v_i) = N(v_i) \cup \{ v_i \}$

## 图神经网络的基本架构

> 核心思想：以每个节点 $v_i$ 的特征 $h_0 = X_i$ 作为初始表示，然后将 $h_i^k$ 更新为 $h_i^{k+1}$

### 图神经网络的节点表示更新

1. 从邻居节点 **聚合** 信息
   - $m_i^k = f(\{W^k_1h_j^{k-1} | v_j \in N(v_i)\})$
   - $f$ 为 **聚合函数**
2. **更新** 节点表示
   - $h_i^k = g(h_i^{k-1}, m_i^k)$

### 经典图神经网络

> 来自各个节点的特征均等对待

- $ m_i^k = f() = \sum_{v_j \in N(v_i)}\frac{W_1^kh_j^{k-1}}{|N(v_i)|} $
- $h_i^k = g() = \sigma(W_0^kh_i^{k-1} + m_i^k)$

### 图卷积神经网络

> 一些节点的信息比另一些节点的信息更重要

1. $m_i^k = f(\{ W_1^k h_j^{k-1} | v_j \in N_+(v_i) \}) = \sum_{v_j} \frac{W_1^kh_j^{k-1}}{\sqrt{|N(v_i)||N(v_j)|}}$
2. $h_i^k = m_i^k$

- $|N(v_j)|$ 越大，来自 $v_j$ 的信息权重越低
- 思想是需要给邻居少的节点更多权重（直观上是假设稀疏的点的重要性较高）

### 图注意力神经网络

> 用注意力建模节点信息的重要性

- $m_i^k = \sum_{v_j \in N_+(v_i)} \alpha_{ij} \cdot (W_1^k h_j^{k-1})$
- $\alpha_{ij} = softmax_{v_j}(sim(h_j^{k-1}, h_i^{k-1}))$
  - $\alpha_{ij}$ 是一个标量权重
  - $sim()$ 可以是
  - $(W_{\alpha 1}^kh_{j}^{k-1})^T (W_{\alpha 2}^kh_i^{k-1})$
  - $FFN(cat(h_j, h_i))$

### 多维图注意力网络

> 不仅不同节点的消息的重要性不同，不同维度的消息的重要性也不同

类似图注意力网络，但是 $\alpha_{ij}$ 是向量，且与 $W_1^k h_j^{k-1}$ 做元素间乘法

- $sim(h_i, h_j) = FFN(h_j) + (W_1 h_j)^T(W_2 h_i)$
  - 前者用于估计不同维度的重要性，后者用于衡量节点消息的重要性

### 高阶图神经网络

普通图神经网络的更新公式

$$ h_i^k = \sigma(W_0^k h_i^{k-1} + W_1^k \bar{h}_i^k) $$

高阶图神经网络增加了不同阶数的邻居的信息

$$ h_i^k = \sigma(W_0^k h_i^{k-1} + W_1^k \bar{h}{i,1}^k + W_2^k \bar{h}_{i,2}^k + \cdots) $$

- 通常阶数不会很高

### 关系图神经网络

将节点按一些关系 $R$ 划分，分别按 $R$ 聚合信息

$$ h_i^k = \sigma( W_0^k h_i^{k-1} + \sum_{r \in R} W_r^k \bar{h}_{i,r}^k ) $$

- 在 $R$ 较多时会导致参数过多，因此通常使用参数分解的方式

$$ W_r^k = \sum_{b=1}^B a_{r,b} V_b $$

- 使用 $B$ 个参数矩阵 $V_b$ 的不同线性组合表示不同参数矩阵
- 能够使参数减少到原来的 $\frac{d^2 | B \times d^2 + |R| \times B}{d^2 + |R| \times d^2}$
