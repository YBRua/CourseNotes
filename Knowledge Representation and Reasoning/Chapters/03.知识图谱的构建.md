# 知识图谱的构建

## 命名实体识别

- 从文本中识别出代表实体的边界，并进一步判断其类别
- 关系抽取、事件抽取任务的基础

### 方法

#### 基于规则

- 基于文本与规则进行匹配
- 准确；且有些命名实体只能依靠规则抽取；适用于半结构化数据
- 需要大量语言学知识；需要处理规则的冲突；可维护性差、可移植性不强

#### 基于统计模型

- 建模为序列标注任务

##### BIO标签体系

- Begin：命名实体开头
- Inside：命名实体中间
- Outside：与命名实体无关

##### BIOES标签体系

- End：命名实体结尾
- Singleton：单个token作为一个命名实体

#### 基于神经网络模型

- 通过 BiLSTM / CNN / BERT 等模型（通常是神经网络编码器）生成用于实体识别的句子特征
- 通过条件随机场、Softmax等解码器生成序列标注标签

### 词汇增强

> 主要针对中文命名实体识别。经验上中文命名实体识别中，用字作为输入表现得比用词作为输入更好，但是中文中“词”才是基本的语义单元，故词汇增强用于将词的信息“融入”字中。

#### 中文命名实体识别

- 边界较难判别
- 嵌套实体较多
  - 复旦大学附属第五人民医院
- 用字变化多，同一实体在不同语境可能是不同类型

#### Lattice LSTM

- 通过词汇表和句子匹配得到 word-character lattice （句子中词汇和字符的对应关系）
- 编码字符时，额外融合来自不同路径的信息

##### 缺陷

> “我觉得大家可能一天都调不出来”

- 实现复杂
- 计算性能低下，较难并行
- 每个字符只能获得以该字符结尾的词汇的信息

#### Collaborative Graph Network CGN

1. 分别编码字和词
2. 输入 *图编码层* 进一步编码
3. 输入 Fushion 层
4. 经解码器输出

##### 图编码层

图编码层定义了不同的 Lattice 连接模式

1. 字符之间无连接，词与其内部的字有链接
2. 相邻字符相连接，词与其前后的字有连接
3. 相邻字符相连接，词与其开头结尾的字有连接

#### FLAT

- 在注意力计算中加入相对位置编码的偏置

相对位置的计算方法基于字词的相对位置

- 在输入时（一般情况下）可以抛弃绝对位置编码
  - 位置编码被替换为 head 和 tail 两个编码
  - head是当前字（或词）在句中的起始位置
  - tail是当前字（或词）在句中的结尾位置
- 根据起始终止位置的相对位置选择对应的相对位置向量

### 嵌套实体识别

- 嵌套的命名实体难以通过序列标注建模

#### Multi-Grained Named Entity Recognition MGNER

- 将命名实体的 **识别** 与 **分类** 分开处理

##### 识别网络

- 假设命名实体的有一个最大长度 $L$ （论文中为 6）
- 预定义了一些长度不超过 $L$ 的 Pattern （类似滑窗？）
- 对每个 Proposal，计算分数判断其是否可能是一个命名实体
- 如果是，则再进一步分类

输入是词嵌入、词性嵌入和字符嵌入（缓解OOV）

### 生成式命名实体建模

#### 基于模板的生成式命名实体识别

- 根据模板构建正负训练样例
- 模板为人工构建
- 推断时，给定输入句子，将给定模板下句子的生成概率作为分数，选择分数最高的

#### 基于拷贝机制的生成式命名实体识别

- 解码时复制输入中的字符作为输出
- 基于BART架构

## 实体消歧

### 基于聚类的实体消歧

在没有知识库的情况下，可以使用聚类

1. 对每个实体指称，构建对应的表征向量
   - 基于TF-IDF
   - 基于扩展特征
   - 基于社会化网络
   - 基于词向量（BERT）
2. 计算实体指称项之间的相似度
3. 采用某种聚类算法对实体指称聚类

### 基于实体链接的实体消歧

- 将某个实体链接到已有知识库中的特定实体
- 分为链接候选过滤和实体链接两步

#### 链接候选过滤

- 根据规则或知识过滤掉大部分指称项不能能指向的实体
  - 通常基于实体指称项词典进行过滤（例如 `交大 -> 上海交通大学，西安交通大学，...`）
  - 可以根据 Wikipedia 等知识资源来构建实体指称项词典

#### 实体链接

##### 向量空间模型

- 将实体指称项上下文与候选实体上下文中特征的共现信息作为两者的一致性分数（TF-IDF或神经网络）
- 本质上是两个句子的相似度匹配

## 关系抽取

### 基本定义

- 关系抽取是从文本中抽取两个或多个实体之间的语义关系
- 是从文本中获取知识图谱三元组的主要技术手段
- 分为封闭领域和开放领域关系抽取
  - 封闭域关系抽取任务中，事先知道所有关系

### 网络文本信息结构

#### 结构化数据

- 置信度高
- 规模小
- 缺乏个性化的属性信息

#### 半结构化数据

- 置信度较高
- 规模较大
- 包含个性化信息、形式多样，但是包含噪声

#### 纯文本

- 置信度低
- 规模大
- 复杂多样

### 纯文本关系抽取

#### 基于模板的方法

##### 基于触发词

- X 妻子 Y

##### 基于依存句法分析

- 以动词为起点构建规则
- 对节点上的词性和边上的依存关系进行限定
- 除了匹配到触发词之外，触发词前后词的词性也需要符合要求
  - 精度较高，但是召回率可能相对低一些

##### 模板获取

- 人工定义
- 自动学习
  - 对实体和模板进行联合迭代式地交替抽取和学习
  - 利用实体对在文本中获得模板信息
  - 再用得到的新模板信息抽取更多的实体对

#### 基于机器学习的方法

将关系抽取建模为分类问题，利用机器学习算法解决

##### 基于特征工程的方法

###### 常见的关系抽取特征

- 词汇特征
- 实体属性特征
- 重叠特征
- 依存句法特征
- 句法树特征

##### 基于递归神经网络的关系抽取

Recursive Neural Network, not Recurrent Neural Network

但是目前在实际中用得比较少

###### 递归神经网络

- 具有树状阶层结构，且网络节点按其连接顺序对输入信息进行递归计算
- 可用于学习自然语言处理任务中不同的序列结构和树状结构
- 每个节点由一个向量和一个矩阵组成
- 对包含两个实体的句法树的最小子树进行递归，并使用该子树的根节点的向量进行分类

###### RNN递归计算

- 每个节点由一个向量和一个矩阵组成
  - 向量是本身的词汇语义
    - 可以使用词本身的词向量进行初始化
  - 矩阵式该词对相邻词的作用
    - 可以用高斯函数初始化
- 自底向上递归计算
  - 两两合并相邻的两个单词或短语对应的向量和矩阵
    - 给定子节点 $(a, A)$ 和 $(b, B)$，其父节点$(p, P)$ 可更新为
    - $ p = f(Ba, Ab) = g(W \cdot \mathrm{concat}(Ba, Ab)^T) $
    - $P = f_M(A, B) = W_M \cdot \mathrm{concat}(A, B)^T$

##### 基于卷积神经网络的关系抽取

- 关系抽取中每个词的“位置信息”很重要
- **词特征** 使用当前词和上下文的词向量拼接成
- **位置特征** 使用每个词汇距离两个实体词的相对位置向量拼接而成

###### Piecewise CNN

- 按实体词出现的位置对句子进行划分，然后对卷积后的输出分段池化

##### 基于BERT的关系抽取
