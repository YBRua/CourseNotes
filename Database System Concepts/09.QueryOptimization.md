# Query Optimization

## Overview of Query Optimization

1. Generate logical plans using equivalence rules
2. Annotate logical plans to get physical plans
   - A physical plan defines exactly what algorithms to use for each operation
   - And how the execution of the operations is coordinated
3. Estimate the cost of the physical plans, and choose the cheapest plan

## Rule-Based Query Rewriting

### Query Rewriting

Two relational algebra expressions are **equivalent** if they generate the same set of tuples on every legal database instance

- By *legal* we mean the instance satisfies all integrity constraints specified in its schema

### Equivalence Rules

An **equivalence rule** says that expressions of two forms are equivalent

#### Pushdown Selection

$$ \sigma_{\theta\land\theta_1\land\theta_2}(r \bowtie_{\theta'} s) \equiv \sigma_{\theta_1}(r) \bowtie_{\theta\land\theta'} \sigma_{\theta_2}(s) $$

where

- $\theta_1$ is a predicate involving only attributes of $r$
- $\theta_2$ is a predicate involving only attributes of $s$
- $\theta$ and $\theta'$ are predicates involving attributes of both $r$ and $s$

By pushing down selections, the join operation can be performed on smaller sets of tuples.

#### Pushdown Projection

$$ \Pi_{L}(\sigma_\theta(r)) \equiv \Pi_{L}(\sigma_\theta(\Pi_{L\cup L'}(r))) $$

where

- $L'$ is the set of attributes referred by $\theta$ that are not in $L$

By pushing down projections, the selection can be performed on fewer attributes

#### Others

- **Cascade of $\sigma$**: Conjunctive selection operations can be deconstructed into a sequence of individual selections
$$\sigma_{\theta_1\land\theta_2}(E) \equiv \sigma_{\theta_1}(\sigma_{\theta_2})(E) $$

- **Cascade of $\Pi$**: Only the last projection in a sequence of projection is needed if they are a sequence of subsets
$$ \Pi_{L_1} (\Pi_{L_2}(r)) \equiv \Pi_{L_1}(r), \quad L_1 \subseteq L_2$$

- Convert product to theta join
$$\sigma_\theta(r\times s) \equiv r \bowtie_\theta s$$

- Selection is **commutative**
$$ \sigma_{\theta_1}(\sigma_{\theta_2})(E) \equiv \sigma_{\theta_2})(\sigma_{\theta_1})(E) $$

- Theta joins are **commutative**
$$ E_1 \bowtie_\theta E_2 \equiv E_2 \bowtie_\theta E_1 $$

- Theta joins are **associative** in the following manner: if $\theta_2$ involves only attributes from $E_2, E_3$,
$$ (E_1 \bowtie_{\theta_1} E_2) \bowtie_{\theta_2\land\theta_3} E_3 \equiv E_1 \bowtie_{\theta_1\land\theta_3} (E_2 \bowtie_{\theta_2} E_3) $$

> $\theta_1,\theta_2,\theta_3$ can be empty

- Natural join is **associative** and **commutative** (unless the order of the attributes in the result is also taken into consideration)
$$r \bowtie s \equiv s \bowtie r$$

$$r \bowtie s \bowtie t \equiv r \bowtie (s \bowtie t)$$

- Set intersections and unions are commutative and associative
- The selection operation and projection operation distribute over set intersection and difference
- The projection operation distributes over set union
- More on textbook page 750-751

### Join Ordering

A good join ordering helps reduce the size of intermediate results

### Query Rewriting Using Equivalence Rules

1. Start with a logical plan
2. Push selections/projections down
   - **Pros:** Reduces the size of intermediate results
   - **Cons:** Can be expensive in some cases (e.g. cases where the join operation filters tuples better than the selection)
3. Join small relations first, and avoid products
   - **Pros:** Reduces the size of intermediate results
   - **Cons:** Size also depends on join selectivity
4. Convert the logical plan to a physical plan

## Cost-Based Query Optimization

### Cost Estimation

- Total cost of a plan is the sum of all estimated costs of the operators in the plan
  - Estimate statistics required are available in the *catalog*
  - The statistics may not always be accurate
- Cost of a certain operator is proportional to the size of its input
- Size of operator input can be estimated by the *selectivity* of the operator and the size of its children

#### Statistics and Catalog

- The DBMS stores internal statistics about tables, attributes and indices in internal catalogs
- Catalogs are updated periodically
- Modern databases use many sophisticated statistics

##### Statistics in Catalog

- $n_r$: number of tuples in the relation $r$
- $b_r$: number of blocks containing tuples in relation $r$
- $l_r$: size of a tuple in relation $r$ (in bytes)
- $f_r$: **blocking factor** of tuples in relation $r$, i.e. number of tuples of relation $r$ that fit into a block
- $V(A,r)$: number of *distinct values* in $r$ for attribute $A$
  - This value is the same as the number of tuples in $\Pi_A(r)$
- Statisitics are not always up-to-date because maintaining them can be costly.
  - They are only updated during periods of light system load
- DBMS may also maintain certain histograms or heavy-hitters for better estimation (see below)

### Selectivity

The **selectivity factor** of a predicate $\theta$ is the probability that a tuple in relation $r$ satisfies $\theta$

#### Selection with Equality Predicates

$$ \sigma_{A = v}(r) $$

- If a precise occurence count is available, it can be directly used for size estimation
- Otherwise, assume *the values of attribute $A$ are uniformly distributed in $r$*. Then $ n_r / V(A,r) $ would be the number of records that will satisfy the selection
  - The assumption may not hold
- Selectivity factor of $A = v$ is $1 / V (A,r)$

##### Conjunctions

$$ \sigma_{A=v \land B=u}(r) $$

- Further assumes $A = v$ and $B = u$ are *independent*
  - which is even less likely to hold
- Selectivity factor: $ 1 / (V(A,r) \cdot V(B,r))$

##### Negation

$$ \sigma_{A \neq v}(r) $$

- Selectivity factor: $1 - 1/V(A,r)$
- Generally, selectivity of $\neg\theta$ is $(1 - Selectivity(\theta))$

##### Disjunctions

$$ \sigma_{A=v\lor B=u}(r) $$

- $ 1 / V(A,r) + 1/V(B,r) - 1/ (V(A,r)V(B,r)) $

#### Range Predicates

Assume the max and min values of $A$ are available in the catalog, and that values are uniformly distributed

- Selectivity factor: $ (v - min(A,r)) / (max(A,r) - min(A,r)) $
  - $0$ if $v < min(A,r)$
  - $n_r$ if $v > max(A,r)$

### Size Estimation

Assume we want to estimate the size of natural join $r(A,B) \bowtie s(A,C)$, where $A$ denotes the common attributes of $r$ and $s$

- If $A$ is empty, then the size is the same as $|r \times s|$
- If $A$ is a key for $r$, then the number of tuples in results will be *no more than $n_s$* because each tuple in $s$ can join with at most one tuple in $r$. The case where $A$ is a key for $s$ is symmetric
- If $A$ is a key of $r$ and a foreign key in $s$ (referencing $r$), then $|r \bowtie s| = n_s$
- Generally ($A$ is not a key for either table), the size is estimated by
  - Estimate the size of the product $r \times s = n_r n_s$
  - Take $\min( n_rn_s / V(A,r), n_rn_s / V(A,s) )$ as the estimation of the size

### Estimation Errors

- Skewness is one of the main reasons that may lead to poor estimations
- The assumptions made during the estimation may not hold
  - For example, mutual independence of predicates when computing join sizes

#### Histograms

- We can build histograms in the catalog to better estimate common predicates
  - Equi-width: Equal range of bins
  - Equi-depth: Break up bins such that each bin has (approximately) the same number of tuples
- Many DBMS also store the $n$ most-frequent values and their counts (heavy hitters), and build histograms on remaining values
  - Exclude frequent items for a better histogram landscape

### Cost-Based Plan Search

- Enumerate possible physical plans
- Pick the plan with the least estimated cost
  - Can be costly because the search space is too large
  - Most optimizers uses heuristic search, which may miss the best solution
- The goal is often not getting the optimal plan, but instead avoiding horrible ones

> We will be focusing on join operators

#### Order of Join

- The search space of join ordering can be huge, making brutal-force enumeration impractical

##### Left-Deep Join

Early DBMS implementations (e.g. the System R optimizer) only considered **left-deep join**: restricting that only the left-join of a join can be a join operator.

- Allows to generate fully pipelined plans in most cases
  - Because only one of the inputs requires computing
  - Intermediate results will not be written to temporary files
  - Some joins are not fully-pipelined (e.g. sort-merge join)
- $n!$ different left-deep join trees

##### Dynamic Programming and Selinger Algorithm

Let $\mathbb{X} \subseteq \{ r_1, \dots, r_n \}$ with $|\mathbb{X}| = k$,

$$ Opt(\mathbb{X}) = \min_{r\in\mathbb{X}} \{ Opt(\mathbb{X} - \{r\}) + Cost(\mathbb{X} - \{r\}, r) \} $$

- Cost
  - Time cost $n\cdot 2^n$
    - Can be slightly larger since we need to consider different join operators
  - Space cost $2^n$

##### Interesting Order

- The order in which tuples are generated by the join of a set of relations is important for finding the overall optimal solution
  - Because it can affect the cost of further joins
    - The result of a sort-merge join is sorted on the join attribute
    - which can be exploited by later processing
- A particular sort order of the tuple is said to be an **interesting sort order** if it could be useful in later operatoin
- This can be dealt with by computing multiple optimal plans (one for each intereting order)
- Increases the complexity by a factor of $k+1$, where $k$ is the number of interesting orders

##### DP for General Join Order

$$ Opt(\mathbb{X}) = \min_{\mathbb{Y}\subset\mathbb{X}} (Opt(\mathbb{X}-\mathbb{Y}) + Opt(\mathbb{Y}) + Cost(\mathbb{X}-\mathbb{Y}, \mathbb{Y})) $$

- Complexity is approximately $O(3^n)$
