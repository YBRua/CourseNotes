# Transaction Processing

## What is a Transaction

A **transaction** is a collection of operations that performs a *single* logical function in a database application.

### Transaction in SQL

```sql
BEGIN TRANSACTION
    /* ... subsequent SQL operations */
COMMIT /* or  ABORT/ROLLBACK */ 
```

- The transaction statement begins with `BEGIN TRANSACTION`
- Ends with `COMMIT` or `ABORT/ROLLBACK`
  - `COMMIT`: Makes all the changes permanent and visible to all other transactions
  - `ABORT/ROLLBACK`: Reverts the effects of the transcation
- Most DBMS have an **autocommit** mechanism that automatically turns each statement into a transaction

### Transaction States

- **Active**: The initial state.
  - The transaction stays in this state while it is executing, and turns into one of the four following states
- **Partially Committed**: After the final statement has been executed
- **Failed**: After the discovery that normal execution cannot be continued
- **Aborted**: After the transaction has been rolled back and the database is in its initial state, before the start of this transaction.
- **Commited**: After a successful execution

### ACID Properties

- **Atomicity**: Either all operations in a transaction happen, or none of them happens (all-or-nothing).
- **Consistency**: Execution of a transaction in isolation perserve the consistency of the database (correctness).
- **Isolation**: Each transaction must be unaware of other concurrently executing transactions (as if alone).
- **Durability**: Effects of commited transactions are resilient against failures (recoverable).

#### Atomicity

All transactions must be all-or-none, and partially-committed transactions are not allowed.

- Aborted transactions can be caused by the system or the user
- The recovery manager is responsible for recovery
- The DBMS logs all actions (both in memory and on disk) so that it can undo the actions of aborted transaction

#### Consistency

- A transaction should not break the consistency of the database.
  - i.e. a transaction should not violate the constraints
- Usually the user (writing SQL queries) is responsible for ensuring the consistency

#### Isolation

- For each transaction, it should be executed in a way as if it were the only transaction running.
  - This can be easily ensured by running transactions serially.
  - However, running multiple transactions concurrently has significant benefits
    - Improved throughput and resource utilization
    - Reduced average response time

##### Concurrency Control

DMBS achieves concurrency by interleaving the operations of transactions

- The **concurrency control manager** is responsible for
  - interleaving transactions
  - assuring the equivalence of interleaved operation and some serialized operations

#### Durability

- The **recovery manager** is responsible for recovering commited transactions from possible failures.

## Transaction Isolation

- Interleaved operations may lead to unexpected execution results, depending on the order of execution
- Even if all transactions are executed serially, the order of execution may still affect the final result

### Schedule

- A schedule specifies the chronological order in which instruction of concurrent transactions are executed
- A *serial transaction* executes transactions sequentially
- Interleaving allows maximizing concurrency, but may lead to results inconsistent with those of sequential execution if a bad interleaving schedule is applied
  - A good schedule requires that the effect must be equivalent to the transactions running serially *in some order*

#### Serializable Schedule

- A schedule is **serializable** if it is equivalent ot some serial execution of the transactions
- If each transaction preserves consistency, then every serializable schedule perserves consistency

$$ Serializable \subseteq ConflictSerializable \subseteq ViewSerializable $$

We will focus on conflict serializable schedules

##### Simplified View of Transactions

- A simpolified schedule contains only **read** and **write** operations

##### Conflicting Operations

Two operations of two *different transactions* **conflict** if they (1) access the same object and (2) at least one of them is a write

Conflicts do not always leads to problems, but they are indeed the source of problems.

###### Write-Read Conflict

- Dirty Read
- A transaction reads a value written by another transaction that has not yet committed

###### Read-Write Conflict

- Unrepeatable Read
- A transaction reads the value of the same object twice. Another transaction modifies the value between the two reads

###### Write-Write Conflict

- Two transactions write the same object, the second write overwriting the first change

#### Conflict Serializability

- Two schedules $S$ and $S'$ are **conflict equivalent** if they
  - involve the same actions of the same transactions
  - every pair of conflicting operations is ordered in the same way
- A schedule $S$ is **conflict serializable** if it is conflict equivalent to some serial schedule
  - i.e. $S$ is conflict serializable if it can be transformed to a serial schedule by swaping non-conflicting operations of different transactions

##### Precedence Graph

Consider a schedule $S$ for transactions $T_1,\dots,T_n$. The precedence grpah $G$ of $S$ is defined as

- Each node of $G$ represents a transaction in $S$
- There exists an edge from $T_i$to $T_j$ if
  - An operation $O_i$ of $T_i$ conflicts with an operation $O_j$ in $T_j$
  - $O_i$ appears earlier in $S$ than $O_j$
- Schedule $S$ is serialzable iff its precedence graph is acyclic
  - Can be checked with a DFS algorithm
- If the precedence graph is acyclic, then a *topological ordering* of the transactions procuces an equivalent serialized schedule

#### View Serializability

- Two schedules $S$ and $S'$ are **view equivalent** if
  - Every value read in $S$ equals to the one read by the same read in $S'$
    - i.e. a read operation in $S$ and $S'$ yields the same result
  - The final value of every object is written by the same $T$ in $S$ and $S'$
- A schedule $S$ is **view serializable** if $S$ is view equivalent to some serial $S'$

Every conflict serializable schedule is also view serialzable. But the converse may not hold.

- Testing for view serializability is NP-hard
  - All possible orderings must be considered
- Conflict serializability is often used in practice
  - Not because of the NP-hardness of view serializability
  - But because we have a way to enforce conflict serializability as transactions run

### Schedule Recovery

- A schedule $S$ is **recoverable** if whenever a transaction $T$ commits, all transactions who have written data read by $T$ have already commited.
  - That is, if $T_1$ writes something and $T_2$ reads it, if $T_1$ commits *after* $T_2$, then $T_1$ would be *non-recoverable*: $T_2$ has read the new value written by $T_1$, and thus $T_1$ cannot be aborted (unless we also rollback $T_2$)

#### Cascading Rollbacks

- If a non-recoverable transaction is aborted, it may force other transactions to abort, and this may lead to undoing a lot of transactions.

#### Cascadeless Schedules

- A cascadeless schedule avoids cascading aborts/rollbacks.
  - Whenver a transaction $T$ reads a data item, any transaction $T'$ that has last written it must already have committed *before the read operation of $T$*.
- A cascadeless schedule enforces stronger constraint, and thus *a cascadeless schedule is always a recoverable schedule*.

## Lock-based Concurrency Control

### Lock: Recap

A transaction $T$ is allowed to access a data item only if $T$ holds a **lock** on that item

- **Shared Lock (S)**: If transaction $T$ holds a shared-mode lock on data $Q$, then $T$ can read but not write $Q$
  - Several transactions can hold the same lock because read operations will not lead to conflicts
- **Exclusive Lock (X)**: If transaction $T$ holds an exclusive-mode lock, then $T$ can both read and write the item
  - At most one transaction can hold an exclusive lock

### 2-Phase Lock (2PL) Protocol

Basic lock machenism is not enough because it does not guarantee conflict serializability.

In the two-phase lock protocol, all lock requests preced all unlock requests.

1. Growing Phase
   - Transactions may obtain locks
   - Transactions may not release locks
2. Shrinking Phase
   - Transactions may release locks
   - Transactions may not obtain locks

2PL protocol guarantees conflict serializability

#### Lock Point

The **lock point** $t$ of a transaction $T$ is the point when $T$ successfully acquires all the lock it needs

#### Analysis of 2PL

Let $t_i$ be the lock point of $T_i$. A 2PL schedule is conflict equivalent to the serial schedule obtained by ordering the transactions w.r.t. their lock points.

**Lemma**: $t_i < t_j$ for every edge $(T_i, T_j)$ in the precedence graph.

- Note that once this lemma hold, it follows that the precedence graph is acyclic.

*Intuition*: Let $O_i$ and $O_j$ be a pair of conflicting operations that induce $(T_i, T_j)$. With 2PL, $T_j$ cannot acquire the lock for $O_j$ until $T_i$ releases it.

*Proof*: Let $t_a(O_i)$ be the time when $T_a$ obtains lock for $O_i$. Let $\bar{t}_a(O_i)$ be the time when $T_i$ releases the lock. Let $t_a, t_b$ be the lock point of $T_a, T_b$. By definition of 2PL, we have $t_a \in [t_a(O_i), \bar{t}_a(O_i)]$.

Further, $t_b(O_j) \in ( \bar{t}_a(O_i), t_b ]$ or otherwise the lock cannot be obtained.

Finally, we have $t_b \ge t_b(O_j) > t_a$

#### Lock Conversion

- During the growing phase
  - Transactions can acquire a shared or exclusive locks
  - Transactions are allowed to convert a shared lock to an exclusive lock (upgrade)
- During the shrinking phase
  - Transactions can release a shared or exclusive locks
  - Transactions are allowed to convert an exclusive lock to a shared lock (downgrade)

2PL with Conversion also ensures serializability

#### Implementation of Locks

Lock and unlock requests are handled by the lock manager. It maintains a **lock table**: a hash table over the lock entries, it maps locks to their entry queues.

##### Lock Table

- On a lock request, the transaction is added to the entry queue.
- On an unlock request, the first transaction in the queue is re-activated
- The lock table may also keep a list of locks held by each transaction
  - when a transaction aborts, the locks held by this transaction can be released efficiently

### Stricter 2PL Protocols

#### Strict Two-Phase Locking

A 2PL protocol is not enough for preventing cascading aborts, because transactions can still read uncommited data.

- The Strict 2PL Protocol requires that *exclusive locks can be released only after the transaction commits*.
  - Guarantees that transactions never read other transactions' uncommited data
  - Ensures *serializability*, *recoverability*, and $avoids cascading aborts$.

#### Rigorous Two-Phase Locking

- Rigorous 2PL requires that a transaction can release all locks only after it commits.
  - R/W operations can be issued interactively
  - Difficult to determine whether a transaction no longer needs a lock

### Deadlock Handling

#### Deadlock Recap

> "拿着锁等锁"

- A deadlock is a cycle of transactions waiting for locks to be released by each other.

##### Waits-for Graph

- Nodes are transactions
- An edge from $T_i$ to $T_j$ implies that $T_i$ is waiting for a lock currently held by $T_j$
- A deadlock occurs iff there exists a cycle in the waits-for graph

#### Deadlock Prevention

- Assign priorities based on timestamp
  - Older timestamps indicate higher priority

Suppose $T$ requests a lock currently held by $T'$

- **Wait-Die**: T waits for $T'$ if $T$ has higher priority, and otherwise $T$ aborts.
- **Wound-Wait**: $T'$ aborts if $T > T'$, and otherwise $T$ waits.

!!!note Notes on Deadlock Prevention
    - There is no cycles in the waits-for graph.
    - If a transaction re-starts, it inherits its previous priority and timestampe
      - In order to prevent starvation

#### Deadlock Detection

- Creates a waits-for graph
- Periodically checks for cycles in the graph

##### Recovery from Deadlock

- **Victim Selection**: Rollback the transaction that will incur minimum cost to break the cycle
- **Rollback**: Determine how far to roll back the transaction
  - Total rollback: Abort the transaction and restarts it
  - Partial rollback: Rollback as far as necessary to release a lock that another transaction in the cycle is waiting for
- **Starvation**: A transaction can only be chosen as the victim for a limited number of times to prevent starvation

### Locking Granularity

- The lock can be placed on a tuple, a page, a table or the entire database
- Tradeoff between overhead and concurrency
  - Finer granularity: high concurrency, high locking overhead
  - Coarse granularity: low locking overhead, low concurrency
- The database hierarchy can be represented as a tree
  - When a transaction locks a node in the tree explicitly, all child nodes are also implicitly locked

#### Examples

- A transaction joins multiple relations in a database
  - Locks the entire database
- A transaction linearly scans a relation
  - Locks the relation
- A transaction accesses only part of a relation with a B+ tree index
  - Locks a small number of tuples

#### Intention Locks

An intention lock allows a higher level node to be locked in shared or exclusive mode without having to check all descendent nodes. (So some attempts to acquire a lock do not need to traverse the children)

- Intention-Shared (IS)
  - An explicit S lock exists in child nodes
- Intention-Exclusive (IX)
  - An explicit X or S lock exists in child nodes
- Shared and Intention-Exclusive (SIX / S + IX)
  - Subtree rooted at current node is locked in shared mode
  - Another explicit shared or exclusive lock exists at lower level
  - Can be used when linearly scanning the relation and update some tuples

##### Compatibility Matrix with Intention Locks Included

##### Examples

> Locks are acquired in a top-down manner

- Scans relation $R$ and updates a few tuples
  - An SIX lock on $R$, and X locks on the tuples that are updated
- Uses index to read part of $R$
  - An IS lock on $R$, and repeatedly gets an $S$ lock on some tuples of $R$
- Linearly scans $R$
  - An S lock on $R$

## Timestamp-Based Concurrency Control

### Timestamps

Each transaction $T$ receives a timestamp $TS(T)$, given by the system's clock or a logical counter (increments after a new timestamp has been assigned)

Each data item $Q$ is associated with two timestamps

- $WTS(Q)$: the most recent TS of any transaction $T$ that **wrote** $Q$ successfully
- $RTS(Q)$: the most recent TS of any transaction $T$ that read $Q$ successfully

The timestamps are updated whenever a read or write operation on $Q$ is executed successfully

### Basic Timestamp-Ordering (T/O) Protocol

- The **timestamp order** defines a serializability order
  - $TS(T) < TS(T')$ indicates that in an equivalent serial schedule, $T$ must appear before $T'$
- For each $R(Q)$ and $W(Q)$ request issued by a transaction $T$, the scheduler checks **operation conflicts**
  - Once a conflict is detected, transaction $T$ is aborted and restarted with a new timestamp

#### Read Rule

Suppose $T$ issues $R(Q)$

- If $TS(T) < WTS(Q)$, this violates the timestamp order of $T$ w.r.t. some transaction $U$ that writes $Q$
  - i.e. a transaction $U$ with more recent timestamp has written $Q$ before $T$ reads it
  - $T$ is aborted and restarted
- Otherwise, $R(Q)$ is executed, and $RTS(Q)$ is updated by $\max(RTS(Q), T(S))$

#### Write Rule

Suppose $T$ issues $R(Q)$

- If $TS(T) < RTS(Q)$ or $TS(T) < WTS(Q)$
  - i.e. a transaction $U$ with more recent timestamp has either read or written $Q$
  - $T$ is aborted and restarted
- Otherwise, the write request is permitted and $WTS(Q)$ is updated by $TS(T)$

#### Serializability

This basic T/O protocol assures serializability, as conflicting operations are executed in the order of the timestamps. In the precedency graph, transactions with smaller timestamps points to those with more recent timestamps, so there will be no cycles.

- Deadlock free, since no transaction is actually waiting
  - Does not prevent starvation
- However, schedules may not be recoverable or cascadeless

### Thomas' Write Rule

- If $TS(T) < RTS(Q)$, then abort and restart $T$
- If $TS(T) < WTS(Q)$, then ignore the write and continue
- Otherwise, perform the write and update the timestamp of $Q$

Thomas' Write Rule ignores some write operations and thus avoids some W-W conflicts.

- Does not guarantee conflict serializability
- However, it *guarantees view serializability*
  - The writes ignored by Thomas' Write Rule are *obsolete* writes that will never be read by other concurrent transactions

#### Basic T/O with Thomas' Write Rule

- $T$ issues $R(Q)$
  - The same as before
- $T$ issues $W(Q)$
  - If $TS(T) < RTS(Q)$ then rollback $T$
  - If $TS(T) < WTS(Q)$ then ignore $W(Q)$ and continue $T$
  - Else allow $W(Q)$

### Recoverability

Basic T/O Protocol (with or without Thomas' Write Rule) does not guarantee recoverability (cascadeless abort)

#### Ensuring Recoverability

Use an additional boolean $C(Q)$ to check *whether the latest transaction that writes $Q$ has committed*.

- Delay any request to read or write $Q$ is $C(Q)$ is false.

#### Revised T/O Protocol

- $R(Q)$
  - If $TS(T) < WTS(Q)$: Abort and restart $T$
  - $C(Q) = false$: Delay $T$
  - Otherwise allow $R(Q)$ of $T$ and update $RTS(Q)$
- $W(Q)$
  - If $TS(T) < RTS(Q)$: Abort and restart $T$
  - Else if $TS(T) < WTS(Q)$
    - If $C(Q) = false$: delay $T$
    - Else ignore $W(Q)$ and continue
  - Otherwise allow $W(Q)$ of $T$, update $WTS(Q)$, and set $C(Q)$ to $false$

### Recap

- Basic T/O Protocol
  - Ensures conflict serializability
- Basic T/O w/ Thomas' Write Rule
  - Ensures view serializability
  - Improves concurrency
- Revised T/O (w/ commit bit)
  - Avoids cascading aborts

## Validation-based Protocol

The validation-based protocol is also based on a timestamp mechanism. However, unlike previous T/O protocols that give each transaction a timestamp when then are started, the timestamp is assigned according to the time when $T$ enters validation (see below).

### Overview

Also known as **optimisitc concurrency control (OCC) protocol** because it assumes transactions would succeed

- Optimistic
- Process transaction as if they would succeed
- Check for serializability *only at commit time*
- If it fails, the transaction is aborted

Have high performance when there are few conflicts and locking

### Three Phases of OCC

1. Read Phase
   - $T$ executes and sorts its writes to temporary local variables
   - $T$ also tracks its **read set** $RS(T)$ and **write set** $WS(T)$
2. Validation Phase
   - A validation test is performed to check if $T$ can commit
   - i.e. whether the local update can be applied without violating serializability constraints
   - If $T$ fails the test, it will be rolled back
3. Write Phase
   - If $T$ passes validation, the update is then actually performed on the database

#### Read Set and Write Set

- **Read Set** $RS(T)$: The elements that $T$ reads
- **Write Set** $WS(T)$: The elements that $T$ writes

The line of text here is used to split the two unordered lists.

- Each transaction stores the updates to $WS(T)$ in a local storage in the Read Phase
  - Dirty results are isolated
  - Other transactions cannot see these updated values
- After the Read Phase, if $T$ passes the validation, then the updates are applied to the DB

#### Validation Phase

- Each transaction is assigned with a *timestamp* that determines the serial equivalent order
- The timestamp of $T$ is assigned according to the time when $T$ *finishes its read phase and enters its validation phase*

When a transaction $T_j$ completes its read phase, then for each $T_i$

- If $TS(T_i) < TS(T_j)$, then $T_j$ must satisfies at least one of the validation rules (see below)
- Otherwise $T_j$ is aborted
- If the transactions satisfies the validation, then serializability can be ensured

!!!note Lemma
    If $TS(T_i) < TS(T_j)$, then for any conflicting operations $O_i$ and $O_j$ such that $O_i \in T_i$ and $O_j \in T_j$, the OCC scheduler places $O_i$ before $O_j$

    The validation rules will make sure that all conflicts go in one way. Thus OCC guarantees conflict serializability.

##### Validation Rules

- $T_i$ completes all three phases before $T_j$ begins

```text
    ┌─────┬─────┬─────┐
Ti  │  R  │  V  │  W  │
    └─────┴─────┴─────┘

           
                        ┌─────┬─────┬─────┐
Tj                      │  R  │  V  │  W  │
                        └─────┴─────┴─────┘
```

- $T_i$ completes its *write phase* before $T_j$ begins its *write phase*, and $WS(T_i) \cap RS(T_j) = \emptyset$ (items written by $T_i$ will not be read by $T_j$)

```text
    ┌─────┬─────┬─────┐
Ti  │  R  │  V  │  W  │
    └─────┴─────┴─────┘

              ┌─────┬─────┬─────┐
Tj            │  R  │  V  │  W  │
              └─────┴─────┴─────┘
```

- $T_i$ completes its *read phase* before $T_j$ starts its *read phase* and $WS(T_i) \cap RS(T_j) = \emptyset$, and $WS(T_i) \cap WS(T_j) = \emptyset$ (items writeen by $T_i$ will not be overwritten by $T_j$)

In the last 2 cases, the additional requirements ensure that items read by $T_j$ is not affected by the operations of $T_i$ (i.e., $T_j$ reads values that are already committed)
