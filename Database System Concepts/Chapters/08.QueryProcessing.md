# Query Processing

## Overview

1. Parsing and Translation
   - Check syntax and verify relations
   - Translate the query into its internal form and then translate it into relational algebra
2. Query Optimization
   - An SQL query can usually be translated into RA in one of several ways, and each relational algebra can be evaluated with one of several different algorithms
   - We use annotations to specify how to evaluate each operation
   - A relational algebra annotated with instructions on how to evaluate it is called an **evaluation primitive**
   - A sequence of primitives that can be used to evaluate a query is a **query-execution plan**
   - Query optimization is the process of constructing an evaluation plan that minimizes query evaluation cost
3. Evaluation

## Query Optimization

### Measures of Query Cost

- Many factors contribute to time costs
  - disk access
  - CPU
  - network
- Cost can be measured by
  - Response time
  - Resource consumption

> We use *total resource consumption* as cost metric. As the response time varies from machine to machine.
> More specifically, we only consider the **disk cost**

#### Disk Cost

We use the *number of pages transferred* from storage and the *number of random I/O accesses* (each of which requires a disk seek on magnetic storage) to estimate disk cost

- **#(seek)**: Number of disk seeks (random IO access)
- **#(pages)**: Number of page read/write (transferring pages)

## Algorithms for RA Operations

- [Selection](#selection)
- [Sort](#sort)

### Selection

$$ \sigma_{attr=value}(R) $$

#### Linear Scan

- Scan each file block and test all records for the selection condition
- Can be applied regardless of
  - selection condition
  - ordering of records in the file
  - availability of indices
- `#(seeks) = 1` for locating the first block of the file
  - Additional seeks may be needed if the file are not stored contiguously, but we ignore this case for simplicity
- `#(pages) = N`
  - Note that if at most one record satisfies the condition (e.g. Equality predicate), the average number of page transfers is `N/2`
- General purpose (can be applied to any file), but slow

#### Index Scan with Clustering Index

For B+-Tree indices, we assume a random I/O access (seek) is required for each node in the path from root to leaf. However, most modern optimizers assume the inner nodes of B+ Tree fit in main memory.

##### Clustering B+ Tree with Equality on Key

> Retrieve a **single record** that satisfies the corresponding equality condition

- Traverse down to a leaf node plus an additional IO to fetch the record
- Let `h_i` be the height of the B+ Tree
  - `#(seek) = h_i + 1`
  - `#(page) = h_i + 1`

##### Clustering B+ Tree with Equality on Non-Key

> Retrieve multiple records.

- Traverse down to a leaf node, fetch the record, and scan sequentially for $b$ consecutive blocks containing records with the specified search-key value
- Let `b` be the number of pages containing the search-key
  - `#(seek) = h_i + 1`
  - `#(page) = h_i + 1 + b`

#### Index Scan with Secondary Index

##### Equality on Key

- Retireve a single-record since the search-key is a candidate key
  - `#(seek) = h_1 + 1`
  - `#(page) = h_i + 1`

##### Equality on Non-Key

- Retrieve multiple candidates
  - `#(seek) = h_i + n`
  - x`#(page) = h_i + n`
  - `n` is the number of matching tuples. Since each record may be on a different page

#### Selections Involving Comparisons

##### Clustering B+ Tree

- For $A \ge V$, use index to find first tuple $\ge V$ and scan sequentially
- For $A \le V$, scan from the beginning sequentially until first tuple $> V$. Does not require using index.
  - `#(seek) = h_i + 1`
  - `#(page) = h_1 + 1 + b`
  - `b` is the number of pages where tuples satisfying the constraints are located

##### Non-Clustering B+ Tree

- Algorithm is basically the same
  - `#(seek) = h_i + n`
  - `#(page) = h_i + n` because tuples may not locate on the subsequent pages
  - Requires an IO per record; Linear scan may be cheaper

#### Conjunction

$$ \sigma_{\theta_1 \wedge \cdots \wedge \theta_n}(R) $$

##### Using One Index

- Select $\theta_i$ s.t. the selection implementation results in the least cost for $\theta_i$ and test other conditions after fetching the tuples into memory buffer

##### Using Composite Index

- Use composite (multi-key) index if available

##### Intersection of Identifiers

- Require indices with record pointers, on the fields involved in the individual conditions
- Scan for each index for pointers that satisfies an individual condition
- Use the intersection of all retrieved sets of pointers
- If some indices does not exist, the system first filter some pointers with the available indices, and then test the corresponding tuples in main memory

#### Disjuction

##### Union of Identifiers

- Applicable if all individual conditions have a corrsponding index
- Otherwise directly using a linear scan would be more efficient

### Sort

#### Sorting in DBMS

- Physical sorting
  - Sort tuples
- Loigcal sorting with indexing
  - Cheaper, but may lead to one IO per record in future queries

#### External Sorting

- If data fits in memory, then a standard sorting algorithm can be applied
- External sorting is used when
  - data do not fit in memory
  - IO-cost-aware

#### External Merge Sort

Takes a **divide-and-conquer** approach that first splits data into separate runs and then sort them individually

- **Sort Phase**: Sort blocks of data that fit in main-memory and then write back the sorted blocks to a sub-file on disk
- **Merge Phase**: Combine sorted sub-files into a single large file

##### Sorting Phase

Assume there are $M$ available pages in memory.

- In the sorting phase, a number of sorted **runs** are created.
- Each run is sorted and contains some of the records in the relation

1. Read $M$ pages of relations into memory
2. Sort the in-memory pages
3. Write each sorted data to a separate **run file**

##### Merging Phase

Merge runs separately

1. Use $M-1$ pages as input from sorted runs
2. The remaining 1 page is used as the output buffer
3. Merge each $m-1$ runs into a new run, stored on the buffer

##### Cost Analysis

Let $N$ be the number of pages containing records in the input table, and the number of buffers in main memory is $M$

1. Sorting Phase
   1. The sorting phase requires $2N$ disk read/write. One read and one write per page.
   2. The initial number of runs is $\lceil N / M \rceil$.
2. Merge Phase
   1. During the merge phase, a block of multiple pages are read or written at a time for performance improvement, thus requiring $B$ buffers to be used together for a single input run or the output run. Thus $\lfloor M/B \rfloor - 1$ runs can be merged for each iteration.
   2. Total number of merge iteration required is $\lceil \log_{\lfloor M/B \rfloor - 1}(N/M) \rceil$, denote this by $K$
3. Total number of passes is $1 + K$ (One for sorting phase and $K$ for merging phase)
   1. Each iteration requires one block read and one block write, except for the final merge iteration (we assume the final output is not written back to disk), so the total number of page transfers is $N \times (2K - 1 + 2) = N(2K - 1)$
4. For disk seeks, each merge pass requires around $\lceil N/B \rceil$ seeks for both read and write, except for the final pass because the output is not written back. So the total number of page seeks is $2\lceil N/M \rceil + \lceil N/B \rceil (2K-1)$

### Join

#### Nested Loop Join

Consider a join operation $r \bowtie_\theta s$

Assume outer table $r$ has $M$ pages and $m$ tuples; Inner table $s$ has $N$ pages and $n$ tuples; Only consider page transfer cost

##### Algorithm

```ts
result = new Set()
for (const t_r of r) {
  for (const t_s of s) {
    if (satisfies(theta, t_r, t_s)) {
      result.add(join(t_r, t_s, theta))
    }
  }
}
```

- Most basic join algorithm
- Does not require indices, can be used with any kind of join operation

##### Complexity

- If the buffer can hold only 2 pages (1 for each relation)
  - Cost: $M + Nm$
  - For each tuple in the outer relation, we need to load and process each page in the inner relation
  - Note that the cost is asymmetric, so the choice of inner and outer table impacts performance
- If the buffer can hold both relation
  - Cost: $M + N$

#### Block Nested Loop Join

- Scan page by page

```ts
for (const block_r of r.pages()) {
  for (const block_s of s.pages()) {
    for (const tuple_r of block_r) {
      for (const tuple_s of block_s) {
        if (satisfies(theta, tuple_r, tuple_s)) {
          result.add(join(tuple_r, tuple_s, theta))
        }
      }
    }
  }
}
```

##### Complexity

- $M + M \times N$
- The relation with fewer number of pages should be used as the outer table
- If $B$ pages are available to compute the join, then
  - Use $B-2$ pages to scan the outer table
  - Use one for inner table and the other for output buffering
  - Cost $M + \lceil M / (B-2) \rceil \times N$
- If $B$ is large enough to store all pages
  - Cost $M + N$

#### Indexed Nested Loop Join

- For nested loop join, for each tuple in the outer table, we must do a sequential scan of the inner table.
- The join operation contains a *selection operation* on the inner table.
- Can be optimized if the inner table $s$ has an available index.
- The index can also be built on the fly, depending on the decision of the optimizer.

```ts
for (const t_r of r) {
  for (const t_s of index_selection(theta, t_r.attribute)) {
    // use the index of s for checking constraint theta
    if (satisfies(theta, t_r, t_s)) {
      result.add(t_r, t_s, theta)
    }
  }
}
```

##### Complexity

Assume the cost of each index scan can be bound by $C$ per tuple. $C$ can be estimated by the cost of selection in the index

- Cost $M + m \times C$

#### Merge Join

- Require $\theta$ to be an equality predicate (equal join or natural join)
- Consists of a *sort pass* and a *join pass*
  - In sort pass, sort tuples in $r$ and $s$ by join key
  - In join pass, merge-scan the sorted tables and emit tuples that match

```ts
let pr, ps, pss;
// pr, ps: points to the first tuple in r and s
// pss: points to the match starting point

while (pr !== null && pss !== null) {
  // scan until we find equal values in table r and s
  while (t_pr[A] < t_pss[A]) {
    ++ pr;
  }
  while (t_pr[A] > t_pss[A]) {
    ++ pss;
  }

  // check matches
  while (t_pr[A] === t_pss[A]) {
    ps = pss;
    while (t_pr[A] === t_ps[A]) {
      result.add(t_pr, t_ps, theta)
      ++ps
    }
    ++pr
  }
  pss = ps
}

```

##### Complexity

- Common case: If every set of match candidates (in the `t_pr[A] === t_pss[A]` loop) in table $s$ and $r$ can be stored in memory
  - Cost $M + N$
- Worst case (very rare): The join attribute for all tuples in both relations has the same value, and the memory can only hold one page for each table.
  - Cost $M + Nm$
  - The same as ordinary nested loop join

#### Hash Join

> Applicable for equal join and natural join

- Observe that if tuple $t_1$ in $r$ and $t_2$ in $s$ satisfy the join condition, they will have the same value for the join condition
  - Use a hash function $h$ to partition tuples of both relations
  - Compute join results on each partition

##### Basic Hash Join

1. Build Phase: Scan outer table $r$ and construct a hash table using a hash function $h$ on the join attributes
2. Probe Phase: Scan the inner table and use $h$ on each tuple to jump to a location in the hash table and find a matching tuple.

- Cost: $M + N$ if we assume the hash table could be stored in memory

##### Improved Hash Join

1. Partition Phase
   - Partition $r$ into $K$ buckets
   - A block of memory is reserved as the output buffer for each partition
   - Partition $s$ in the same way
2. Probe Phase
   - Load each partition $s_i$ into memory and build an in-memory hash index using another hash function
   - Load $r_i$ from disk and run the probe phases

- Assumes each $s_i$ and its hash index can be put in the memory
  - Can be done by choosing a proper $K$ and $h$
  - A typical value for $K$ is $\lceil N / B \rceil \times f$ where $f$ is a *fudge factor* and $B$ is the available memory pages
  - Note that $r_i$ of the probe input need not fit in memory
  - A *recursive* partition may be required if $K$ is greater than the number of pages $B$

###### Cost

> Assume no recursive partition

- Partition Phase: $2(M+N)$
  - R & W both input tables
- Probe Phase: $M + N$
  - Read both input tables

#### Wrapping Up

|        Algorithm         |        IO Cost         |
| :----------------------: | :--------------------: |
|     Nested Loop Join     |       $ M + mN$        |
|  Block Nested Loop Join  |        $M + MN$        |
| Indexed Nested Loop Join | $M + m\cdot C_{index}$ |
|     Sort-Merge Join      |  $ M + N + C_{sort} $  |
|        Hash Join         |        $3(M+N)$        |

### Duplication Elimination

#### Sorting-Based

- Sort and eliminate duplication during sorting process
- Worst-case cost the same as sorting cost

#### Hashing-based

- Partition the table with a hash function
  - Duplicated tuples will be in the same partition
- Build a hash index for each partition and eliminate duplication during the process

### Projection

#### Without deduplication

- Trivial. Can be done on the fly or integrated into other operators

#### With deduplication

1. Perform projection first
2. Deduplication

### Set Operations

#### Set Op via Sorting

- Sort $s$ and $r$
- Scan the sorted tables to produce results in a way similar to sort-merge join
  - Cost $M + N + C_{sort}$

#### Set Op via Hashing

##### Union

1. Partition both relations
2. Process each partition $r_i$
   1. Use another hash function to build in-memory index for $r_i$
   2. Add tuples in $s_i$ to the hash index if they are not already in it
   3. Add tuples in the hash index to the result

##### Intersection

- Add tuples in $s_i$ to the results if it has a match in the hash index
  - And remove the tuple from the constructed hash index for de-duplication

### Aggregation

#### Aggregation via Sorting

1. Sort tuples on the `GROUP BY` attributes
2. Perform sequential scan over the sorted data for aggregation

- Cost: $C_{sort} + M$

#### Aggregation via Hashing

1. Use $h_1$ for partition
2. For each partition, build an in-memory hash table with another hash function $h_2$ and compute the aggregation
   - During this step, apply partial aggregation (keep track of a running value) on the fly

| GroupBy | Running Value  |
| :-----: | :------------: |
|  `MIN`  |     `MIN`      |
|  `MAX`  |     `MAX`      |
| `COUNT` |    `COUNT`     |
|  `SUM`  |     `SUM`      |
|  `AVG`  | `(COUNT, SUM)` |

### Evaluation of Expression Tree

#### Materialization

- Each operator processes its input all at once and then emits the output all at once
  - Output is *materialized* as a single result to disk
  - Always applicable
    - Especially when there is extremely insufficient memory
- Each evaluation step produces a temporary relation

#### Pipelining

- Each operator passes on tuples to its parent operation as the operation is executed
- Evaluate several operators simultaneously, passing results of one operator on to the next
- Much cheaper than materialization as there is no need to store a temporary relation
- Not always possible because some operator requires blocking until it is fully evaluated
  - Joins, subqueries and sorting

##### Pipelinging via the Iterator Model

> a.k.a. Volcano Model

- An operator is implemented as an iterator

```ts
// a typical interface of the iterator
interface Iterator {
  open(): () => void;
  next(): () => Tuple;
  close(): () => void;
}
```

- Each operator produces a tuple stream, and iterates over its input streams
- Applies a *pull* based execution strategy
  - Call `next()` repeatedly on the root
  - The iterator model recursively calls `next()` on the inputs

###### Push-based Iterator Model

> a.k.a. Producer-driven pipelining

- Operators produce tuples *eagerly* and pass them up to their parents
- Buffers are used between operators for temporary storage
  - Child puts tuples into buffer, and parent removes tuples from buffer
  - If the buffer is full, the child waits until there is space

###### Blocking Operators

- Blocking operators cannot generate any output until all input is consumed
- However, blocking operators can usually consume inputs from pipeline, or produce outputs to a pipeline
  - For example, we can split a hash join into a parition phase and a build-and-probe phase. A single partition phase is a blocking phase, but once a partition has been built, it can be passed down to the next build-and-probe phase.
