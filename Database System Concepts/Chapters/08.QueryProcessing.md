# Query Processing

## Overview

1. Parsing and Translation
   - Check syntax and verify relations
   - Translate the query into its internal form and then translate it into relational algebra
2. Query Optimization
   - An SQL query can usually be translated into RA in one of several ways, and each relational algebra can be evaluated with one of several different algorithms
   - We use annotations to specify how to evaluate each operation
   - A relational algebra annotated with instructions on how to evaluate it is called an **evaluation primitive**
   - A sequence of primitives that can be used to evaluate a query is a **query-execution plan**
   - Query optimization is the process of constructing an evaluation plan that minimizes query evaluation cost
3. Evaluation

## Query Optimization

### Measures of Query Cost

- Many factors contribute to time costs
  - disk access
  - CPU
  - network
- Cost can be measured by
  - Response time
  - Resource consumption

> We use *total resource consumption* as cost metric. As the response time varies from machine to machine.
> More specifically, we only consider the **disk cost**

#### Disk Cost

We use the *number of pages transferred* from storage and the *number of random I/O accesses* (each of which requires a disk seek on magnetic storage) to estimate disk cost

- **#(seek)**: Number of disk seeks (random IO access)
- **#(pages)**: Number of page read/write (transferring pages)

## Algorithms for RA Operations

- [Selection](#selection)
- [Sort](#sort)

### Selection

$$ \sigma_{attr=value}(R) $$

#### Linear Scan

- Scan each file block and test all records for the selection condition
- Can be applied regardless of
  - selection condition
  - ordering of records in the file
  - availability of indices
- `#(seeks) = 1` for locating the first block of the file
  - Additional seeks may be needed if the file are not stored contiguously, but we ignore this case for simplicity
- `#(pages) = N`
  - Note that if at most one record satisfies the condition (e.g. Equality predicate), the average number of page transfers is `N/2`
- General purpose (can be applied to any file), but slow

#### Index Scan with Clustering Index

For B+-Tree indices, we assume a random I/O access (seek) is required for each node in the path from root to leaf. However, most modern optimizers assume the inner nodes of B+ Tree fit in main memory.

##### Clustering B+ Tree with Equality on Key

> Retrieve a **single record** that satisfies the corresponding equality condition

- Traverse down to a leaf node plus an additional IO to fetch the record
- Let `h_i` be the height of the B+ Tree
  - `#(seek) = h_i + 1`
  - `#(page) = h_i + 1`

##### Clustering B+ Tree with Equality on Non-Key

> Retrieve multiple records.

- Traverse down to a leaf node, fetch the record, and scan sequentially for $b$ consecutive blocks containing records with the specified search-key value
- Let `b` be the number of pages containing the search-key
  - `#(seek) = h_i + 1`
  - `#(page) = h_i + 1 + b`

#### Index Scan with Secondary Index

##### Equality on Key

- Retireve a single-record since the search-key is a candidate key
  - `#(seek) = h_1 + 1`
  - `#(page) = h_i + 1`

##### Equality on Non-Key

- Retrieve multiple candidates
  - `#(seek) = h_i + n`
  - x`#(page) = h_i + n`
  - `n` is the number of matching tuples. Since each record may be on a different page

#### Selections Involving Comparisons

##### Clustering B+ Tree

- For $A \ge V$, use index to find first tuple $\ge V$ and scan sequentially
- For $A \le V$, scan from the beginning sequentially until first tuple $> V$. Does not require using index.
  - `#(seek) = h_i + 1`
  - `#(page) = h_1 + 1 + b`
  - `b` is the number of pages where tuples satisfying the constraints are located

##### Non-Clustering B+ Tree

- Algorithm is basically the same
  - `#(seek) = h_i + n`
  - `#(page) = h_i + n` because tuples may not locate on the subsequent pages
  - Requires an IO per record; Linear scan may be cheaper

#### Conjunction

$$ \sigma_{\theta_1 \wedge \cdots \wedge \theta_n}(R) $$

##### Using One Index

- Select $\theta_i$ s.t. the selection implementation results in the least cost for $\theta_i$ and test other conditions after fetching the tuples into memory buffer

##### Using Composite Index

- Use composite (multi-key) index if available

##### Intersection of Identifiers

- Require indices with record pointers, on the fields involved in the individual conditions
- Scan for each index for pointers that satisfies an individual condition
- Use the intersection of all retrieved sets of pointers
- If some indices does not exist, the system first filter some pointers with the available indices, and then test the corresponding tuples in main memory

#### Disjuction

##### Union of Identifiers

- Applicable if all individual conditions have a corrsponding index
- Otherwise directly using a linear scan would be more efficient

### Sort

#### Sorting in DBMS

- Physical sorting
  - Sort tuples
- Loigcal sorting with indexing
  - Cheaper, but may lead to one IO per record in future queries

#### External Sorting

- If data fits in memory, then a standard sorting algorithm can be applied
- External sorting is used when
  - data do not fit in memory
  - IO-cost-aware

#### External Merge Sort

Takes a **divide-and-conquer** approach that first splits data into separate runs and then sort them individually

- **Sort Phase**: Sort blocks of data that fit in main-memory and then write back the sorted blocks to a sub-file on disk
- **Merge Phase**: Combine sorted sub-files into a single large file

##### Sorting Phase

Assume there are $M$ available pages in memory.

- In the sorting phase, a number of sorted **runs** are created.
- Each run is sorted and contains some of the records in the relation

1. Read $M$ pages of relations into memory
2. Sort the in-memory pages
3. Write each sorted data to a separate **run file**

##### Merging Phase

Merge runs separately

1. Use $M-1$ pages as input from sorted runs
2. The remaining 1 page is used as the output buffer
3. Merge each $m-1$ runs into a new run, stored on the buffer

##### Cost Analysis

Let $N$ be the number of pages containing records in the input table, and the number of buffers in main memory is $M$

1. Sorting Phase
   1. The sorting phase requires $2N$ disk read/write. One read and one write per page.
   2. The initial number of runs is $\lceil N / M \rceil$.
2. Merge Phase
   1. During the merge phase, a block of multiple pages are read or written at a time for performance improvement, thus requiring $B$ buffers to be used together for a single input run or the output run. Thus $\lfloor M/B \rfloor - 1$ runs can be merged for each iteration.
   2. Total number of merge iteration required is $\lceil \log_{\lfloor M/B \rfloor - 1}(N/M) \rceil$, denote this by $K$
3. Total number of passes is $1 + K$ (One for sorting phase and $K$ for merging phase)
   1. Each iteration requires one block read and one block write, except for the final merge iteration (we assume the final output is not written back to disk), so the total number of page transfers is $N \times (2K - 1 + 2) = N(2K - 1)$
4. For disk seeks, each merge pass requires around $\lceil N/B \rceil$ seeks for both read and write, except for the final pass because the output is not written back. So the total number of page seeks is $2\lceil N/M \rceil + \lceil N/B \rceil (2K-1)$
