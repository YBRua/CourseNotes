# Index and Hashing

## Creation of Indices in SQL

- In SQL, we can create an index with `CREATE INDEX` command
- Indices can greatly speed up lookups, but will impose on updates
- Databases usually automatically creates indices on primary keys

## Basics

The database system would look up an index to find on which disk block the corresponding record resides.

### Creating Indices

In SQL, `CREATE INDEX` is used for creating an index

```sql
CREATE INDEX index_name ON relation_name (attributes)
```

- Usually, an index on *primary keys* are automatically created by the DB system

### Indices

An **index file** consists of records (called **index entries**) of the form `searchKey + pointer`. An index entry has a search-key value and pointers to one or more records with that value as the search-key value

- The **search key** is an attribute or a set of attributes used to lookup records in a file
- Index files are typically much smaller than the original file

#### Evaluation Metrics on Indices

- Access Types
  - Point Query
  - Ranged Query
- Access/Query Time
- Insertion Time
- Deletion Time
- Space Overhead

#### Basic Types of Indices

- **Ordered indices**: Search keys are stored in a sorted order
- **Hash indices**: Search keys are distributed uniformly across *buckets* using a *hash* function

#### Ordered Indices

An **ordered index** stores the values of the search-keys in sorted order and associates each key with the records that contain it.

- Index entries are stored sorted on the search key

##### Clustering Index and Nonclustering Index

- A **clustering index** (a.k.a. **primary index**) is an index whose search key has the same order as the sequential order of the file
- A **nonclustering index** (a.k.a. **secondary index**) is an index whose search key specifies an order different from the sequential order of the file

An **index-sequential file** is a sequential file ordered on the search key, with a clustering index on the search key

!!! note Note: Comparison between clustering and nonclustering indices
    - Clustering indices
      - Good locality
      - Substantial benefits when searching for records
      - Expensive to maintain on database update
        - Because the index must be in the same order as the sequential order of the relation
    - Nonclustering indices
      - Easier to update
      - But sequential scan using secondary indices can be slow on disk
        - Expensive ranged query

##### Dense and Sparse Index Files

- A **dense index** is an index whose record appears for each search key value in the file.
  - i.e., every possible value in the file has a corresponding search key in the index.
  - In a *dense clustering index*, the index record containing the search-key value points to the first data record with that value; the rest of the records are stored sequentially after the first record
  - In a *dense secondary index*, the index must store a list of pointers to all records with that value.
- A **sparse index** is an index that only contains records for *some* search-key values
  - Can be used records are sequentially ordered on a search-key (the index is a clustering index)
  - Consumes less space while still guarantees some lookup performance (although it is generally slower than a dense index)
  - Also easier to maintain on updates

!!! note Note: Tradeoff between dense and sparse indices
    For a clustering index, we can split the file into blocks, and create an index entry for every block in the file.

    For a non-clustering index, we can use a *multi-level index*: create a sparse index on top of dense index.

###### Secondary Dense Indices

- For secondary indices, they actually points to 'buckets' that contains pointers to all the actual records with the corresponding search-key value
- Secondary indices have to be dense, or otherwise the lookup cannot be done efficiently

### Multi-level Index

For a single-level index, if the index is too large to fit into memory, accessing the index becomes expensive

- To solve this problem, database systems treat index on disk as a sequential file, and create a sparse index on it
  - **outer index** refers to the sparse index of the basic index
  - **inner index** refers to the basic index file

### Index Update

We assume clustering index in this section

#### Deletion

- For a dense index, deletion of a search-key is similar to the file record deletion.
  - Assume the search key points to only a single value, then the search key can be directly deleted
  - Assume the search key points to multiple values, then the deletion of the search key depends on position of the record being deleted
- For a sparse index
  - If an entry for the search key exists in the index, the search key is deleted, and replaced with the next saerch-key value in the file
  - If the next search-key value is also in the index, then the search key can be directly deleted

#### Insertion

- For a dense index
  - If the search-key value does not exist in the index, insert it
    - Need to create new space for the new entry, in order to maintain the sequential order
- For a sparse index
  - Assume the index stores an entry for each block of the file, then no change needs to be made unless a new block is created
    - In the case where a new block is created, the first search-key value in the new block will be inserted into the index

## B+ Tree

> The B+ Tree is a special multi-level index. It is a data structure that supports both efficient lookup and efficient sequential (incremental) access

### Definition of a B+ Tree

A B+ Tree is a tree with the following properties

- All paths from root to leaves are of the same length (balanced)
- Any node that is neither a root nor a leaf has between $\lceil n/2 \rceil$ and $n$ children (at least half-full)
- A leaf node has between $\lceil (n-1)/2 \rceil$ and $n-1$ values (at least half full)
- Special cases
  - If the root is not a leaf, it should have at least 2 children
  - If the root is a leaf (i.e., there are no other nodes in the tree), it can have between $0$ and $(n-1)$ values

### Structure of a B+ Tree

#### Typical Structure of a Node

A node with $n$ children have $n-1$ keys to guide lookups. The value of key $i$ is the minimum value of the $(i+1)$-th subtree

|  P1   |  K1   |  P2   |  K2   |  ...  | Pn-1  | Kn-1  |  Pn   |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |

- Each $P$ is a pointer
  - Pointers at non-leaf nodes points to child nodes
  - Pointers at leaf nodes points to actual records (or buckets of records)
- Each $K$ is a search-key value
  - Search keys are ordered. $K_1 < K_2 < \cdots < K_n$

#### Non-leaf Nodes in B+ Trees

- Non-leaf nodes form a multi-level sparse index on leaf nodes

For a non-leaf node with $m$ pointers

- All search keys in the subtree pointed to by $P_1$ are less than $K_1$
- For $2 \le i \le n-1$, all search keys in the subtree pointed to by $P_i$ have values greater than or equal to $K_{i-1}$, and less than $K_i$. $K_i$ indicates the minimum value in subtree $i-1$.
- All search keys in the subtree pointed to by $P_n$ are greater than or equal to $K_{n-1}$

#### Leaves of B+ Trees

- Leaves of B+ trees store pointers pointing to file records
- For $i = 1,\dots,n-1$, pointer $P_i$ points to file record with search-key value $K_i$
  - If $L_i, L_j$ are two leaves with $i < j$, then the search-key values in $L_i$ are less than or equal to those of $L_j$.
- $P_n$ points to the next leaf node in the search-key order (for sequential access performance)

### Queries on B+ Trees

#### Single Query

Suppose we are querying value $v$

1. Start from root node
2. Do until reaches leaf nodes
   1. Find key $k$ such that $v \le k$
      1. If no such $k$ exists, then descend to the last non-null subtree
      2. If $ v = C.K_i$ then descend to $P_{i+1}$
      3. Else, desend to $P_{i}$
3. If some key $K_i$ at leaf node satisfies $K_i = v$, then return $C.P_i$
   1. Else, return null (query not hit)

#### Ranged Query

Find the lower bound, and then traverse along the linked list

#### Performance

For a file with $K$ search keys, the height of the tree is no more than $ \lceil \log_{\lceil n/2 \rceil}K \rceil $. Typically, 4 nodes are accessed in a lookup traversal from root to leaf

### Insertion

1. Find correct leaf $L$
2. Add new entry into leaf $L$
   - If $L$ has enough space (not full), then the insertion is done
   - Otherwise split $L$ into two smaller trees
     - Split in the middle, and copy the middle value to the parent tree as a key
3. If the parent is also full, then recursively split the parent, pushing (not copying) middle key to upper levels of the tree

### Deletion

1. Find correct leaf $L$
2. Remove the entry from $L$
   - If $L$ is still half-full after deletion, then the operation is done
   - Otherwise borrow elements from siblings
   - If siblings also do not have enough elements, then merge $L$ with its siblings
3. If the parent is also not-half-full after a merge, then recursively merge nodes, deleting entries pointing to the merged subtrees

### Complexity of B+ Trees

Time complexity for query, insertion and deletion of a $n$-nary B+ tree for $K$ search keys are all $\log_{\lceil n/2 \rceil}(K)$

## Hash Index

### Static Hashing

- A **bucket** is a unit of storage containing one or more entries
  - A bucket is typically a database page (for disk-based indices)
- We obtain the bucket of an entry from its search-key using a **hash function** $h: K \mapsto B$
  - where $K$ is a search-key and $B$ is a bucket address
- Entries with different hash-keys may be mapped to the same bucket
  - So the bucket has to be searched *sequentially* to locate an entry

#### Bucket Overflows

When inserting a new record, if the hashed bucket does not have enough space, a **overflow** is said to occur.

- Bucket overflows occur because
  - Insufficient buckets
  - Skew in distribution of records: some buckets are more likely to be mapped to
    - Multiple records have the same serach-key value
    - The hash function produces non-uniform distribution of key values

##### Overflow Chaining

- The overflowed buckets of a given bucket are chained together in a linked list
- Closed hashing / closed addressing

##### Linear Probing

- The set of buckets is fixed
- If the bucket is full, the system looks for a next (cyclic order) empty bucket.
- Open hashing / open addressing
- Database systems still prefer closed hashing, because deletion in a open addressing hash table is complex

#### Hash Index and Hash File Organization

- In a **hash index**, buckets store entries with pointers to records
  - Tuples themselves are still stored together
- In a **hash file organization**, buckets store records
  - Tuples are stored in separate buckets

#### Deficiencies of Static Hashing

- Static hashing maps search-key values to *fixed* set of $B$ bucket addresses
- The number of buckets is fixed when the index is created
  - Performance degenerates as databases grow or shrink with time
  - Lack of flexibility

### Hash Function

- The distribution should be *uniform*. The hash function assigns each bucket the same number of search-key values from the set of all possible search-key values
- The distribution should be *random*. On average, each bucket should have almost the same number of values assigned to it. The hash value should not correlate to any external order.

### Dynamic Hashing

#### Extendable Hashing

- Extendable hashing splits and coalescing buckets as the database grows and shrinks.
- Uses a hash function that generates a relatively large range ($b$-bit integer where $b$ is typically $32$)
  - Create buckets on demand, when records are inserted into buckets
  - Uses only $i$ bits for addressing, where $0 \le i \le b$. $i$ is used as an address in an additional table of bucket addresses
  - $i$ changes with size of database

```text
                    buckets
    prefix       ┌───────────────┐
      i      ┌───►               │ i1
   ┌──────┐  │   └───────────────┘
00 │      ├──┤
   ├──────┤  │   ┌───────────────┐
01 │      ├──┘ ┌─►               │ i2
   ├──────┤    │ └───────────────┘
10 │      ├────┘
   ├──────┤      ┌───────────────┐
11 │      ├─────►│               │ i3
   └──────┘      └───────────────┘
    bucket
    address
    table
```

- $i$ bits of $h(K)$ are required to locate the bucket for $K$
- Several consecutive table entries may point to the same bucket
- Entries in the same bucket have the same prefix
  - The length of the prefix can be shorter than $i$
  - For bucket $j$, we use a *local* count $i_j$ to denote the length of shared prefix in $j$

#### Queries and Updates in Dynamic Hashing Systems

##### Query

1. Compute $h(K)$
2. Use the first $i$ bits to find the corresponding table entry
3. Follow the pointer in the table entry

##### Insertion

1. Look up bucket $j$
2. Attempt to insert
   1. If the bucket still has space, then simply insert
   2. Otherwise, the bucket must be splitted and re-distributed
      1. If $i = i_j$, only one entry in the bucket address table points to $j$.
         1. Increment $i$ by $1$ (i.e. use an additional bit for prefix, thus doubling the number of buckets) and update pointers for address table entries
         2. Allocate a new bucket $i_z$, set $i_j = i_z = i$
         3. Rehash entries in $j$ into $i_j$ and $i_z$
         4. Attempt to re-insert. But this step may cause additional bucket splits in some cases
         5. If all records in $j$ are using the same search-key value, splitting would not help, and a normal overflow bucket is used
      2. If $i > i_j$, then the system can split $i_j$ without increasing $i$
         1. Allocate a new bucket $i_z$, set $i_j = i_z = i_j + 1$
         2. Rehash entries in $j$
         3. Attempt to re-insert.

##### Deletion

1. Look up bucket $j$
2. Remove the search-key and the record
3. Coalesce if possible

#### Pros and Cons

- Advantage
  - Always access only 1 hash block to lookup a record
- Disadvantage
  - The size of the hash table will double each time it is extended

#### Summary of Hash Index

- $O(1)$ query complexity on average
- Supports point query, but does not support ranged query
- Useful for temporary files created during query processing
  - e.g. Hash Join
