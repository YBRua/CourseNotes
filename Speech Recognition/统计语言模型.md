# 统计语言模型

[TOC]

## 概述

### 回顾：统计语音识别的数学框架

$$\hat{W} = \arg\max_W \mathbb{P}[W|O] = \arg\max_W \mathbb{P}[O|W]\mathbb{P}[W]$$
其中$W=[w_1,\dots,w_N]$通常情况下表示一个**词序列**。

### 词表

- 语言模型建模需要定义一个词表。
- 搜索空间随词表大小呈指数增长。
- 在词表中无法找到的词称为集外词（Out-Of-Vocabulary）

### 基于规则的语法网络

- 一个语法网络是一个**加权有限状态机**。
- 语法间包括词之间的回环、替代、重复等。
- 状态机的边上赋有权重，对于经常观测到的路径权重较高。
- 难以手工制定大量语法规则
- 自然语言通常情况下不合语法

### 统计语言模型概述

- 完全数据驱动
- 适合自然语音
$$ \mathbb{P}[W] = \mathbb{P}[w_1,w_2,\dots,w_N] = \prod_{k=1}^{K+1} \mathbb{P}[w_k|w_1,\dots,w_{k-1}] $$

## N-gram语言模型

### N-gram语言模型概述

N-gram模型近似地表示在一个较短历史情况下的条件概率（序列最长为N个）
$$ \mathbb{P}[w_k|w_1,\dots,w_{k-1}] = \mathbb{P}[w_k|w_{k-1},\dots,w_{k-n+1}] $$
给定的一个当前词的概率仅依赖于这个词之前的$N-1$个前继词

- Unigram: $\mathbb{P}[W] = \mathbb{P}[w_k]$ （不依赖前继词）
- Bigram: $\mathbb{P}[W] = \mathbb{P}[w_k|w_{k-1}]$ （仅依赖于前一个词）
- Trigram: $\mathbb{P}[W] = \mathbb{P}[w_k|w_{k-1},w_{k-2}]$
- Quadrigram: $\mathbb{P}[W] = \mathbb{P}[w_k|w_{k-1},w_{k-2},w_{k-3}]$

### 文本归一化

转换训练文本使之成为纯语言，移除和语言不相关的部分。一般包括

- 移除标点
- 日期、货币、数字归一化（统一格式）
- 缩写词的归一化（统一格式）

### 最大似然参数估计

- 数据：词序列 $W_1^N = [w_1,\dots,w_N]$
- 模型：$\mathbb{P}[w_k|W^{k-1}_{k-n+1}], \quad w \in \mathcal{V}$，其中$ \mathcal{V} $是词表。

进行参数估计时，最大化对数似然函数
$$
\begin{align*}
&\hat{\mathbb{P}}[v|y] = \arg\max_{\mathbb{P}[v|y]} \sum_{v\in\mathcal{V}}\sum_{y\in\mathcal{Y}}C(y,v)\log\mathbb{P}[v|y]\\
&\text{s.t.} \quad \sum_{v\in\mathcal{V}}\mathbb{P}[v|y]=1
\end{align*}
$$
这是6一个等式约束的优化问题，可以用拉格朗日乘子法求解。
$$ \hat{\mathbb{P}}[v|y] = \frac{C(y,v)}{C(y)} $$
其中
$$ C(y) = \sum_{v\in\mathcal{V}}C(y,v) $$

### 模型评估

- 理想：语音识别的结果与词错误率之差
- 实际：在预留的文本数据集上预测

### 评价指标

#### 交叉熵 Cross-Entropy

$$  $$

#### 困惑度 Perplexity

$$ \mathrm{PPL} = 2^{-\mathcal{L}(\theta)} = \left( \prod_{k=1}^K\frac{1}{\mathbb{P}[w_k|W^{k-1}_{k-n+1}]} \right)^{\frac{1}{k}} $$

### 数据稀疏与零概率

训练集上未见过的词与词之间的关系会导致无穷大的Perplexity，训练集上低频次的关系也可能导致不可靠的估计。

#### 折扣法 Discounting

解决零概率的一种方法。将一些概率值“重新分配”到未见过的N-gram上。

#### Backing-off and Interpolation

解决零概率的一种方法，递归地回退到较低阶的N-gram模型上，直到得到一个较为鲁棒的概率估计。
