\chapter{Probability Models}
Write something here.
\newpage
\section{Multivariable Gaussian Distributions}
\subsection{Product of Experts}
\[ p(x) = \frac{1}{Z}\prod_{m=1}^M p_m(x) \]
Is (informally) equivalent to the intersection of Gaussian distributions.
\begin{remark}
    Product of Experts is beyond the scope of this course.
\end{remark}
\subsection{Mixture of Experts}
Weighted sum of multiple distributions.
\[ p(x) = \sum_{m=1}^M c_m p_m(x) \]
\begin{remark}
    Weighted sum of Gaussians is not necessarily a Gaussian. So mixture of Gaussians can be used to ``fit'' different distributions.
\end{remark}

\section{Gaussian Mixture Model}
\subsection{Sampling from a GMM}
\begin{enumerate}
    \item Sample $c_m$ from $c_1,c_2,\dots,c_M$
    \item Sample data from $\mathcal{N}(x|\mu_m,\Sigma_m)$
\end{enumerate}

\section{Some Information Theory}
\textbf{Information} contains \textbf{uncertainty}. The more uncertain something is, the more information is needed to eliminate its uncertainty.

\begin{definition}
    The information in discrete variable is
    \[ I(x) = -\log_2 P(x) \]
\end{definition}
\begin{definition}[Entropy]
    The 
    \[ H = \mathbb{E}[-\log_2 P(x)] = -\sum_{x\in X}P(x)\log_2 P(x) \]
    \[ H = \mathbb{E}[I(x) = -\int_{-\infty}^{\infty}p(x)\ln(p(x))\mathrm{d}x] \]
\end{definition}
\begin{remark}
    \begin{itemize}
        \item Entropy is a function of distribution $p(x)$.
        \item Is the expectation of information of random variables.
    \end{itemize}
\end{remark}

\subsection{Entropy of common distriutions}

\subsection{Conditional Entropy}
\begin{definition}
    \[ H[y|x] = \sum_{x'}P(x)H(y|x=x') = -\sum_{x}P(x)\sum_yP(y|x)\ln P(TooFast) \]
\end{definition}