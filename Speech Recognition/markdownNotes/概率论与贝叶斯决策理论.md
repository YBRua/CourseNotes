# 目录

[TOC]

## 联合概率与多元随机变量

### 联合概率

- **联合概率**是所有随机变量 $x_i \in \mathcal{X}_i, i = 1,2,\dots,d$ 同时发生的概率。
- 常将联合概率合并为多元随机变量 $\mathbf{x} \in \mathcal{X}$。
$$\mathbf{x} = \begin{bmatrix} x_1\\x_2\\\vdots\\x_d \end{bmatrix}$$

### 多元随机变量的统计值

#### 均值

$$\bm{\mu} = \mathbb{E}[\mathbf{x}] = \int_{\mathbf{x}}\mathbf{x}\cdot p(\mathbf{x})\mathrm{d}\mathbf{x}$$

#### 协方差

$$\Sigma = \mathbb{E}[(\mathbf{x} - \bm{\mu})(\mathbf{x}-\bm{\mu})^T]=\mathbb{E}[\mathbf{x}\mathbf{x}^T]-\bm{\mu}\bm{\mu}^T = (\sigma_{ij})$$

- $\sigma_{xy} = \mathbb{E}[(x-\mu_x)(y-\mu_y)]$
- $\sigma_{xx} = \sigma_x^2$

#### 相关系数

$$\rho = \frac{\sigma_{xy}}{\sigma_x\sigma_y}$$

- $-1 \le \rho \le 1$
- $\rho = 0$ 表示两个变量不相关
- 独立 $\Rightarrow$ 不相关
- 对高斯分布： 独立 $\Leftrightarrow$ 不相关

### 多元高斯分布

$d$ 维多元高斯分布的形式为
$$p(\mathbf{x}) = \frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^{\frac{1}{2}}}\exp\left\{ -\frac{1}{2}(\mathbf{x}-\bm{\mu})^T\Sigma^{-1}(\mathbf{x}-\bm{\mu}) \right\}$$

若 $\Sigma$ 为对角阵，则
$$ p(\mathbf{x}) = \prod_{i=1}^d \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left\{-\frac{(x-\mu_i)^2}{2\sigma_i^2}\right\} $$

- 多元高斯分布的边缘概率分布仍是高斯分布
- 任何一个子集的联合边缘分布仍是高斯分布
- 条件分布 $p(x_i|x_j)$ 仍是高斯分布
- $\mathbf{x}$ 的线性变换 $A\mathbf{x}+\mathbf{b}$ 仍是高斯分布，其均值为 $A\bm{\mu} + \mathbf{b}$，方差为 $A \Sigma A^T$

#### 多元高斯分布的参数估计

##### 充分统计量

- $\Gamma_0 = \sum_{i=1}^N1 = N$
- $\Gamma_1 = \sum_{i=1}^N\mathbf{x_n}$
- $\Gamma_2 = \sum_{i=1}^N\mathbf{x_n}\mathbf{x_n}^T$

##### 最大似然估计值

- $\hat{\bm{\mu}} = \Gamma_1/\Gamma_0$
- $\hat{\Sigma} = \Gamma_2/\Gamma_0 - \hat{\bm{\mu}}\hat{\bm{\mu}}^T$

## 贝叶斯决策理论

摸了。

## 高斯混合模型 Gaussian Mixture Model

将 $M$ 个高斯分布加权求和合并为一个更复杂的模型
$$p(\mathbf{x}) = \sum_{i=1}^M p(m)p(\mathbf{x}|m) = \sum_{i=1}^Mc_m\mathcal{N}(\mathbf{x}|\bm{\mu}_m,\Sigma_m)$$

- $c_m = p(m)$
- $\mathcal{N}(\mathbf{x}|\bm{\mu}_m,\Sigma_m) = p(\mathbf{x}|m)$
- $c_m$、$\bm{\mu}_m$ 和 $\Sigma_m$ 为待估计的参数
- 令 $p(z_m = 1) = c_m$，其中$z_m$为one-hot编码，则$z_m$用于指示“哪个高斯分量用于生成模型”。$z_m$不能被直接观测到，因此称为**隐变量**。

### 隐变量的估计

#### 硬分配

对每个样本，假设已知（或者强制分配）该样本所属的高斯分量，则可以使用常规的最大似然估计等参数估计方法估计出高斯混合模型的各项参数。

#### 软分配

解决了实际应用中隐变量事实上不能被观测到的问题，但是会导致对数似然函数过于复杂，难以优化。

### 期望最大化算法 Expectation-Maximization

#### Jensen不等式

假设$f$是一个凸函数，$X$是一个随机变量，则
$$f(\mathbb{E}[X]) \le \mathbb{E}[f(X)]$$
若$f$是凹函数则不等式反向

#### EM算法的一般形式

- 我们希望建模 $p(x,z;\theta)$，但$z$为隐变量，只能观测到$x$
- 假设 $z_n$ 服从分布 $\gamma_n(m) = p(z_n=m|x_n;\hat{\theta})$
$$
\begin{align*}
\mathcal{L}(\theta) &= \sum_{n=1}^N\log p(x_n;\theta)\\
&= \sum_{n=1}^N\log\sum_{m=1}^M p(x_n, z_m; \theta)\\
&= \sum_{n=1}^N\log\sum_{m=1}^M\gamma_n(m)\frac{p(x_n,m;\theta)}{\gamma_n(m)} \quad \text{(先乘后除$\gamma_n(m)$)}\\
&= \sum_{n=1}^N\log\mathbb{E}\left[\frac{p(x_n,m;\theta)}{\gamma_n(m)}\right] \quad \text{(期望的定义)}\\
&\ge \sum_{n=1}^N\mathbb{E}\left[\log\left(\frac{p(x_n,m)}{\gamma_n(m)}\right)\right] \quad \text{(Jensen不等式)}\\
&= \sum_{n=1}^N\sum_{m=1}^M\gamma_n(m)\log\frac{p(x_n,m;\theta)}{\gamma_n(m)}\\
&= \sum_{n=1}^N\mathrm{H}\left(\gamma_n(m)\right) + \mathcal{Q}(\theta;\hat{\theta})
\end{align*}
$$

- $\mathrm{H}(\cdot)$是熵
- 定义$\mathcal{Q}(\theta;\hat{\theta})=\sum_{n=1}^N\sum_{m=1}^Mp(m|x_n;\theta)\log p(x_n,m;\theta)$
- 取$\gamma_n(m) = p(z_n=m|x_n;\hat{\theta})$时，Jensen不等式可以取等
- $\sum_{n=1}^N\mathrm{H}\left(\gamma_n(m)\right) + \mathcal{Q}(\theta;\hat{\theta})$给出了$\mathcal{L}(\theta)$的一个下界，EM算法迭代地最大化这一下界，进而最大化$\mathcal{L}(\theta)$。

Jensen不等式在$\hat{\theta}$处取等，因此
$$\mathcal{Q}(\theta;\hat{\theta}) \ge \mathcal{Q}(\hat{\theta};\hat{\theta})$$

$$\sum_{n=1}^N\mathrm{H}\left(\gamma_n(m)\right) + \mathcal{Q}(\theta;\hat{\theta}) \ge \sum_{n=1}^N\mathrm{H}\left(\gamma_n(m)\right) + \mathcal{Q}(\hat{\theta};\hat{\theta})$$

$$\mathcal{L}(\theta) \ge \mathcal{L}(\hat{\theta})$$

因此EM算法的一般形式为

- *E-Step*: $\gamma_n(m) \leftarrow p(m|x_n;\theta)$
- *M-Step*: $\theta = \arg\max_{\theta} \sum_{n=1}^N\sum_{m=1}^M\gamma_n(m)\log\frac{p(x_n,m;\theta)}{\gamma_n(m)}$

#### 使用EM计算GMM参数

- 初始化参数
- 重复直到收敛：
   1. *E-Step*：对每个样本 $x_n$，更新 $\gamma_n(m) = p(m|x_n)$
   2. *M-Step*：对每个高斯分量 $m$，更新 $\mathcal{N}_m$的参数

##### E-Step

如果假设$m$服从多项式分布，则相关更新公式如下：
$$\gamma_n(m) = p(z_n=m|x_n) = \frac{p(x_n|z_n=m)p(z_n=m)}{\sum_kp(x_n|z_n=k)p(z_n=k)}$$
在*E-Step*，上式右侧所有的值都是已知且可计算的。

##### M-Step

目标函数是
$$\begin{align*}
Q &= Const\\
&+ \sum_{n=1}^N\sum_{m=1}^M\gamma_n(m)\log c_m\\
&- \frac{1}{2} \sum_{n=1}^N\sum_{m=1}^M\gamma_n(m)\left( \log|\Sigma_m|+(x_n-\mu_m)^T\Sigma_{m}^{-1}(x_n-\mu_m) \right)
\end{align*}$$

令
$$\Gamma^{(m)}_0 = \sum_{n=1}^N\gamma_n(m) \quad \Gamma^{(m)}_1 = \sum_{n=1}^N\gamma_n(m)\mathbf{x_n} \quad \Gamma_2^{(m)} = \sum_{n=1}^N \gamma_n(m)\mathbf{x}_n\mathbf{x}_n^T$$

则
$$\bm{\mu}_m = \frac{\Gamma^{(m)}_1}{\Gamma^{(m)}_0}$$

$$\Sigma_m = \frac{\Gamma^{(m)}_2}{\Gamma^{(m)}_0}-\bm{\mu}_m\bm{\mu}^T_m$$

$$c_m = \frac{\Gamma^{(m)}_0}{\sum_m\Gamma^{(m)}_0}$$
