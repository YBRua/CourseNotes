# 隐马尔科夫模型 Hidden Markov Models

[TOC]

## 孤立词语音识别

### 确定化方法

1. 提取待识别词的特征序列 $O=[o_1,o_2,\dots,o_T]$
2. 计算 $O$ 与所有已知参考序列 $V_k$ 之间的距离
3. 找距离最近的作为识别结果

#### 动态时间规整 DTW

Dynamic Time Warping

参考算法设计与分析的编辑距离。

DTW算法计算序列 $X \in \mathbb{R}^M$ 和 $Y \in \mathbb{R}^N$ 的代价，定义一个 $M \times N$ 的矩阵 $D$

$$ D(i,j) = \min
\begin{cases}
D(i-1,j) + C_I \quad &\text{Insertion}\\
D(i,j-1) + C_D \quad &\text{Deletion}\\
D(i-1,j-1) + d(x,y) &\text{Substitution}
\end{cases} $$

其中 $C_I$、$C_D$是插入和删除的代价，$d(x,y) =\sqrt{(x-y)^T(x-y)}$是替换的代价

#### 基于DTW的语音识别

1. 训练：为每个词录制样例语音，作为词的模板
2. 解码：录制测试语音，然后通过DTW计算距离

### 概率模型方法

#### 统计语音识别的数学框架

$$ \hat{W} = \arg\max_W \mathbb{P}[W|O] = \arg\max_W \mathbb{P}[O|W]\mathbb{P}[W] $$

- $\mathbb{P}[O|W]$是声学模型
- $\mathbb{P}[W]$是语言模型

#### 孤立词识别

假设所有候选词都是等似然的，目标是找到声学似然最高的词

$$k = \arg\max_i p(O|w_i)$$

- $p(O|w_i)$需要完成特征向量长度的归一化
- 常用的表示$p(O|w_i)$的模型为隐马尔可夫模型

## 马尔科夫链

### 回顾随机过程

- （一阶）马尔科夫链
  - $X = [X_1, X_2, \dots, X_T]$
  - $\mathcal{S} = \{ s_1, s_2, \dots, s_M \}$
- 初始概率分布 $\pi$
- 状态转移矩阵 $P$

### 高阶马尔科夫链

以二阶为例，将**当前状态**和**过去状态**的**组合**作为转移矩阵的行（$|\mathcal{S}|^2$行）

## 隐马尔科夫模型

隐马尔科夫模型用于建模**每一个状态的输出是随机的**情况。

- 马尔科夫链：每个状态的输出确定；状态序列与观测序列完全一致。
- 隐马尔科夫模型：每个状态的输出随机，由一个概率分布决定

### 模型参数

- **隐含状态** $Q=\{q_i, 1 \le i \le S\}$
- **转移概率** $A=\{a_{ij}, 1 \le i,j \le S\}$
- **状态输出分布** $B(o) = \{b_j(o), 1 \le j \le S\}$ 表示在状态 $j$ 下观测到 $o$ 的概率。
- **观测值** $o_t, 1 \le t \le T$

### 基本假设

- **马尔科夫假设**：下一状态只与当前状态有关

$$\mathbb{P}[q_{t+1}|q_{t},q_{t-1},\dots,q_1] = \mathbb{P}[q_{t+1}|q_{t}] = A(q_t, q_{t+1}) $$

- **条件独立性假设**：给定当前状态 $q_t$，观测值 $o_t$ 与历史状态和历史观测值无关

$$\mathbb{P}[o_t|q_t,q_{t-1},\dots,q_1] = \mathbb{P}[o_t|q_t] =b_{q_t}(o_t) $$

因此

$$\mathbb{P}[o_1,\dots,o_T|q_1,\dots,q_T] = \prod_{t=1}^T\mathbb{P}[o_t|q_t] $$

### 隐马尔可夫模型中有两个序列

- **状态序列**：隐含的、对观测者不可见的序列，表示该随机过程的实际状态
- **观测序列**：实际观测到的特征序列

### 隐马尔可夫模型包含两个步骤

- 状态转移
- 每个状态发射(emit)一个观测值

### HMM用于孤立词语音识别的三个关键问题

1. **似然分数计算**：模型参数已知，如何计算一个观测序列 $O$ 的整体似然
$$p(O|\theta)$$
2. **解码**：模型参数已知、观测序列 $O$ 已知，如何确定最可能产生该观测序列的状态序列 $Q$？
$$\hat{Q} = \max_Q p(O,Q|\theta)$$
3. **参数估计**：模型参数未知、给定部分训练数据，如何确定模型参数？
$$\hat{\theta} = \max_\theta \mathcal{L}(\theta)$$

#### 似然度计算

- 状态序列 $Q$ 已知的情况：
$$p[O|Q,\theta] = \prod_{t=1}^Tp[o_t|q_t,\theta]$$
- 状态序列 $Q$ 未知的情况：
$$\begin{align*}
p[O;\theta] &= \sum_{Q}p[O,Q|\theta] \quad \text{(全概率公式)}\\
&= \sum_Q p[O|Q,\theta]p[Q,\theta] \quad \text{(贝叶斯公式)}\\
&=\sum_Q \left( \prod_{t=1}^T p(o_t|q_t,\theta)p(q_t|q_{t-1},\theta) \right) \quad \text{(模型假设)}
\end{align*}$$
其中 $p[Q,\theta] = \prod_{t=1}^T \mathbb{P}[q_t|q_{t-1},\theta]$ 是每一个状态序列 $Q$ 的先验，且 $\mathbb{P}[q_0]=1$

##### 前向算法

给定长度为 $T$ 的观测序列 $O_1^T = [o_1,\dots,o_T]$，其似然为
$$p(O_1^T|\theta) = \sum_Qp(O_1^T, Q, q_0=1, q_{T+1}=N|\theta)$$

其中 $O_1^T$ 表示自 $t=1$ 起至 $t=T$ 的观测序列 ；$q_0$ 和 $q_{T+1}$ 分别是初始和结束的两个非发射状态。

则似然度可以通过递归计算
$$
\begin{align*}
\alpha_j(t) &\triangleq p[O_1^t, q_t=j|\theta]\\
&= \sum_{i=1}^{N-1}p[O_1^t,q_t=j,q_{t-1}=i|\theta] \quad \text{(全概率公式)}\\
&= \sum_{i=1}^{N-1}p[O_1^{t-1},o_t,q_t=j,q_{t-1}=i|\theta]\\
&= \sum_{i=1}^{N-1}p[o_t|O_1^{t-1},q_t=j,q_{t-1}=i|\theta]\\
&\quad \cdot p[q_t=j|O_1^{t-1},q_{t-1}=i|\theta]\\
&\quad \cdot p[O_1^{t-1},q_{t-1}=i|\theta] \quad \text{(两次乘法定理)}\\
&= \sum_{i=1}^{N-1}p[o_t|q_t,\theta] \cdot p[q_t=j|q_{t-1}=i,\theta] \cdot p[O_1^{t-1},q_{t-1}=i,\theta]\quad\text{(模型假设)}\\
&= b_j(o_t)\sum_{i=1}^{N-1}a_{ij}\alpha_i(t-1)
\end{align*}
$$

在计算得到 $\alpha_j(T)$ 后，
$$ p[O_1^T|\theta] = \sum_{i=1}^{N-1}a_{iN}\alpha_i(T) $$

**初始条件**：
初始状态一定是 $q_0=1$，结束状态一定是 $q_{T+1}=N$。

- $q_0=1$
- $\alpha_1(0)=1$
- $\alpha_j(0) = 0 \quad \forall j \neq 1$

或者，在给定了初始分布的情况下，

- $\alpha_j(1) = \pi_0(i) \cdot b_j(o_1)$

**注**： 如果不使用前项算法而暴力枚举所有可能的隐层状态 $Q$，则复杂度是关于隐层状态个数的指数函数 $O(nm^n)$，其中 $n$ 是观测序列的长度，$m$ 是隐层状态的个数，前向算法将复杂度减小到了 $O(nm^2)$。

#### 解码

##### Viterbi算法

给定观测 $O$，Viterbi算法寻找最有可能产生当前观测 $O$ 的隐层序列 $\hat{Q}$
$$\hat{Q} = \arg\max_Q p[O,Q,q_0=1,q_{T+1}=N|\theta]$$

令 $\phi_j(t)$ 表示长度为 $t$ 且末状态为 $j$ 的所有隐层序列 $Q_1^t$ 中的最大似然值
$$
\begin{align*}
\phi_j(t) &\triangleq \max_{Q_1^t} p[O_1^t,q_0=1,Q_1^{t-1},q_t=j|\theta]\\
&= \max_{1\le i \le N}\max_{Q_1^t}p[O_1^t,q_0=1,Q_1^{t-2},q_t=j,q_{t-1}=i|\theta]\\
&= \max_{1\le i \le N}\max_{Q_1^t}p[o_t|q_t=j,\theta]\cdot \mathbb{P}[q_t=j|q_{t-1}=i,\theta] \cdot p[O_1^{t-1},Q_1^{t-2},q_{t-1}=i|\theta]\\
&= b_j(o_t)\max_{1 \le i \le N} a_{ij}\phi_i(t-1)
\end{align*}
$$

**初始条件**：

- $q_0=1$
- $\phi_1(0)=1$
- $\phi_j(0)=0 \quad \forall j \neq 1$

或者，在给定了初始分布的情况下，

- $\phi_j(1) = \pi_0(i) \cdot b_j(o_1)$

**回溯**：
令 $q_j^{max}(t)$ 表示使当前的似然 $\phi_j(t)$ 取到最大值的状态
$$ q_j^{max}(t) = \arg\max_{1 \le i \le N}a_{ij}\phi_i(t-1) $$

则在 $t$ 时刻取到最大似然分数时，对应 $t-1$ 时刻的最优状态为
$$ \hat{q}_{t-1} = q_{\hat{q}_t}^{max}(t) $$

因此，算法最后给出的结果是
$$ \phi_N(T+1) = \max_{1 \le i \le N}a_{iN}\phi_i(T) $$

$$ q_N^{max}=\arg\max_{1 \le i \le N}a_{iN}\phi_i(T) $$

##### Viterbi算法的工程优化

**对数似然**：常被用于代替似然函数以防止浮点数运算溢出

**剪枝**：移除似然度较低的路径以保持较小的搜索空间

- **束剪枝**：移除似然度低于某个阈值 $\tau$ 的路径
- **直方图剪枝**：计算似然的直方图分布，选择分数最高的 $N$ 条路径

#### 参数估计：Baum-Welch算法

##### 概述

Baum-Welch算法是一种用于估计HMM参数的EM算法。它使用前向-后向算法在E-Step计算期望。

给定训练集 $(O^{(r)}, H^{(r)}=w^{(r)}$, 其中上标 $(r)$ 是训练集中第 $r$ 个样本。Baum-Welch算法估算HMM的模型参数 $\theta = (A, B, \pi)$。其中 $A=\{a_{ij}\}$ 是转移矩阵、$B= \{ b_j(o_i) \}$ 是状态发射矩阵，$\pi$ 是初始分布。
$$ \hat{\theta} = \arg\max_{\theta}p(O|\theta) $$

根据最大似然准则
$$ \mathcal{L}(\theta) = \sum_{i=1}^R \log p[O^{(r)}|\theta] = \sum_{r=1}^R\log\left( \sum_{Q}p[O^{(r)},Q|\theta] \right) $$

EM算法迭代地更新 $\hat{\theta}$
$$
\begin{align*}
\mathcal{L}(\theta) &= \sum_{r=1}^R\log\left( \sum_Q \mathbb{P}[Q|O^{(r)},\hat{\theta}]\frac{p[O^{(r)},Q|\theta]}{\mathbb{P}[Q|O^{(r)},\hat{\theta}]} \right)\\
&\ge \sum_{r=1}^R\sum_Q\mathbb{P}[Q|O^{(r)},\hat{\theta}]\log\left( \frac{p[O^{(r)},Q|\theta]}{\mathbb{P}[Q|O^{(r)},\hat{\theta}]} \right)\\
&= \sum_{r=1}^R \mathrm{H}\left( \mathbb{P}[Q|O^{(r)},\hat{\theta}] \right) + Q(\theta,\hat{\theta})
\end{align*}
$$

其中
$$ Q(\theta, \hat{\theta}) = \sum_{r=1}^R\sum_Q \mathbb{P}[Q|O^{(r)},\hat{\theta}]\log p[O^{(r)},Q|\theta] $$

考虑训练集中的某一个观测序列 $O^{(r)} = O_1^{T^{(r)}}$（为简便起见，省略 $T^{(r)}$ 的上标）

先考虑 $Q(\theta,\hat{\theta})$中的后一项 $\log$。
$$
\begin{align*}
\log p[O_1^T, Q_1^T|\theta] &= \log p[O_1^T|Q_1^T,\theta]\cdot p[Q_1^T,\theta] \quad\text{(全概率公式)}\\
&= \log \prod_{t=1}^T\mathbb{P}[o_t|q_t,\theta]\cdot\mathbb{P}[q_t|q_{t-1},\theta] \quad\text{(模型假设)}\\
&= \sum_{t=1}^T \log\mathbb{P}[o_t|q_t,\theta] + \sum_{t=1}^T \log\mathbb{P}[q_t|q_{t-1},\theta]
\end{align*}
$$

因此
$$ Q(\theta, \hat{\theta}) = \sum_{r=1}^R\sum_Q\mathbb{P}[Q|O,\hat{\theta}]\left( \sum_{t=1}^T\log p[o_t|q_t,\theta] + \sum_{t=1}^T\mathbb{P}[q_t|q_{t-1},\theta] \right) $$

上式展开后是两项求和，其中第一项求和与输出矩阵 $B$ 有关，而第二项求和与转移矩阵 $A$ 有关，因此
$$ Q(\theta, \hat{\theta}) \triangleq Q_A(\theta,\hat{\theta}) + Q_B(\theta, \hat{\theta}) $$

$Q_B$ 的推导较为简单，因此先从 $Q_B$ 开始，后续推导中先忽略最外层对所有样本 $r=1:R$ 的求和，而只考虑某个观测序列 $O_1^T$ 及其隐层序列 $Q_1^T$。

注意到 $\sum_Q$ 是对所有可能的隐层状态的暴力枚举，这一求和与对每一时刻 $q_t$ 分别求和是等价的，因此我们可以将序列上的求和转换为点上的求和
$$
\begin{align*}
&\sum_{Q_1^T}\mathbb{P}[Q_1^T|O_1^T,\hat{\theta}]\left(\sum_{t=1}^T\log p[o_t|q_t,\theta]\right)\\
&= \sum_{t=1}^T\sum_{Q_1^T} \mathbb{P}[Q_1^T|O_1^T,\hat{\theta}]\log p[o_t|q_t,\theta]\\
&= \sum_{t=1}^T\sum_{q_1}\cdots\sum_{q_T}\mathbb{P}[q_1,\dots,q_T|O_1^T,\hat{\theta}]\log p[o_t|q_t,\theta]\\
&= \sum_{t=1}^T\sum_{q_t}\left( \sum_{q_1}\cdots\sum_{q_{t-1}}\sum_{q_{t+1}}\cdots\sum_{q_T}\mathbb{P}[q_t, Q_1^{t-1},Q_{t+1}^T|O_1^T,\hat{\theta}]\log p[o_t|q_t,\theta] \right)\\
&= \sum_{t=1}^T\sum_{j=1}^N \mathbb{P}[q_t=j|O_1^T,\hat{\theta}]\log p[o_t|q_t=j,\theta]
\end{align*}
$$

令 $\gamma_j(t)$ 表示“给定观测序列和模型参数，在 $t$ 时刻处在状态 $j$ 的概率”
$$ \gamma_j(t) = \mathbb{P}[q_t=j|O^{(r)},\hat{\theta}] $$

则
$$ Q_B = \sum_{r=1}^R\sum_{t=1}^{T^{(r)}}\sum_{j=1}^N \gamma_j(t)\log p[o_t|q_t=j,\theta] $$

类似地，对 $Q_A$ 有
$$
\begin{align*}
&\sum_{Q_1^T}\mathbb{P}[Q_1^T|O_1^T,\hat{\theta}]\sum_{t=1}^T\log \mathbb{P}[q_t|q_{t-1},\theta]\\
&= \sum_{q_1}\cdots\sum_{q_{t-1}}\sum_{q_t}\cdots\sum_{q_T}\sum_{t=1}^T\mathbb{P}[Q_1^T|O_1^T,\hat{\theta}]\log\mathbb{P}[q_t|q_{t-1},\theta]\\
&= \sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^T\mathbb{P}[q_t=j,q_{t-1}=i|O_1^T,\hat{\theta}]\log\mathbb{P}[q_t=j|q_{t-1}=i,\theta]
\end{align*}
$$

令 $\gamma_{ij}(t)$ 表示“给定观测序列和模型参数，在 $t$ 和 $t-1$ 时刻状态分别为 $j$ 和 $i$”的概率。
$$ \gamma_{ij}(t) =\mathbb{P}[q_t=j,q_{t-1}=i|O_1^T,\hat{\theta}] $$
则
$$ Q_A = \sum_{r=1}^R\sum_{t=1}^{T^{(r)}}\sum_{i=1}^N\sum_{j=1}^N \gamma_{ij}(t)\log\mathbb{P}[q_t=j|q_{t-1}=i,\theta] $$

##### 前向概率

与Viterbi算法类似，前向概率 $\alpha_j(t)$ 定义为在状态 $q_t=j$ 时观测到前序观测序列 $O_1^t = [o_1,\dots,o_t]$的似然。
$$ \alpha_j(t) = p[O_1^t,q_t=j|\theta] = b_j(o_t)\sum_{i=1}^{N-1}a_{ij}\alpha_i(t-1) $$

##### 后向概率

后向概率 $\beta_j(t)$ 定义为给定状态 $q_t=j$，后序序列为 $O_{t+1}^T=[o_{t+1},\dots,o_T]$的似然。
$$
\begin{align*}
\beta_j(t) &= p[O_{t+1}^T|q_t=j,\theta]\\
&= \sum_{i=1}^{N-1}p[o_{t+1},O_{t+2}^T,q_{t+1}=i|q_t=j,\theta]\\
&= \sum_{i=1}^{N-1}p[o_{t+1}|q_{t+1}=i,\theta]\mathbb{P}[q_{t+1}=i|q_t=j]p(O_{t+2}^T|q_{t+1}=i)\\
&= \sum_{i=1}^{N-1}b_i(o_{t+1})a_{ji}b_{i}(t+1)
\end{align*}
$$

其中边界条件为

- $\beta_i(T)=a_{jN}$

##### 全序列似然

$$ p[q_t = i, O_1^T] = \alpha_i(t)\beta_i(t) $$

整个序列的似然为
$$ p[O_1^T|\theta] = \alpha_N(T+1) = \beta_1(0) = \sum_{i=1}^N\alpha_i(t)\beta_i(t) $$

在得到了前向和后向概率后，可以进行进一步的参数估计。

##### 转移概率估计

**E-Step**
$$
\begin{align*}
\gamma_{ij}(t) &= \mathbb{P}[q_t=j,q_{t-1}=i|O_1^T,\hat{\theta}]\\
&= \frac{p[q_t=j,q_{t-1}=i,O_1^T|\hat{\theta}]}{p[O_1^T|\hat{\theta}]}
\end{align*}
$$

$$
\begin{align*}
Numerator &= p[O_{t+1}^T|q_t,q_{t-1},O_{1}^{t-1},o_t] \cdot p[o_t|q_t,O_1^{t-1},q_{t-1}]\\
&\quad \cdot p[q_t|q_{t-1},O_1^{t-1}] \cdot p[O_1^{t-1},q_{t-1}]\\
&=p[O_{t+1}^{T}|q_t]\cdot p[o_t|q_t]\cdot p[q_t|q_{t-1}]\cdot p[O_1^{t-1},q_{t-1}]\\
&= \beta_j(t)b_j(o_t)a_{ij}\alpha_i(t-1)
\end{align*}
$$

$$\begin{align*}
Denominator &= \sum_{i=1}^N\alpha_i(t)\beta_i(t)\\
&= \sum_{i=1}^N\mathbb{P}[O_1^t,q_t=i]\mathbb{P}[O_{t+1}^T|q_t=i]\\
&= \sum_{i=1}^N\mathbb{P}[O_1^t，q_t=i]\sum_{j=1}^N\mathbb{P}[O_{t+1}^T,q_{t+1}=j|q_t=i]\\
&= \sum_{i=1}^N\alpha_i(t)\sum_{j=1}^N\mathbb{P}[o_{t+1}|q_{t+1}=j]\mathbb{P}[O_{t+2}^T|q_{t+1}=j]\mathbb{P}[q_{t+1}=j|q_t=i]\\
&= \sum_{i=1}^N\sum_{j=1}^N\alpha_i(t)a_{ij}\beta_j(t+1)b_j(o_{t+1})
\end{align*}$$

故
$$ \gamma_{ij} = \frac{\beta_j(t)b_j(o_t)a_{ij}\alpha_i(t-1)}{\sum_{i=1}^N\alpha_i(t)\beta_i(t)} $$

或者，若不使用 $t$ 和 $t-1$ 而使用 $t$ 和 $t+1$
$$ \xi_{ij}(t) \triangleq \mathbb{P}[q_t=i,q_{t+1}=j,O_1^T|\theta] = \frac{\alpha_i(t)a_{ij}\beta_j(t+1)b_j(o_{t+1})}{\sum_{i=1}^N\sum_{j=1}^N\alpha_i(t)a_{ij}\beta_j(t+1)b_j(o_{t+1})} $$

**M-Step**
使用上一步计算得到的 $\gamma_{ij}(t)$ 估算新一轮迭代的 $a_{ij}$
$$\begin{align*}
 a_{ij} = &\arg\max_{a_{ij}} \sum_{r=1}^R\sum_{t=1}^T\sum_{i=1}^N\sum_{j=1}^N \gamma_{ij}(t)\log a_{ij}\\
 &\text{s.t.}\quad \sum_{j=1}^Na_{ij} = 1
\end{align*}$$

使用拉格朗日乘子法
$$ L(a_{ij},\lambda) = \sum_{r=1}^R\sum_{t=1}^T\sum_{i=1}^N\sum_{j=1}^N \gamma_{ij}(t)\log a_{ij} - \lambda\left( \sum_{j=1}^Na_{ij} - 1 \right) $$

求导、置零、计算后得到
$$ a_{ij} = \frac{1}{\lambda}\sum_{r=1}^R\sum_{t=1}^T\gamma_{ij}(t) \qquad \lambda = \sum_{j=1}^N\sum_{r=1}^R\sum_{t=1}^T \gamma_{ij}(t) $$

或，在使用 $\xi_{ij}(t)$ 时
$$ a_{ij} = \frac{\sum_{r=1}^R\sum_{t=1}^{T-1}\xi_{ij}(t)}{\sum_{r=1}^R\sum_{t=1}^{T-1}\gamma_i(t)} $$

其中 $\gamma_i(t)$ 的计算方法在下一部分。

##### 输出概率估计

**E-Step**
$$\begin{align*}
\gamma_j(t) &= \mathbb{P}[q_t=j|O_1^T,\hat{\theta}]\\
&= \frac{p[q_t=j,O_1^T|\hat{\theta}]}{p[O_1^T|\hat{\theta}]}\\
&= \frac{\alpha_j(t)\beta_j(t)}{\sum_{i=1}^N \alpha_i(t)\beta_i(t)}
\end{align*}$$

**M-Step**
输出概率的M-Step根据HMM模型的具体假设有所差异。

###### 离散HMM

目标函数
$$ Q_B = \sum_{r=1}^R\sum_{t=1}^T\sum_{j=i}^N \gamma_j(t)\log b_j(s) $$

约束条件
$$ \sum_{s=1}^N b_j(s) = 1 $$

注意到目标函数中只有 $\gamma_j(t)$ 的序列 $O_1^T$ 中 $t$ 时刻的观测是状态 $s$ 时，即 $o_t = s$ 时，概率才非零，因此
$$ Q_B = \sum_{r=1}^R\sum_{t=1}^T\sum_{j=1}^N\mathbb{I}[o_t=s]\gamma_j(t)\log b_j(s) $$

同样使用拉格朗日乘子法
$$ L(b_j(s), \lambda) = \sum_{r=1}^R\sum_{t=1}^T\sum_{j=1}^N\mathbb{I}[o_t=s]\gamma_j(t)\log b_j(s) - \lambda\left(\sum_{s=1}^N b_j(s) - 1\right) $$

计算得到
$$ b_j(s) = \frac{\sum_{r=1}^R\sum_{t=1}^T\mathbb{I}[o_t=s]\gamma_j(t)}{\sum_{r=1}^R\sum_{t=1}^T\gamma_j(t)} $$

###### 高斯HMM

$$ b_j(o_t) = \mathcal{N}(o_t|\mu_j,\Sigma_j) $$

目标函数
$$ Q_B = -\frac{1}{2}\sum_{r=1}^R\sum_{t=1}^T\sum_{j=1}^N\gamma_j(t)\left( \log|\Sigma_j| + (o_t-\mu_j)^T\Sigma^{-1}(o_t-\mu_j) \right) $$

求 $\mu_j$
$$\begin{align*}
\frac{\partial Q}{\partial \mu_j} &= \sum_{r}\sum_t \gamma_j(t)\Sigma^{-1}_j(o_t-\mu_j):=0\\
&\Rightarrow \sum_r\sum_t\gamma_j(t)o_t = \sum_r\sum_t\gamma_j(t)\mu_j \quad \text{(注意到 $\Sigma_j$ 正定)}\\
&\Rightarrow \mu_j = \frac{1}{\Gamma_j}\sum_r\sum_t\gamma_j(t)o_t
\end{align*}$$

其中
$$ \Gamma_j = \sum_{r=1}^R\sum_{t=1}^T\gamma_j(t) $$

求 $\Sigma_j$，先对目标函数做变换
$$ Q_B = \frac{1}{2}\sum_r\sum_t\gamma_j(t)\log|\Sigma^{-1}| - \frac{1}{2}\sum_r\sum_t(o_t-\mu_j)^T\Sigma^{-1}(o_t-\mu_j) $$

对 $\Sigma^{-1}$ 求导
$$\begin{align*}
\frac{\partial Q}{\partial \Sigma^{-1}} &= \frac{1}{2}\sum_r\sum_t\gamma_j(t)\Sigma_j - \frac{1}{2}\sum_r\sum_t(o_t-\mu_j)(o_t-mu_j)^T :=0\\
&\Rightarrow \Sigma_j = \frac{1}{\Gamma_j}\sum_r\sum_t(o_t-\mu_j)(o_t-\mu_j)^T
\end{align*}$$

###### GMM-HMM

> The root of all evil.

GMM-HMM模型假设每个隐层状态的输出概率可以用一个高斯混合模型来建模。
$$ b_j(o_t) = \sum_{m=1}^Mc_{jm}b_{jm}(o_t) = \sum_{m=1}^Mc_{jm}\mathcal{N}(o_t|\mu_{jm},\Sigma_{jm}) $$

则
$$\begin{align*}
\log b_j(o_t) &= \log\sum_{m=1}^Mc_{jm}b_{jm}(o_t)\\
&= \log\sum_{m=1}^Mc_{jm}\gamma_m(t)\frac{b_{jm}(o_t)}{\gamma_m(t)}\\
&\ge \sum_{m}\gamma_m(t)\log c_{jm}\frac{b_{jm}(o_t)}{\gamma_m(t)} \quad \text{(Jensen 不等式)}
\end{align*}$$

其中
$$ \gamma_m(t) = \mathbb{P}[g_t = m|O_1^t] $$

表示 $t$ 时刻的观测样本来自第 $m$ 个高斯成分的概率。

**目标函数**
抛去和优化参数无关的部分后，目标函数可写为
$$ Q_B \ge Q_B' = \sum_{r=1}^R\sum_{t=1}^T\sum_{j=1}^N\sum_{m=1}^M\gamma_j(t)\gamma_m(t)\log c_{jm}b_{jm}(o_t) $$

令
$$ \gamma_{jm}(t) = \mathbb{P}[q_t=j,g_t=m|O_1^T] = \mathbb{P}[q_t=j|g_t=m,O_1^T]\mathbb{P}[g_t=m|O_1^T] = \gamma_j(t)\gamma_m(t) $$

则
$$\begin{align*}
Q_B' &= \sum_{r=1}^R\sum_{t=1}^T\sum_{j=1}^N\sum_{m=1}^M \gamma_{jm}(t)\log c_{jm}b_{jm}(o_t)\\
&=\sum_r\sum_t\sum_j\sum_m\gamma_{jm}(t)\left( \log c_{jm}+ \left( \frac{1}{2}\log|\Sigma_{jm}^{-1}| - \frac{1}{2}(o_t-\mu_{jm})^T\Sigma_{jm}^{-1}(o_t-\mu_{jm}) \right) \right)
\end{align*}$$

**均值**
$$\begin{align*}
  \frac{\partial Q_B'}{\partial \mu_{jm}} &= \sum_r\sum_t\gamma_{jm}(t)\Sigma^{-1}_{jm}(o_t-\mu_{jm}) := 0\\
  &\Rightarrow \sum_r\sum_t\gamma_{jm}(t) (o_t-\mu_{jm}) = 0\\
  &\Rightarrow \mu_{jm} = \frac{\sum_r\sum_t\gamma_{jm}(t)o_t}{\Gamma_{jm}}\\
  &\triangleq \frac{\mu^{acc}_{jm}}{\Gamma_{jm}}
\end{align*}$$

其中

- $\mu^{acc}_{jm} = \sum_r\sum_t\gamma_{jm}(t)o_t = \sum_r\mu_{jm}^{acc(r)}$
- $\Gamma_{jm} = \sum_r\sum_t\gamma_{jm}(t) = \sum_r\gamma_{jm}^{(r)}(t)$

**协方差矩阵**
$$\begin{align*}
  \frac{\partial Q_B'}{\partial \Sigma^{-1}} &= \frac{1}{2}\sum_r\sum_t\gamma_{jm}(t)\Sigma_{jm} - \frac{1}{2}\sum_r\sum_t\gamma_{jm}(t)(o_t-\mu_{jm})(o_t-\mu_{jm})^T := 0\\
  &\Rightarrow \Sigma_{jm} = \frac{\sum_r\sum_t\gamma_{jm}(t)(o_t-\mu_{jm})(o_t-\mu_{jm})^T}{\Gamma_{jm}}\\
  &= \frac{\sum_r\sum_t\gamma_{jm}(t)(o_t-\mu_{jm}^{acc}/\Gamma_{jm})(o_t-\mu_{jm}^{acc}/\Gamma_{jm})^T}{\Gamma_{jm}}\\
  &= \frac{\sum_r\sum_t\gamma_{jm}(t)(o_to_t^T - \frac{1}{\Gamma_{jm}}o_t{\mu_{jm}^{acc}}^T - \frac{1}{\Gamma_{jm}}\mu_{jm}^{acc}o_t^T + \frac{1}{\Gamma_{jm}^2}\mu_{jm}^{acc}\mu_{jm}^{accT})}{\Gamma_{jm}}
\end{align*}$$

注意到 $\sum_r\sum_t\gamma_{jm}(t)o_t = \mu_{jm}^{acc}$、$\sum_r\sum_t\gamma_{jm}(t) = \Gamma_{jm}$
$$\begin{align*}
  Numerator &= \sum_r\sum_t\gamma_{jm}(t)o_to_t^T - \frac{1}{\Gamma_{jm}}\mu_{jm}^{acc}\mu_{jm}^{accT} - \frac{1}{\Gamma_{jm}}\mu_{jm}^{acc}\mu_{jm}^{accT} + \frac{\Gamma_{jm}}{\Gamma_{jm}^2}\mu_{jm}^{acc}\mu_{jm}^{accT}\\
  &= \sum_r\sum_t\gamma_{jm}(t)o_to_t^T - \frac{1}{\Gamma_{jm}}\mu_{jm}^{acc}\mu_{jm}^{accT}
\end{align*}$$

令
$$ \sum_{r=1}^R\sum_{t=1}^T\gamma_{jm}(t)o_to_t^T = \Sigma_{jm}^{acc} $$

故
$$ \Sigma_{jm} = \frac{\Sigma_{jm}^{acc}}{\Gamma_{jm}} - \frac{\mu_{jm}^{acc}\mu_{jm}^{accT}}{\Gamma_{jm}^2} $$

**混合系数**
$c_{jm}$ 应满足 $\sum_{m}c_{jm}=1$，是等式约束的优化问题，使用拉格朗日乘子法。
$$ L(c_{jm}, \lambda) = Q_B - \lambda\left( \sum_{m=1}^Mc_{jm} - 1 \right) $$

$$\begin{align*}
  \frac{\partial L}{\partial c_{jm}} &= \sum_r\sum_t\gamma_{jm}(t)\frac{1}{c_{jm}} - \lambda := 0\\
  &\Rightarrow c_{jm} = \frac{1}{\lambda}\sum_r\sum_t\gamma_{jm}(t) = \frac{1}{\lambda}\Gamma_{jm}
\end{align*}$$

$$\begin{align*}
  \frac{\partial L}{\partial \lambda} &= \sum_mc_{jm} - 1 := 0\\
  &\Rightarrow \lambda = \sum_m\sum_r\sum_t\gamma_{jm}(t) = \sum_m\Gamma_{jm}
\end{align*}$$

故
$$ c_{jm} = \frac{\Gamma_{jm}}{\sum_{m=1}^M\Gamma_{jm}} $$

## 附录：矩阵和向量微积分

$$ Tr[ABC] = Tr[CAB] = Tr[BCA] $$

$$ x^TAx = Tr(x^TAx) = Tr(x^TxA) $$

$$ \frac{\partial}{\partial A} Tr[AB] = B^T $$

$$ \frac{\partial}{\partial A} \log|A| = (A^{-1})^T $$

$$ \frac{\partial}{\partial A} (x^TAy) = xy^T $$

$$ \frac{\partial}{\partial x} (x^Ta) = a $$

$$ \frac{\partial}{\partial x} (x^TAx) = (A+A^T)x $$
