# 目录
隐马尔科夫模型 Hidden Markov Models
[TOC]


# 孤立词语音识别

## 确定化方法
1. 提取待识别词的特征序列 $O$
2. 计算 $O$ 与所有已知参考序列 $V_k$ 之间的距离
3. 找距离最近的作为识别结果

### 动态时间规整 DTW
Dynamic Time Warping

DTW算法计算序列 $X \in \mathbb{R}^M$ 和 $Y \in \mathbb{R}^N$ 的代价，定义一个 $M \times N$ 的矩阵 $D$
$$ D(i,j) = \min
\begin{cases}
D(i-1,j) + C_I \quad &\text{Insertion}\\
D(i,j-1) + C_D \quad &\text{Deletion}\\
D(i-1,j-1) + d(x,y) &\text{Substitution} 
\end{cases} $$
其中 $C_I$、$C_D$是插入和删除的代价，$d(x,y) =\sqrt{(x-y)^T(x-y)}$是替换的代价

### 基于DTW的语音识别
1. 训练：为每个词录制样例语音，作为词的模板
2. 解码：录制测试语音，然后通过DTW计算距离


## 概率模型方法
### 统计语音识别的数学框架
$$ \hat{W} = \arg\max_W \mathbb{P}[W|O] = \arg\max_W \mathbb{P}[O|W]\mathbb{P}[W] $$

### 孤立词识别
假设所有候选词都是等概率的，目标是找到声学似然最高的词
$$k = \arg\max_i p(O|w_i)$$
- $p(O|w_i)$需要完成特征向量长度的归一化
- 常用的表示$p(O|w_i)$的模型为隐马尔可夫模型


# 马尔科夫链
## 回顾随机过程
- （一阶）马尔科夫链
  - $X = [X_1, X_2, \dots, X_T]$
  - $\mathcal{S} = \{ s_1, s_2, \dots, s_M \}$
- 初始概率分布 $\pi$
- 状态转移矩阵 $P$


## 高阶马尔科夫链
以二阶为例，将**当前状态**和**过去状态**的**组合**作为转移矩阵的行（$|\mathcal{S}|^2$行）


## 隐马尔科夫模型
隐马尔科夫模型用于建模**每一个状态的输出是随机的**情况。
- 马尔科夫链：每个状态的输出确定；状态序列与观测序列完全一致。
- 隐马尔科夫模型：每个状态的输出随机，由一个概率分布决定

### 模型参数
- **隐含状态** $Q=\{q_i, 1 \le i \le S\}$
- **转移概率** $A=\{a_{ij}, 1 \le i,j \le S\}$
- **状态输出分布** $B(o) = \{b_j(o), 1 \le j \le S\}$
- **观测值** $o_t, 1 \le t \le T$

### 基本假设
- **马尔科夫假设**：下一状态只与当前状态有关
$$\mathbb{P}[q_{t+1}|q_{t},q_{t-1},\dots,q_1] = \mathbb{P}[q_{t+1}|q_{t}] = A(q_t, q_{t+1}) $$
- **条件独立性假设**：给定当前状态 $q_t$，观测值 $o_t$ 与历史状态和历史观测值无关
$$\mathbb{P}[o_t|q_t,q_{t-1},\dots,q_1] = \mathbb{P}[o_t|q_t] =b_{q_t}(o_t) $$
因此
$$\mathbb{P}[o_1,\dots,o_T|q_1,\dots,q_T] = \prod_{t=1}^T\mathbb{P}[o_t|q_t] $$

### 隐马尔可夫模型中有两个序列
- **状态序列**：隐含的、对观测者不可见的序列，表示该随机过程的实际状态
- **观测序列**：实际观测到的特征序列

### 隐马尔可夫模型包含两个步骤
- 状态转移
- 每个状态发射(emit)一个观测值

### HMM的相关公式
- 状态序列已知的情况：
$$ \mathbb{P}[o|q;\theta] = \prod_{t=1}^T\mathbb{P}[o_t|q_t;\theta] $$
- 状态序列未知的情况：
$$ \begin{align*}
\mathbb{P}[o;\theta] &= \sum_{q}\mathbb{P}[o,q;\theta] \quad \text{全概率公式}\\
&= \sum_p \mathbb{P}[o|q;\theta]\mathbb{P}[q;\theta] \quad \text{贝叶斯公式}\\
&=...
\end{align*} $$

### 似然度计算：前向算法
$$
\begin{align*}
\alpha_j(t)&=\mathbb{P}[o_1,\dots,o_t, q_t=j]\\
 &= \sum_{i=1}^{N} \mathbb{P}[o_1,\dots,o_{t-1},o_t, q_t=j,q_{t-1}=i] \quad \text{全概率公式}\\
 &= \sum_{i=1}^{N}\mathbb{P}[o_1,\dots,o_{t-1},o_t|q_{t-1}=i,q_t=j]\mathbb{P}[q_t=j,q_{t-1}=i]\\
 &= \mathbb{P}[o_t|q_t=j]\sum_{i=1}^{N}\mathbb{P}[q_t=j|q_{t-1}=i]\mathbb{P}[O^{(t-1)}, q_{t-1}=i]\\
 &= b_j(o_t)\sum_{i=1}^N A(i,j)\alpha_i(t-1)
\end{align*}
$$
前向算法并没有大幅度减少计算量。

### 解码：Viterbi算法
给定观测 $O$，Viterbi算法寻找
$$\hat{Q} = \arg\max_Q p[O,Q,q_0=1,q_{T+1}=N|\theta]$$
$$
\begin{align*}
\psi_j(t) &= \max_Q p[O^{(t)},q_0=1,Q^{(t-1)},q_t=j|\theta]\\
&= \max_{1\le i \le N}\max_{Q^{(t)}}p[]
\end{align*}
$$


## 语音信号的HMM建模
- 假设语音信号由准平稳的语音片段组成
- 具有有限长度
- 可以用小的声学单元（音素）的连接对语音信号进行较好的描述

## EM
$$
\begin{align*}
\log p(O,Q) &= \log p(O|Q)\cdot P(Q)\\
&= \sum_t \log p(T) 
\end{align*}
$$