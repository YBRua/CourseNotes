# 熵
## 信息
$$I(x) = -\log_2p(x)$$

## 熵
$$H = \mathbb{E}[-\log_2p(x)] = -\sum_{x}p(x)\cdot\log_2p(x)$$
- 是随机变量的信息 $I(x)$ 的期望

## 一些分布的熵
### 伯努利分布
$$H(B(1,p)) = -(1-\mu)\ln(1-\mu) - \mu\ln(\mu) $$
### 高斯分布
$$H(\mathcal{N}(\mu, \sigma^2)) = \frac{1}{2}\ln(2\pi{}e\sigma^2) $$

## 条件熵
**条件熵**是分布 $p(y|x)$ 的熵的期望
$$H[y|x] = -\sum_{x'} p(x)H(y|x=x') = -\sum_{x'}p(x)\sum_{y}p(y|x)\ln p(y|x)$$
$$H[y|x] = -\iint p(y,x) \ln p(y|x) \mathrm{d}y\mathrm{d}x$$
**联合熵**是条件熵与边缘熵之和
$$H[x,y] = H[y|x] + H[x]$$

## 互信息
**互信息**是边缘熵和条件熵的差
$$I[x,y] = H[x] - H[x|y] = H[y] - H[y|x]$$
- 互信息是对称的

## KL距离
**KL距离**可用于描述两个分布之间的差异
$$KL(p\|q) = -\int p(x) \ln q(x)\mathrm{d}x - \left(-\int p(x) \ln p(x) \mathrm{d}x\right)$$
$$KL(p\|q) = -\int p(x) \ln\left\{\frac{p(x)}{q(x)}\right\}\mathrm{d}x$$
- $KL(p\|q) \le 0$
- $KL(p\|q) \neq KL(q\|p)$
- $I[x,y] = KL(p(x,y)\|p(x)p(y))$

## 交叉熵
$$H_c(P,Q) = -\sum_{x}P(x)\log_2Q(x)$$
- $H_c(P,Q) \neq H_c(Q,P)$

