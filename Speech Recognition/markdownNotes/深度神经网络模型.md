# 语音识别的深度神经网络模型
[TOC]

# 深度神经网络基础
## 数据预处理
### 倒谱均值归一化
1. 减去每句话的特征平均值
2. 估计逐句均值
   $$ \mu_i = \frac{1}{T}\sum_{t=1}^T o_i^t $$
3. 对于每一维度 $i$ 减去所有帧中的平均值
   $$ \hat{o}_i^t = o_i^t - \mu_i $$

### 全局特征标准化
将MFCC和FBANK归一化为零均值、单位方差

## 初始化
1. 受限玻尔兹曼机预训练
2. 鉴别性预训练
3. 从高斯分布 $\mathcal{N}(w,0.05)$ 或均匀分布 $\mathcal{U}(-0.05,0.05)$ 中采样初始化。

## 权重衰减 Weight Decay

## 随机失活 Dropout
- 更新参数时忽略某些节点
- 释放一些节点的依赖关系
- 测试时输出要乘以 dropout rate $p$

## 批次大小 Batch Size
- Batch Gradient Descent
- Stochastic Gradient Descent
- Mini-batch Gradient Descent

对SGD和Minibatch，样本随机初始化很重要，否则参数可能会沿着相似的方向移动太长时间。

### 实现
- 对小训练集，可以直接加载到内存中，然后随机化训练集数组下标
- 对打训练集，每次读入一部分，然后在内部进行随机采样。
  - 如果数据来自不同来源，随机化语句列表文件也有帮助。

## 动量 Momentum
- 负梯度可以认为是在参数空间中移动粒子的力
- 假设加速过程中粒子为单位质量，且力衰减系数为 $\eta$，粒子动量衰减系数为 $\gamma$
$$ v_t = \gamma v_{t-1} - \eta \nabla J(W) $$

$$ W = W + v_t $$
- 通常 $\gamma$ 取 $0.9$
- 可以减少常规随机梯度下降中的震荡问题

## 学习率 Learning Rate
- 减半
  - 在交叉验证误差增加时减半学习率
- 指数衰减
  - 在经过指定步长后衰减学习率

## 网络架构
- 在语音识别任务中
  - 5-7层、每层1000-3000个神经元的深度神经网络效果非常好。
  - 宽而深的模型往往容易找到更优的配置

# 语音识别的深度神经网络应用
- 深度神经网络不能直接用于语音信号建模
  - 语音信号是变长的
  - DNN需要固定长度的输入
- DNN-HMM系统
  - HMM建模语音信号的动态变化
  - DNN建模观测概率

## DNN-HMM
### 组成部分
- 输入：上下文窗长大小的特征向量（多帧输入）
- 输出：senones (tri-phone state) 的后验概率
- DNN：所有状态共用一个DNN（与GMMHMM不同）
$$ p(o_t|q_t, \theta) = \frac{p(q_t=s|o_t)p(o_t)}{p(s)} \quad \text{其中$p(q_t=s|o_t)$ 是神经网络输出} $$

### 流程
1. 训练一个GMM-HMM
   - 为了获得每一帧的状态标注
2. 用GMM-HMM解码生成强制对齐的标签
3. 用得到的标签数据构建数据集训练DNN
4. 获得DNN-HMM

### 损失函数
通常使用交叉熵损失函数。

### 缩减尺寸与稀疏性 Size Reduction and Sparsity
- 语音识别的DNN训练完成后，很多权重值都是 $0$
- 在一些应用中，即使只保留 $20\%$ 或 $15\%$ 的非零参数，模型的性能仍然不会有明显下降。

# 其他神经网络及其应用
## 卷积神经网络 CNN
easy.
