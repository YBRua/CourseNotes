# 语音识别的深度神经网络模型

[TOC]

## 深度神经网络基础

### 数据预处理

#### 倒谱均值归一化

1. 减去每句话的特征平均值
2. 估计逐句均值
   $$ \mu_i = \frac{1}{T}\sum_{t=1}^T o_i^t $$
3. 对于每一维度 $i$ 减去所有帧中的平均值
   $$ \hat{o}_i^t = o_i^t - \mu_i $$

#### 全局特征标准化

将MFCC和FBANK归一化为零均值、单位方差

### 初始化

1. 受限玻尔兹曼机预训练
2. 鉴别性预训练
3. 从高斯分布 $\mathcal{N}(w,0.05)$ 或均匀分布 $\mathcal{U}(-0.05,0.05)$ 中采样初始化。

### 权重衰减 Weight Decay

### 随机失活 Dropout

- 更新参数时忽略某些节点
- 释放一些节点的依赖关系
- 测试时输出要乘以 dropout rate $p$

### 批次大小 Batch Size

- Batch Gradient Descent
- Stochastic Gradient Descent
- Mini-batch Gradient Descent

对SGD和Minibatch，样本随机初始化很重要，否则参数可能会沿着相似的方向移动太长时间。

#### 实现

- 对小训练集，可以直接加载到内存中，然后随机化训练集数组下标
- 对打训练集，每次读入一部分，然后在内部进行随机采样。
  - 如果数据来自不同来源，随机化语句列表文件也有帮助。

### 动量 Momentum

- 负梯度可以认为是在参数空间中移动粒子的力
- 假设加速过程中粒子为单位质量，且力衰减系数为 $\eta$，粒子动量衰减系数为 $\gamma$
$$ v_t = \gamma v_{t-1} - \eta \nabla J(W) $$

$$ W = W + v_t $$

- 通常 $\gamma$ 取 $0.9$
- 可以减少常规随机梯度下降中的震荡问题

### 学习率 Learning Rate

- 减半
  - 在交叉验证误差增加时减半学习率
- 指数衰减
  - 在经过指定步长后衰减学习率

### 网络架构

- 在语音识别任务中
  - 5-7层、每层1000-3000个神经元的深度神经网络效果非常好。
  - 宽而深的模型往往容易找到更优的配置

## 语音识别的深度神经网络应用

- 深度神经网络不能直接用于语音信号建模
  - 语音信号是变长的
  - DNN需要固定长度的输入
- DNN-HMM系统
  - HMM建模语音信号的动态变化
  - DNN建模观测概率

### DNN-HMM

#### 组成部分

- 输入：上下文窗长大小的特征向量（多帧输入）
- 输出：senones (tri-phone state) 的后验概率
- DNN：所有状态共用一个DNN（与GMMHMM不同）
$$ p(o_t|q_t, \theta) = \frac{p(q_t=s|o_t)p(o_t)}{p(s)} \quad \text{其中$p(q_t=s|o_t)$ 是神经网络输出} $$

#### 流程

1. 训练一个GMM-HMM
   - 为了获得每一帧的状态标注
2. 用GMM-HMM解码生成强制对齐的标签
3. 用得到的标签数据构建数据集训练DNN
4. 获得DNN-HMM

#### 损失函数

通常使用交叉熵损失函数。

#### 缩减尺寸与稀疏性 Size Reduction and Sparsity

- 语音识别的DNN训练完成后，很多权重值都是 $0$
- 在一些应用中，即使只保留 $20\%$ 或 $15\%$ 的非零参数，模型的性能仍然不会有明显下降。

## 其他神经网络及其应用

### 卷积神经网络 CNN

easy.

#### 语音识别中的CNN应用

当输入具有以下两个性质时，CNN能发挥较好作用

- 局部相关性
- 平移不变性

### 循环神经网络 RNN

#### 前向计算

每个时刻 $t$ 的输入由两部分组成

- 输入序列在当前时刻的取值 $x_{t-1}$
- 上一时刻的网络输出 $o_{t-1}$ 或隐层状态 $h_{t-1}$

每个时刻网络的隐层状态 $h_t$ 为
$$ h_t = \sigma_h(Ux_{t-1} + Vh_{t-1} + b_h) $$

每个时刻的输出 $o_t$ 为
$$ o_t = \sigma_o(Wh_t + b_o) $$

#### 参数更新

$$ \mathcal{L}(r, o) = \sum_{t=1}^T \mathcal{L}_t(r_t, o_t) $$
其中 $t$ 是时间戳，$r$ 是标签，$o$ 是模型输出

##### 沿时间的反向传播 BPTT

$$ \frac{\partial \mathcal{L}}{\partial V} = \sum_t\frac{\partial \mathcal{L}_t}{\partial V} $$

$$ \frac{\partial \mathcal{L}_t}{\partial V} = \frac{\partial h_t}{\partial V}\frac{\partial o_t}{\partial h_t}\frac{\partial \mathcal{L}_t}{\partial o_t} $$

#### RNN的问题

> 比全连接DNN更容易遇到梯度消失

- 梯度消失
- 梯度爆炸

### LSTM

缓解RNN中存在的梯度消失问题。

#### Cell

每个LSTM 的 Cell 由遗忘门、输入门、输出门三部分组成

输入由前一个cell的隐状态 $h_{t-1}$、当前cell的输入 $x_t$ 和前一个 cell 的状态 $C_{t-1}$ 组成。

- $C_{t-1}$在传递过程中几乎只经过线性变换。

##### 遗忘门

决定要丢弃多少历史信息
$$ f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f) $$

##### 输入门

决定前一个隐状态和当前输入中保留多少到当前cell的隐状态 $h_t$
$$ I_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) $$

$$ \hat{C}_i = \tanh(W_{xg}x_t + W_{hg}h_{t-1} + b_g) $$

##### 输出门

决定当前cell多少信息被添加到cell的状态 $C_t$
$$  $$

$$ h_t = o_t * \tanh(C_t) $$

$$ C_t = f_t *C_{t-1} + i* \hat{C}_t $$

其中 $*$ 为按元素相乘

- $C_t$ 定义为当前网络的长期记忆
- $\hat{C}_t$ 定义为当前网络的短期记忆

### 注意力机制

$$ Attention(Query, Key, Value) = \sum_{i=1}^N Similarity(Query, Key_i) \times Value_i $$

其中Similarity可以定义为余弦距离、向量内积等。
$$ Attention(Q,K,V) = Softmax\left( \frac{QK^T}{\sqrt{d_k}} \right)V $$

其中

- $X$ 为输入，长度为 $N$
- $C$ 为上下文序列，长度为 $M$
- $Q=XW_Q \in \mathbb{R}^{N\times d_k}$
- $K=CW_K \in \mathbb{R}^{M\times d_k}$
- $V=CW_V \in \mathbb{R}^{M\times d_v}$

### 可解释性

t-SNE
