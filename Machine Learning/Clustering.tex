\chapter{Clustering}
\newpage


\section{K-Means}
    \begin{itemize}
        \item Dataset: $H=\{x^{(1)},x^{(2)},\dots,x^{(n)}\}$
        \item $d$ dimensions
        \item Want to divide $H$ into disjoint subsets $\{H_1,H_2,\dots,H_c\}$
    \end{itemize}

    \subsection{Objective Function}
        Find an assignment such that the samples in the same class are as close to each other as possible.
        \[ J_c \triangleq \sum_{i=1}^c\sum_{x\in H_i} \|x-m_i\|^2 \]
        where $m_i$ is the centroid of each class.

    \subsection{Before K-Means}
        Let
        \[ J_i = \sum_{x\in H_i}\|x-m_i\|^2 \]

        \subsubsection{Framework}
        \begin{enumerate}
            \item Initialize
            \item Re-assign data samples to make the loss smaller.
        \end{enumerate}

        \subsubsection{Update Rule}
        Suppose a sample $\hat{x}$ is moved from $H_i$ to $H_j$. Let $n_j$ be the number of samples in $H_j$.
        \[ m^*_j = \frac{n_jm_j + \hat{x}}{n_j+1} = m_j - m_j + \frac{n_jm_j + \hat{x}}{n_j+1} = m_j + \frac{\hat{x}-m_j}{n_j+1} \]
        \[ m^*_i = \frac{n_im_i - \hat{x}}{n_i-1} = m_i + \frac{m_i - \hat{x}}{n_i+1} \]

        \begin{align*}
            J_j^* &= \sum_{x\in H_j}\| x-m_j^* \|^2 + \| \hat{x} - m_j^* \|^2\\
            &= \sum_{x\in H_j}\|x-m_j-\frac{\hat{x}-m_j}{n_j+1}\|^2 + \| \frac{n_j}{n_j+1}(\hat{x}-m_j) \|^2\\
            &= \sum_{x\in H_j} \| x - m_j \|^2 - \frac{2}{n_j+1}\sum_{x\in H_j}(\hat{x}-m_j)^T(x-m_j)\\
            &\quad + \frac{1}{(n_j+1)^2}\sum_{x\in H_j}\| \hat{x} -m_j \|^2 + \frac{n_j^2}{(n_j+1)^2}\|\hat{x}-m_j\|^2\\
            &= J_j + \frac{n_j}{(n_j+1)^2}\|\hat{x}-m_j\|^2 + \frac{n_j^2}{(n_j+1)^2}\|\hat{x}-m_j\|^2
        \end{align*}
        Notice that the second term is $0$ and the $\|\hat{x}-m_j\|^2$ in the first term is constant w.r.t. $x$ in the summation.

        Therefore
        \[ J^*_j = J_j + \frac{n_j}{n_j+1}\|\hat{x}-m_j\|^2 \]
        \[ J^*_i = J_i - \frac{n_i}{n_i+1}\|\hat{x}-m_j\|^2 \]

        \subsubsection{Convergence}
        Always converges, because the loss function is always decreasing and has a lower bound $0$.

        But it is SLOW.

    \subsection{K-Means}
        \subsubsection{Procedure}
        \begin{enumerate}
            \item Update centroids
            \item Re-assign points
        \end{enumerate}

        \subsubsection{Mathematical Framework}
        Let $r_{ik} \in \{0,1\}$ denote that the $i$-th sample is classified into the $k$-th class. $\sum_kr_{ik} = 1$. Then the loss function is
        \[ J = \sum_i \sum_k \| x_i - m_k \|r_{ik} \]

        \subsubsection{Convergence Analysis}
        In E-Step, the loss always goes down because each sample finds the nearest centroid.
        \[ J_c^{(old)} = \sum_i\sum_kr_{ik}^{(old)}\|x_i-m)k\| \ge \sum_i\sum_k\|x_i-m_k\| = J_c^{(new)} \]

        In M-Step,
        \begin{align*}
            J_c^k(old) &= \sum_{i\in H_k}\|x_i - m_k^{(old)}\|^2\\
            &= \sum_{i\in H_k}\| x_i - m_k^{(new)} + m_k^{(new)} - m_k^{(old)} \|^2\\
            &= \sum_{i\in H_k} \|x_i - m_k^{(new)}\|^2\\
            &\quad + \sum_{i\in H_k}\|m_k^{(old)} - m_k^{(new)}\|^2\\
            &\quad + \sum_{i\in H_k}(x_i-m_k^{(new)})^T(m_k^{(old)}-m_k^{(new)})
        \end{align*}
        The first term is $J_c^k(new)$, the second term is non-negative and the third term is zero.

        \subsubsection{Remarks}
        \begin{itemize}
            \item Generally K-Means has good performance.
            \item Initialization matters.
            \item The choice of $k$ matters.
        \end{itemize}

    \subsection{Agglomerative Clustering}
        \begin{enumerate}
            \item Initialize: everypoint is its own cluster.
            \item Find the most similar pair of clusters.
            \item Merge.
        \end{enumerate}
