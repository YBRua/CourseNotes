\chapter{Clustering}
\newpage


\section{K-Means}
\begin{itemize}
    \item Dataset: $H=\{x^{(1)},x^{(2)},\dots,x^{(n)}\}$
    \item $d$ dimensions
    \item Want to divide $H$ into disjoint subsets $\{H_1,H_2,\dots,H_c\}$
\end{itemize}

\subsection{Objective Function}
Find an assignment such that the samples in the same class are as close to each other as possible.
\[ J_c \triangleq \sum_{i=1}^c\sum_{x\in H_i} \|x-m_i\|^2 \]
where $m_i$ is the centroid of each class.

\subsection{Before K-Means}
Let
\[ J_i = \sum_{x\in H_i}\|x-m_i\|^2 \]

\subsubsection{Framework}
\begin{enumerate}
    \item Initialize
    \item Re-assign data samples to make the loss smaller.
\end{enumerate}

\subsubsection{Update Rule}
Suppose a sample $\hat{x}$ is moved from $H_i$ to $H_j$. Let $n_j$ be the number of samples in $H_j$.
\[ m^*_j = \frac{n_jm_j + \hat{x}}{n_j+1} = m_j - m_j + \frac{n_jm_j + \hat{x}}{n_j+1} = m_j + \frac{\hat{x}-m_j}{n_j+1} \]
\[ m^*_i = \frac{n_im_i - \hat{x}}{n_i-1} = m_i + \frac{m_i - \hat{x}}{n_i-1} \]

\begin{align*}
    J_j^* & = \sum_{x\in H_j}\| x-m_j^* \|^2 + \| \hat{x} - m_j^* \|^2                                                \\
          & = \sum_{x\in H_j}\|x-m_j-\frac{\hat{x}-m_j}{n_j+1}\|^2 + \| \frac{n_j}{n_j+1}(\hat{x}-m_j) \|^2           \\
          & = \sum_{x\in H_j} \| x - m_j \|^2 - \frac{2}{n_j+1}\sum_{x\in H_j}(\hat{x}-m_j)^T(x-m_j)                  \\
          & \quad + \frac{1}{(n_j+1)^2}\sum_{x\in H_j}\| \hat{x} -m_j \|^2 + \frac{n_j^2}{(n_j+1)^2}\|\hat{x}-m_j\|^2 \\
          & = J_j + \frac{n_j}{(n_j+1)^2}\|\hat{x}-m_j\|^2 + \frac{n_j^2}{(n_j+1)^2}\|\hat{x}-m_j\|^2
\end{align*}
Notice that the second term is $0$ and the $\|\hat{x}-m_j\|^2$ in the first term is constant w.r.t. $x$ in the summation.

Therefore
\[ J^*_j = J_j + \frac{n_j}{n_j+1}\|\hat{x}-m_j\|^2 \]
\[ J^*_i = J_i - \frac{n_i}{n_i-1}\|\hat{x}-m_i\|^2 \]

\subsubsection{Convergence}
Always converges, because the loss function is always decreasing and has a lower bound $0$.

But it is SLOW.

\subsection{K-Means}
\subsubsection{Procedure}
\begin{enumerate}
    \item Update centroids
    \item Re-assign points
\end{enumerate}

\subsubsection{Mathematical Framework}
Let $r_{ik} \in \{0,1\}$ denote that the $i$-th sample is classified into the $k$-th class. $\sum_kr_{ik} = 1$. Then the loss function is
\[ J = \sum_i \sum_k \| x_i - m_k \|r_{ik} \]

\subsubsection{Convergence Analysis}
In E-Step, the loss always goes down because each sample finds the nearest centroid.
\[ J_c^{(old)} = \sum_i\sum_kr_{ik}^{(old)}\|x_i-m)k\| \ge \sum_i\sum_k\|x_i-m_k\| = J_c^{(new)} \]

In M-Step,
\begin{align*}
    J_c^k(old) & = \sum_{i\in H_k}\|x_i - m_k^{(old)}\|^2                               \\
               & = \sum_{i\in H_k}\| x_i - m_k^{(new)} + m_k^{(new)} - m_k^{(old)} \|^2 \\
               & = \sum_{i\in H_k} \|x_i - m_k^{(new)}\|^2                              \\
               & \quad + \sum_{i\in H_k}\|m_k^{(old)} - m_k^{(new)}\|^2                 \\
               & \quad + \sum_{i\in H_k}(x_i-m_k^{(new)})^T(m_k^{(old)}-m_k^{(new)})
\end{align*}
The first term is $J_c^k(new)$, the second term is non-negative and the third term is zero.

\subsubsection{Remarks}
\begin{itemize}
    \item Generally K-Means has good performance.
    \item Initialization matters.
    \item The choice of $k$ matters.
\end{itemize}

\subsection{Agglomerative Clustering}
\begin{enumerate}
    \item Initialize: everypoint is its own cluster.
    \item Find the most similar pair of clusters.
    \item Merge.
\end{enumerate}


\section{Gaussian Mixture Model}
    K-Means can be easily affected by outliers.

    \subsection{Multivariate Gaussian Distribution}
        \[ p(x) = \frac{1}{(2\pi|\Sigma|)^{d/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right\} \]

    \subsection{Gaussian Mixture Model}
        Suppose the data has a distribution that can be modeled by the weighted sum of $K$ Gaussian distributions. Let $\pi_k = p(x=k)$ be the likelihood of the $k$-th Gaussian.
        \[ p(x) = \sum_{k=1}^Np(x=k)p(x|k) = \sum_{k=1}^K\pi_k \mathcal{N}(x|\mu_k,\Sigma_k) \]

        So the parameters of a GMM are
        \begin{itemize}
            \item $\pi_k$
            \item $\mu_k$
            \item $\Sigma_k$
        \end{itemize}

        Given the parameters of a GMM, the probabilities can be calculated.
        \begin{itemize}
            \item Prior: $p(x=k)=\pi_k$
            \item Likelihood: $p(x|x=k)=\mathcal{N}(x|\mu_k,\Sigma_k)$
            \item Joint Distribution: $p(x,x=k)=p(x=k)p(x|x=k)=\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)$
            \item Evidence: $p(x) = \sum_{k=1}^Kp(x,x=k)=\sum_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)$
            \item Posterior: $p(x=k|x)=\frac{p(x,x=k)}{p(x)}=\frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum_{i=1}^K\pi_i\mathcal{N}(x|\mu_i,\Sigma_i)}$
        \end{itemize}

    \subsection{Estimating Parameters}
        Clustering using a GMM is simply estimating the parameters of the model.

        Suppose we have a dataset $D = \{x^{(1)},x^{(2)},\dots,x^{(N)}\}$. We maximize the log likelihood
        \[ \mathcal{L}(\theta) = \ln p(D|\theta) = \sum_{n=1}^N\ln\left\{\sum_{k=1}^K\pi_k\mathcal{N}(x^{(n)}|\mu_k,\Sigma_k)\right\} \]

        However, traditional MLE methods do not work due to the complexity of $\mathcal{L}$. To illustrate this, we take derivative w.r.t. $\mu_k$
        \[ \frac{\partial \mathcal{L}}{\partial \mu_k} = -\sum_{n=1}^N\frac{\pi_k\mathcal{N}(x^{(n)}|\mu_k,\Sigma_k)}{\sum_j\pi_j\mathcal{N}(x^{(n)}|\mu_j,\Sigma_j)}\Sigma_k^{-1}(x_n-\mu_k) \]
        but we cannot derive a closed-form solution of $\mu_k$.

        Instead, this is an exponential function that can be solved by iterative numerical methods, let
        \[ \gamma(z_{nk}) = \frac{\pi_k\mathcal{N}(x^{(n)}|\mu_k,\Sigma_k)}{\sum_j\pi_j\mathcal{N}(x^{(n)}|\mu_j,\Sigma_j)} \]
        \[ N_k = \sum_{n=1}^N \gamma(z_{nk}) \]

        To solve for $\mu_k$, we run multiple iterations, in each iteration, we first fix $\theta = (\pi_k,\mu_k,\Sigma_k)$ and update $\gamma(z_{nk})$, and then fix $\gamma(z_{nk})$ and update $\theta$.

        \subsubsection{Estimating Mean and Covariance Matrix}
        Using previous conclusions,
        \[ \mu_k = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{nk})x^{(n)} \]
        Similarly if we solve for $\Sigma_k$,
        \[ \Sigma_k = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{nk})(x^{(n)}-\mu_k)(x^{(n)}-\mu_k)^T \]

        \subsubsection{Estimating Mixture Coefficients}
        Notice that $\sum_k\pi_k = 1$, so maximizing w.r.t. $\pi_k$ is an ECP.

        The solution is given by
        \[ \pi_k = \frac{N_k}{N} \]

    
\section{Spectral Clustering}
    K-Means and GMM may have poor performance when the data is non-convex.
    
    \subsection{Graph-based Clustering}
        We use a graph $G=(V,E)$ to represent the dataset. Each sample is a node in $V$ and each edge has a weight $w_{ij}=f(d(i,j))$.

        \begin{enumerate}
            \item Construct the graph
            \item Perform edge cuts.
        \end{enumerate}

    \subsection{Graph Cut}
        \subsubsection{Definitions and Notations}
            \begin{itemize}
                \item Degree of a node $d_i = \sum_{j\in \mathcal{N}(i)}w_{ij}$.
                \item Degree of a subgraph $A_i = \sum_{i \in A}d_i$. There are two cases.
                    \begin{itemize}
                        \item $i \in A$, $j \in A$. In this case $w_{ij}$ is counted twice.
                        \item $i \in A$, $j \notin A$. In this case $w_{ij}$ is counted once.
                    \end{itemize}
                \item Cut of a graph $Cut(A, B) = \sum_{i \in A}\sum_{j \in B}w_{ij}$.
            \end{itemize}

        We minimize
        \[ \min Cut(A_1,A_2,\dots,A_k) \]

        However, the algorithm may favor isolated clusters and outliers.

        Can be mitigated by normalizing edge cuts.
        \[ NCut(A,B) = \frac{Cut(A,B)}{d_A} + \frac{Cut(A,B)}{d_B}\]
        Works. But it's NP-hard.

    \subsection{Optimizing Graph Cuts}
        \subsubsection{More Notations}
            \begin{itemize}
                \item Degree matrix $D_{ij} = \text{$\sum_j w_{ij}$ if $i=j$ else $0$}$
                \item $L=D-W$
                \item Define an indicator vector $x$. $x_i = 1$ if $i \in A$ and $x_i=0$ if $i \notin A$, i.e. $i \in B = V-A$. $x$ is unknown and we want to find $x$.
            \end{itemize}

        We can represent $d_A$ and $d_B$ in matrix form
        \[ d_A = \sum_{i\in A}d_i = \sum_i d_ix_i = x^TDx \]
        \[ d_B = \bar{x}^TD\bar{x} \]
        where $\bar{x}$ is the bitwise NOT of $x$.

        \[ Cut(A, V-A) = \sum_{i\in A}\sum_{j\in B}w_{ij} = \sum_{i \in A}d_i - \sum_{i \in A}\sum_{j \in A}w_{ij} = x^TDx - x^TWx = x^TLx \]

        Therefore
        \begin{align*}
            \min_x &\quad x^TLx\\
            \text{s.t.} &\quad x^TDx \ge \delta
        \end{align*}

        Can be solved by Lagrangian multipliers
        \[ \mathcal{L}(x, \lambda_A, \lambda_B) = x^TLx - \lambda_A(x^TDx-\delta) \]
        \[ D^{-1}Lx = \lambda_A x \]
        So we find the eigenvector of $D^{-1}L$ and take approximations to make elements of $x$ either $0$ or $1$.

        \subsubsection{Procedure}
            \begin{enumerate}
                \item Normalize $L = D^{-1/2}LD^{-1/2}$
                \item Compute the smallest $k_1$ eigenvalues and corresponding eigenvectors.
                \item Use eigenvectors to construct $N$ by $k_1$ matrix $P$.
                \item Use $P$ as the new input of some other clustering algorithm, where each sample has $k_1$ dimension.
            \end{enumerate}

        \subsubsection{Strengths and Weaknesses}
        \begin{itemize}
            \item Can handle non-convex datasets.
            \item Runs into trouble when new data samples comes. Cannot be easily extended.
        \end{itemize}
