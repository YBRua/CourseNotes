\chapter{Basic Classification Models}
\newpage


\section{k Nearest Neighbours}
    kNN is a classical non-parametrized lazy-learning algorithm.

    \subsection{Distance}
        A distance function $d(x,y)$ is a function that satisfies
        \begin{itemize}
            \item Positive Definite. $d(x,y)>0$ and $d(x,y)=0$ iff $x=y$.
            \item Symmetric.
            \item Triangle Inequality. $d(x,y) \le d(x,z) + d(z,y)$.
        \end{itemize}
        Common distance functions include
        \begin{itemize}
            \item $l_2$ distance (Euclidean). $\|x-y\|_2 = \sqrt{\sum(x_i - y_i)^2}$.
            \item $l_1$ distance (Manhattan). $\|x-y\|_2 = \sum |x_i-y_i|$.
            \item $l_{\infty}$ distance. $\|x-y\|_{\infty} = \max\{|x_i-y_i|\}$.
        \end{itemize}

    \subsection{A general form of Euclidean Distance}
        Let $P$ be a projection matrix, we can generalize $l_2$ distance by
        \[\|Px-Py\|_2^2 = (P(x-y))^T(P(x-y)) = (x-y)^TP^TP(x-y)\]
        \[\Rightarrow Dist(x,y) = (x-y)^TA(x-y)\]
        \begin{remark}
            Actually $A$ can be learned.
        \end{remark}

    \subsection{kNN Algorithm}
        Given a dataset $\mathcal{D}$ and an input $x$, the kNN algorithm
        \begin{enumerate}
            \item Computes the distance between $x$ and each $y$ in $\mathcal{D}$.
            \item Finds the nearest $k$ neighbours of $x$ in $\mathcal{D}$.
            \item Determines the class of $x$ by the label of neighbours.
        \end{enumerate}

    \subsection{Choosing k}
        \begin{itemize}
            \item For a binary classification task, $k$ should be an odd number.
            \item $k$ should not be too large or too small.
            \item Parameter tuning techniques can be used to choose a proper $k$.
        \end{itemize}

    \subsection{More on kNN}
        \paragraph{Complexity.}
        kNN requires
        \begin{itemize}
            \item An $O(n)$ time to calculate distances.
            \item An $O(n\log n)$ time to sort and find nearest neighbours.
        \end{itemize}
        \paragraph{Can it be faster?}
        Yes if we use parallelism.


\section{Naive Bayes Classifer}

    \subsection{Posterior, Prior, Likelihood and Evidence}
        Recall Bayes' Rule
        \[ p(w_j|x) = \frac{p(x|w_j) \cdot p(w_j)}{p(x)} \]
        \begin{itemize}
            \item $p(w_j|x)$ is called the \textbf{posterior}.
            \item $p(w_j)$ is called the \textbf{prior}.
            \item $p(x|w_j)$ is called the \textbf{likelihood}.
            \item $p(x)$ is called the \textbf{evidence}.
        \end{itemize}
        \[ Posterior = (Likelihood \cdot Prior) / Evidence \]
        Optimizing w.r.t. $w$ is independent of $p(x)$,
        \[ Posterior \sim (Likelihood \cdot Prior) \]

    \subsection{Bayesian Decision Theory}
        Suppose there are $N$ classes $c_1,c_2,\dots,c_N$. Let $\lambda_{ij}$ be the cost of (mistankenly) classifying a sample in $c_i$ into $c_j$.
        \begin{remark}
            In a more general sense, $c_1, c_2, \dots, c_N$ is more than just a set of classes. It can be seen as a set of ``actions''.
        \end{remark}

        \begin{definition}[Conditional Risk]
            The \textbf{expected loss}, or the \textbf{conditional risk} is defined as
            \[ R(c_i|x) = \sum_{j=1}^N \lambda_{ij}p(c_j|x) \]
        \end{definition}
        Our goal is to find a minimizer $h(x)$ of the general conditional risk $R(h) = \mathbb{E}[R(h(x)|x)]$.

        For each $x$, if $h(x)$ minimizes $R(h(x)|x)$, then obviously $h(x)$ also minimizes $R(h)$. The minimizing $h^*(x)$ is called the \textbf{Bayes optimal classifier}, and $R(h^*)$ is called the \textbf{Bayes risk}.

        If
        \[
        \lambda_{ij} = 
        \begin{cases}
            0 \quad & i = j\\
            1 \quad & otherwise
        \end{cases}    
        \]
        Then
        \[ R(c|x) = 1 - p(c|x) \]
        minimizing this risk is equivalent to maximizing $p(c|x)$ and
        \[ h^*(x) = \arg \max_{c_i} p(c_i|x) = \arg \max_{c_i} \frac{p(x|c_i)p(c_i)}{p(x)} = \arg\max_{c_i} p(x|c_i)p(c_i)\]
        So we choose the $x$ that \emph{maximizes posterior}.

        \begin{remark}
            A \textbf{discriminative model} directly models likelihood $p(c|x)$, while a \textbf{generative model} models joint distribution $p(x,c)$ by modeling $p(c) \cdot p(x|c)$.
        \end{remark}

        \subsubsection{Example.}
        For a binary classification (decision-making), if
        \[ R(c_1|x) < R(c_2|x) \]
        then we take $c_1$.

        This is equivalent to taking $c_1$ if
        \[ (\lambda_{21}-\lambda_{11})p(x|c_1)p(c_1) > (\lambda_{12}-\lambda_{22})p(x|c_2)p(c_2) \]
        or
        \[ \frac{p(x|c_1)}{p(x|c_2)} > \frac{\lambda_{12} - \lambda_{22}}{\lambda_{21} - \lambda_{11}} \cdot \frac{p(c_2)}{p(c_1)} \]

        If we further assume that $p(x|C_1)$ and $p(x|c_2)$ have Gaussian distributions with the same variance, then we test whether
        \[ \exp\left\{ -\frac{1}{2}(x-\mu_1)^2/\sigma^2 + \frac{1}{2}(x-\mu_2)^2/\sigma^2 \right\} \ge \lambda \]
        where $\lambda$ is a threshold.
        Taking logarithm and simplifying
        \[ (\mu_1/\sigma^2 - \mu_2/\sigma^2)x + constant \ge \log\lambda \]
        i.e.
        \[ ax+b > c \]
        or in higher dimensions
        \[ w^Tx > b \]
        It is a linear classifier.

    \subsection{Parameter Estimation}
    \subsubsection{Maximum Likelihood Estimation}
    Let $D$ be a dataset with $n$ i.i.d. samples $x_i$, MLE maximizes
    \[ p(D|\theta) = \prod_{i=1}^n p(x_i|\theta) \]
    to avoid underflow of floating point operations, we take the log likelihood
    \[ l(\theta) = \log(p(D|\theta)) = \sum_{i=1}^n \log(p(x_i|\theta)) \]
    
    \subsubsection{Example of MLE}
    Consider the multivariate Gaussian distribution with dimension $p$
    \[ f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\cdot \exp \left\{ -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right\} \]
    The log likelihood is
    \[ \log\left( p(x_i|\theta) = -\frac{1}{2}\log\left[(2\pi)^p|\Sigma|\right] - \frac{1}{2}(x_i - \mu)^T\Sigma^{-1}(x_i - \mu)  \right) \]
    Taking gradient w.r.t. $\mu$
    \[ \nabla_{\mu} = \Sigma^{-1}(x_i-\mu) \]
    And the MLE estimator is
    \[ \hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i \]
    The covariance matrix can be estimated similarly.

    \subsubsection{Maximum-A-Posterior}
    Beyond the scope.

    \subsection{Naive Bayes}
        \paragraph{Notations.}
            \begin{itemize}
                \item $X$: $N \times d$ training set. 
                \item $x$: a $d$-dim data sample.
                \item $x^{(i)}$: the $i$-th sample in $X$.
                \item $y^{(i)}$: the label of $x^{(i)}$.
                \item $x_j$: the $j$-th feature in $x$.
                \item $c_i$'s: classes.
            \end{itemize}
        The naives Bayes classifier makes the attribute conditinoal independence assumption, aka. the naive Bayes assumption: \emph{all features of x are independent}.
        \[ p(x_1, x_2,\dots,x_d | y) = \prod_{i=1}^d p(x_i|y) \]
        Then we can compute the optimal classifier by
        \[ h_{nb}(x) = \arg\max_{c}p(c)\prod_{j=1}^d p(x_j|c) \]
        Therefore a naive Bayes classifier estimates $p(c)$ and $p(x_j|c)$.

        If we assume $p(x^{(i)}|c) \sim \mathcal{N}(\mu_{c,i}, \sigma^2_{c,i})$, then
        \[ p(c) = \frac{\sum_{i=1}^N\mathbb{I}[y^{(i)}=c]}{N} \]
        i.e. the frequency of $x$ with label $c$ in $X$.
        \[ p(x_j|c) = \frac{\sum_{i=1}^N\mathbb{I}[x^{(i)}_j = 1, y^{(i)}=c]}{\sum_{i=1}^N\mathbb{I}[y^{(i)}=c]} \]
        i.e. the frequency of those $x$ with feature $x_j$ in all $x$ with label $c$.

    \subsection{Laplace Smoothing}
        If an attribute value has never appeared in the training set, the output will either be $0$ or $\frac{0}{0}$.

        To fix this, we introduce \textbf{Laplace smoothing}.
        \[ p(y=i) = \frac{\sum_{j=1}^N\mathbb{I}[y^{(j)=i}] + 1}{N + k} \]
        where $k$ is the number of classes.


\section{Decision Trees and Ensembling}
    A decision tree is a tree-based classifier. It has a root, several leaf nodes and several internal nodes. Each leaf is a decision and in each internal node we perform a test on one or more attributes of input.

    \subsection{Entropy and Information Gain}
    \begin{definition}[Entropy]
        Given a dataset $D$ with $k$ classes, suppose the proportion of the $i$-th class in $D$ is $p_i$. The entropy is defined by
        \[ H(D) = -\sum_{i=1}^k p_i \log_2 p_i \]
    \end{definition}
    \begin{remark}
            The larger $H(D)$ is, the more information $D$ contains.
    \end{remark}

    \begin{definition}[Conditional Entropy]
        Suppose an attribute $a$ has $V$ possible values. If we split $D$ based on the value of $a$, we will have $V$ branches.
        \[ H(Y|X) = \sum_{v=1}^V \mathbb{P}[X=x_v]\sum_{i=1}^k\mathbb{P}[Y=y_i|X=x_v]\log_2\mathbb{P}[Y=y_i|X=x_v] \]
        or
        \[ H(Y|X) = \sum_{v=1}^V\frac{|D_v|}{|D|}H(D_v) \]
    \end{definition}
    \begin{remark}
        Conditional Entropy is a weighted sum.
    \end{remark}

    \begin{definition}[Information Gain]
        Given a dataset $D$ and an attribute $a$, the information gain of splitting $D$ by $a$ is defined by
        \[ IG(D,a) = H(D) - \sum_{v=1}^V\frac{|D_v|}{|D|}H(D_v) \]
    \end{definition}
    \begin{remark}
        Larger information gain usually means splitting $D$ by $a$ results in more ``pure'' subsets. So choosing $a_* = \arg\max IG(D,a)$ is a common way of choosing attributes when training decision trees.
    \end{remark}

    \begin{definition}[Gain Ratio]
        The \textbf{gain ratio} is defined by
        \[ GainRatio(D,a) = \frac{Gain(D,a)}{IV(a)} \]
        where $IV(a)$ is the \textbf{intrinstic value} of attribute $a$,
        \[ IV(a) = -\sum_{v=1}^V \frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|} \]
    \end{definition}
    \begin{remark}
        Information gain prefers attributes with more values, and gain ratio is proposed to mitigate the negative effects. Gain ratio is used in the famous C4.5 algorithm.

        However, gain ratio prefers attributes with fewer possible values, and the C4.5 algorithm actually uses a heuristic approach: choose those attributes whose information gains are above the average, and then choose the one with the highest gain ratio.
    \end{remark}

    \subsection{Gini Index}
        \begin{definition}[Gini]
            \[ Gini(D) = \sum_{i=1}^{k}\sum_{i' \neq i} p_ip_{i'} = 1 - \sum_{i=1}^k p_i^2 \]
        \end{definition}

        \begin{definition}[Gini Index]
            The \textbf{Gini index} is defined by
            \[ GiniIndex(D, a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D) \]
        \end{definition}

    \subsection{Generating a Decision Tree}
        A decision tree is generated recursively, and there are three return conditions
        \begin{enumerate}
            \item All samples in current data set belongs to the same class.
            \item No available attributes exist for splitting (the attribute set is empty, or all samples take the same value on all atrributes)
            \item No samples exist in current data set.
        \end{enumerate}

    \subsection{Ensemble Learning}
        Ensemble learning constructs multiple weak individual classifiers and combines their prediction outputs.

        Suppose the final prediction is determined by voting (the output will be $1$ if more than half classifiers output $1$), when there are $T$ base classifiers,
        \[ H(x) = \mathrm{sign}\left( \sum_{i=1}^T h_i(x) \right) \]

        Suppose the error rates of individual classifiers are independent, by Heoffding Inequality,
        \[ P(H(x) \neq f(x)) = \sum_{k=0}^{T/2}C_T^k(1-\epsilon)^k\epsilon^{T-k} \le \exp\left( -\frac{1}{2}T(1-2\epsilon)^2 \right) \]

        Therefore as we increase the number of base classifiers, the error rate drops exponentially. However, notice that this conclusion is based on the assumption that all classifiers are independent, which in practice is barely possible.

        \subsubsection{Bagging}
        While it is practically difficult to make each individual classifier independent of each other, it is still possbile to maximize their variances.
        \begin{enumerate}
            \item Construct $T$ datasets, each containing $m$ samples, with bootstrapping.
            \item Train $T$ models with the datasets.
            \item Ensemble by voting or taking means.
        \end{enumerate}
        \begin{remark}
            From the perspective of bias-variance decomposition, bagging reduces the variance term in the error.
        \end{remark}

        \subsubsection{Random Forests}
        Random forests uses decision trees in bagging. Furthermore, it adds randomness in attribute selection.

        For each node, instead of choosing the best attribute from all attributes, RF first sample a subset of $k$ attributes, and then choose the best attribute from this subset. This further increases the diversity of base classifiers.