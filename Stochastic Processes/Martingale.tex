\chapter{(Discrete) Martingale}
\emph{这个词中文叫做“鞅”。}
\newpage


\section{Definition of a Martingale}
    \subsection{Definition}
        \begin{definition}[Martingale]\label{Def:Martingale}
            A stochastic process is a \textbf{Martingale} if
            \[ \forall t \ge 0 \quad \mathbb{E}[X_{t+1}|X_0,\dots,X_t] = X_t \]
        \end{definition}

        For conciseness in this chapter we use
        \[ X_{(m,n)} = (X_m,\dots,X_n) \]
        to denote a portion of the stochastic process from time $m$ to $n$.

        \begin{definition}[Generalized Martingale]\label{Def:GeneralizedMartingale}
            Given two stochastic processes $X_t$ and $Z_t$. $Z_t$ is a \textbf{martingale} \emph{with respect to} $X_t$ if
            \[ \forall t \ge 0 \quad \mathbb{E}[Z_{t+1} | X_{(0,t)}] = Z_t \]
        \end{definition}
        \begin{remark}
            Definition \ref{Def:Martingale} is a special case of \ref{Def:GeneralizedMartingale} where $Z_t = X_t$.
        \end{remark}

    \subsection{Fancier Definitions}
        Recall measure theory stuff in the first lecture.
        Let $\mathcal{F}_n = (X_0,\dots,X_n)$ be the smallest $\mathcal{F}$-measurable $\sigma$-algebra.
        \begin{definition}[Filteration]
            $\{\mathcal{F}_n\}$ is called a \textbf{filteration} if
            \[ \mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \cdots \subseteq \mathcal{F}_n \]
        \end{definition}

        \begin{definition}[Martingale (Fancy)]\label{Def:FancyMartingale}
            $\{Z_t\}$ is a \textbf{martingale} \emph{w.r.t. a filteration} $\{\mathcal{F}_t\}$ if
            \begin{enumerate}
                \item $\forall t \ge 0$, $Z_t$ is $\mathcal{F}_t$-measurable.
                \item $\mathbb{E}[Z_{t+1}|\mathcal{F}_t] = Z_t$. Or equivalently $\mathbb{E}[Z_{t+1}] = \mathbb{E}[Z_t]$
            \end{enumerate}
        \end{definition}

        \begin{remark}
            If $\mathbb{E}[X_{t+1}|\mathcal{F}_t] \le X_t$, then it is called a \textbf{Super-martingale}.

            If $\mathbb{E}[X_{t+1}|\mathcal{F}_t] \ge X_t$, then it is called a \textbf{Sub-martingale}.
        \end{remark}

    \subsection{Examples}
        \subsubsection{Random Walk on 1-d Integers}
        Consider the random walk on $\mathbb{Z}$. Let $X_t \in \{-1, 1\}$ be a uniform-at-random r.v. denoting the direction of the $t$-th move.

        Let $Z_t = \sum_{i=1}^tX_i$ and $Z_0=0$, then $Z_t$ is a martingale w.r.t. $X_t$.

        \[ \mathbb{E}[Z_{t+1}|X_{(0,t)}] = \mathbb{E}[Z_t + X_{t+1}|X_{(0,t)}] = Z_t + \mathbb{E}[X_{t+1}|X_{(0,t)}] = Z_t \]

        \subsubsection{Branching Process}
        The branching process is introduced in section \ref{Sub:BranchingProcess}.

        Let $Z_t$ be the number of people in the $t$-th generation, let $X_{ti}$ be the children of the $i$-th people in the $t$-th generation.

        Assume $\mathbb{E}[X_{ti}] = \mu$.

        Notice that we have
        \[ Z_{t+1} = \sum_{i=1}^{Z_t}X_{ti} \]

        \[ \mathbb{E}[Z_{t+1}|\mathcal{F}_t] = \mathbb{E}[Z_{t+1}|Z_t] \]

        Since
        \[ \mathbb{E}[Z_{t+1}|Z_t=z] = \mathbb{E}[\sum_{i=1}^zX_{ti}|Z_t=z] = \mu z \]

        Therefore
        \[ \mathbb{E}[Z_{t+1}|\mathcal{F}_t] = \mu Z_t \]

        $Z_t$ is \emph{not} a martingale because of the scaling factor $\mu$.
        \[ \mathbb{E}[Z_{t+1}|Z_t] \neq Z_t \]

        However if we let
        \[ M_t = \mu^{-t}Z_t \]

        Then $\{M_t\}$ is a martingale w.r.t. $\{\mathcal{F}_t\}$

        \subsubsection{Polya's Urn}
        Suppose we have an urn (buskets/bottles/whatever) containing two kinds of balls: black balls and white balls.
        
        Suppose we start from time $t=2$ (so that the urn contains $t$ balls at time $t$); suppose the urn has one black ball and one white ball at the beginning.
        
        Each time we draw a ball at random and replace it together with a ball of the same color.

        Let $X_t$ be the number of white balls after time $t$. Let $Z_t = X_t/t$ be the ratio of white balls. We claim that $\{Z_t\}$ is a martingale.

        \begin{align*}
            \mathbb{E}\left[Z_{t+1}|X_{(2,t)}\right] &= \frac{1}{t+1}\mathbb{E}[X_{t+1}|X_{(2,t)}]\\
            &= \frac{1}{t+1}\left(\frac{X_t}{t}\left(X_t+1\right) + \frac{t-X_t}{t}\left(X_t\right)\right)\\
            &= \frac{1}{t+1}\frac{X_t + tX_t}{t} = \frac{X_t}{t} = Z_t
        \end{align*}

        \subsubsection{Likelihood Ratio}
        Suppose we have a ``dataset'' $X_1,\dots,X_t$ that has a (true) underlying pdf $f$; suppose we have a hypothesis (or estimation) $g$.

        Let 
        \[ M_t \triangleq \frac{g(X_1)\cdot \cdots \cdot g(X_t)}{f(X_1)\cdot \cdots \cdot f(X_t)} \]

        We claim that $\{M_t\}$ is a martingale.

        \begin{align*}
            \mathbb{E}[M_{t+1}|X_{(1,t)}] &= M_t\cdot\mathbb{E}\left[\frac{g(X_{t+1})}{f(X_{t+1})}|X_{(1,t)}\right]\\
            &= M_t \cdot \mathbb{E}\left[\frac{g(X_{t+1})}{f(X_{t+1})}\right]\\
            &= M_t \int \frac{g(x)}{f(x)}\mathrm{d}x \\
            &= M_t
        \end{align*}


\section{Stopping Time}
    \subsection{Introduction}
        By definition \ref{Def:FancyMartingale},
        \[ \forall t \ge 0 \quad \mathbb{E}[Z_t] = \mathbb{E}[Z_0] \]

        If $\tau$ is a random variable, will the result still hold?
        \[\mathbb{E}[Z_{\tau}] \questeq \mathbb{E}[Z_0]\]
        This does not always hold, but in some cases it holds.

    \subsection{Stopping Time and Optional Stopping Theorem}
        \begin{definition}[Stopping Time]\label{Def:StoppingTime}
            A random variable $\tau \in \mathbb{N}$ is a \textbf{stopping time} if
            \[ \forall t \in \mathbb{N} \quad [\tau \le t] \text{ is $\mathcal{F}_t$-measurable.} \]
        \end{definition}
        \begin{remark}
            The definition says that $\tau$ is a stopping time with respect to a martingale $X_1,\dots$ if for each integer $k$, the indicator r.v. $\mathbb{I}[\tau = k]$ is a function of $W_{(0,k)}$.
        \end{remark}

        \begin{theorem}[Optional Stopping Theorem]\label{Thm:OptionalStoppingTheorem}
            Let $\{X_t\}$ be a matringale, let $\tau$ be a stopping time w.r.t. $\{\mathcal{F}_t\}$. Then $\mathbb{E}[X_{\tau}] = \mathbb{E}[X_0]$ if \emph{at least one of the following holds}.
            \begin{enumerate}
                \item $\tau$ is bounded.
                \item $\mathbb{P}[\tau < \infty] = 1$ and $\exists M$ s.t. $|X_t| \le M$ for all $t < \tau$.
                \item $\mathbb{E}[\tau] < \infty$ and $\exists c$ s.t. $\mathbb{E}[|X_{t+1} - X_{t}||\mathcal{F}_t] \le c$. (Expectation of $\tau$ is bounded and the changing rate of $X_t$ can be bounded)
            \end{enumerate}
        \end{theorem}

    \subsection{Examples}
        \subsubsection{Birth Rate of Boys and Girls}
            Suppose a politically incorrect village has a policy: Each family keep having children until the new child is a boy. We examine the stopping time $\tau$.

            Let $X_t \in \{-1, 1\}$ denote the gender of the child, and $1$ stands for boy and $-1$ stands for girl. Let $Z_t$ denote the difference in number of boys and girls.
            \[ Z_t = Z_{t-1} + X_t \]

            Obviously
            \[ \mathbb{E}[\tau] = 1/2 + 2 \times 1/4 + \cdots = \sum_n \frac{n}{2^n} < \infty \]
            and the changing rate is obviously bounded, so the third condition of theorem \ref{Thm:OptionalStoppingTheorem} holds, and
            \[ \mathbb{E}[Z_t] = \mathbb{E}[Z_0] = 0 \]

            However\footnote{“我前段时间还在论坛上和别人对线”--Chihao}, if the village uses an even more politically incorrect policy: keep having children until the number of boys is larger than the number of girls by 1. i.e.
            \[ \tau = \min\{n|Z_n=1\} \]
            
            Notice that if we model $X_t$ with the random walk on integers with probability $1/2$, the walk is null recurrent and $\mathbb{E}[\tau] = \infty$ so the theorem cannot be applied.
            
            Actually
            \[ \mathbb{E}[Z_t] = 1 \]

            Further, if the villagers are not extremely 头铁 and they will stop if they have 10 girls in a row without having a boy.
            \[ \tau = \min\{\min\{n|Z_n=1\}, 10\} \]
            Then condition (3) of Theorem \ref{Thm:OptionalStoppingTheorem} holds again and $\mathbb{E}[Z_t] = 0$.

        \subsubsection{Random Walk on Integers with Barrier}
            Consider a random walk $X_t \in \{-1, 1\}$ on integers with two absorbing barriers at $-a$ and $b$. Let $Z_t$ be the position at $t$, let $\tau$ be the r.v. denoting the time when we arrive at either $-a$ or $b$.
            \[ Z_{t+1} = Z_t + X_{t+1} \]

            Obviously $|Z_t| \le M$, and obviously $\mathbb{P}[\tau < \infty]=1$ because we have a positive probability to walk $(a+b)$ steps toward $b$ and thus we will always stop. So condition (2) of OST \ref{Thm:OptionalStoppingTheorem} holds.

            \[ \mathbb{E}[Z_t] = \mathbb{E}[Z_0] = 0 \Rightarrow -aP_a + b(1-P_a) = 0 \]

            So $P_a = \frac{b}{a+b}$ and $P_b = \frac{a}{a+b}$.

            And we proceed to compute the stopping time. Let
            \[ Y_t \triangleq Z_t^2 - t \]

            We claim that $Y_t$ is a martingale.
            \begin{align*}
                \mathbb{E}[Y_{t+1}|\mathcal{F}_t] &= \mathbb{E}[Z_{t+1}^2 - (t+1)|\mathcal{F}_t]\\
                &= \mathbb{E}[(Z_t + X_{t+1})^2 - (t+1) | \mathcal{F}_t]\\
                &= Z_t^2 + 2Z_t\mathbb{E}[X_{t+1}] + \mathbb{E}[X_{t+1}^2] - (t+1)\\
                &= Z_t^2 - t = Y_t
            \end{align*}

            It can be shown that $\{Y_t\}$ satisfies (3) of OST \ref{Thm:OptionalStoppingTheorem}. Therefore
            \[ \mathbb{E}[Y_{\tau}] = \mathbb{E}[Y_0] = 0 \]
            And since
            \[ \mathbb{E}[Y_{\tau}] = \mathbb{E}[Z_{\tau}^2] - \mathbb{E}[\tau] = \frac{a^2b}{a+b} + \frac{b^2a}{a+b} + \mathbb{E}[\tau] \]
            We can solve for
            \[ \mathbb{E}[\tau] = ab \]

        \subsubsection{Pattern Occurence}
            Suppose we flip a fair coin, let $\{X_t\}$ be the sequence of results. 
            
            Given a sequence $p = (p_1,p_2,\dots,p_N) \in \{0,1\}^N$, we want to know the expected throws before the pattern occurs (as a subsequence) in the result sequence.

            Notice that the expectation depends on $p$. For example it is easier to get sequence $10$ than to get $11$. Because the probability of succeeding, given the previous flip is $1$, is equal, but
            \begin{itemize}
                \item For $11$, if we fail and get $0$ in the second flip (got $10$), we will have to start from the beginning.
                \item For $10$, if we fail and get $1$ in the second flip, (got $11$), we only need to start from the second bit.
            \end{itemize}

            We construct a martingale, let $A_t$ be a person who joins a gambling at time $j$, and acts as follows
            \begin{itemize}
                \item Bet \$1 on $X_t=p_1$.
                \item If win, bet \$2 on $X_{t+1}=p_2$.
                \item If win, bet \$4 on $X_{t+2}=p_3$.
                \item \dots (doubles the bet each time)
                \item Whenever loses, quit the game.
            \end{itemize}

            Let $j \in \{1, 2, \dots, N\}$, let $G_j(t)$ be the total money (can be negative) of person $A_j$ at time $t$. Obviously $G_j(t)$ is a martingale, because this is a fair gamble.

            Let $Z(t) = \sum_j G_j(t)$ be the total number of money in the game at time $t$. By linearity of expectation, $Z(t)$ is also a martingale.

            Let $\tau$ be the first time that pattern $p$ occurs in $\{X_t\}$.

            It can be verified that $Z(t)$ satisfies (3) of OST \ref{Thm:OptionalStoppingTheorem}.
            \[ \mathbb{E}[Z(\tau)] = \mathbb{E}[Z(1)] = 0 \]

            Now suppose the pattern occurs, we compute the expected money for each person $A_j$ (which is another way of computing the LHS).

            All people before $\tau - N$ have all lost the gamble (or otherwise the pattern would have appeared earlier than $\tau$). If a person loses, the total money he has will be $-1$. 

            The person who joined the gamble at $t = \tau-N$ kept winning until the very end\footnote{“肯定赢麻了。”--Chihao}, and he will have earned
            \[ 1 + 2 + \cdots + 2^{N-1} = 2^N-1 \]

            The situation of the remaining people depens on $p$. For the $l$-last person, he did not lose until the end if and only if the first $l$ bits he betted equals to the first $l$ bits of $p$, i.e. the last $l$ bits of $p$ equals to the first $l$ bits of $p$. To indicate this, let $\chi_j$ denote whether $j$ winned money.
            \[ \chi_j = \mathbb{I}[p_{(1,j)} = \mathbb{I}p_{m-j+1,m}] \]

            Therefore
            \[Z(\tau) = -\left(\tau - \sum_{j=1}^m \chi_j\right) + \sum_{j=1}^m \chi_j(2^j-1)\]

            Take expectations on both sides and solve for $\mathbb{E}[\tau]$
            \[\mathbb{E}[\tau] = \sum_{j=1}^m\chi_j \cdot 2^j\]

            For a 4-bit pattern $0101$, $\chi_1=0$, $\chi_2 = 1$, $\chi_3=0$, $\chi_4=1$.


\section{Convergence of Non-negative Super-Martingales}
    \begin{proposition}\label{Prop:BoundOnSuperMartingale}
        Let $\{X_t\}$ be a nonnegative super-martingale. Suppose $X_0 \le a $, $\forall b > a$, let
        \[ T_b \triangleq \inf\{ X_t \ge b \} \]
        then
        \[ \mathbb{P}[T_b < \infty] \le a/b \]
    \end{proposition}

    Intuitively, when we are playing a super-martingale starting with money $a$, and we will stop whenever we reach $b$ (if it ever happens), then the larger $b$ is, the less likely we are to reach $b$ within finite steps (because we are expected to lose money in a super-martingale).

    \begin{proof}
        Let $T_b \wedge t =\triangleq \min\{T_b, t\}$. Since $T_b$ is not necessarily bounded, we cannot apply OST on $T_b$. However, for a fixed $t$, $T_b \wedge t$ is bounded.

        By OST \ref{Thm:OptionalStoppingTheorem}, for each non-negative $t$,
        \[ \mathbb{E}[X_{T_b \wedge t}] < \mathbb{E}[X_0] \le a \]

        Notice that
        \[ X_{T_b \wedge t} = \begin{cases}
            X_t &\quad T_b > t\\
            b &\quad T_b < t
        \end{cases} \]

        Therefore we can rewrite $X_{T_b \wedge t}$ into
        \[ X_{T_b \wedge t} \ge b\mathbb{I}[T_b \le t] \]

        Take expectation on both sides
        \[ a \ge \mathbb{E}[X_{T_b \wedge t}] \ge b\mathbb{P}[T_b \le t] \]

        Since $t$ is arbitrary,
        \[ \mathbb{P}[T_b < \infty] \le \mathbb{P}[T_b \le t] \le a/b \]
    \end{proof}

    \begin{theorem}[Convergence of Super-martingale]\label{Thm:ConvergenceOfSuperMartingale}
        Any non-negative super-martingale converges with probability 1.
    \end{theorem}
    \begin{proof}
        To prove the theorem, we only need to prove that it neither diverges nor oscillitates. That a super-martingale does not diverge is trivial, because it is lower-bounded. So we only prove that it does not have two sub-sequences that converges to different values.

        The intuition is, since $b > a$, whenever the super-martingale jumps from $a$ to $b$, the probability is $a/b$, if the super-martingale oscillitates, then it would have to jump from $a$ to $b$ for infinitely many times, and this probability would be zero.

        Formally, fix $a$ and $b$ s.t. $a < b$. Let $S_i$ denote the $i$-th time to reach $a$, and let $T_i$ denote the $i$-th time to reach $b$.
        \[ S_1 = \inf\{ t > T_0 | X_t = a \} \]
        \[ T_1 = \inf\{ t > S_1 | X_t = b \} \]
        \[ S_2 = \inf\{ t > T_1 | X_t = a \} \]
        
        $S_i$ and $T_i$ are not necessarily bounded, so OST cannot be directly applied. However, we can use the same trick and fix a upper bound: $\forall n \in \mathbb{N}$, the OST implies that
        \[ \mathbb{E}[X_{S_k \wedge n}] > \mathbb{E}[X_{T_k \wedge n}] \]

        Notice that
        \[ X_{T_k \wedge n} = \begin{cases}
            b &\quad T_k \le n\\
            X_n &\quad T_k > n
        \end{cases} \]
        Then
        \[ X_{T_k \wedge n} = b\mathbb{I}[T_k \le n] + X_n\mathbb{I}[T_k > n] \]

        Similarly
        \[ X_{S_k \wedge n} = a\mathbb{I}[S_k \le n] + X_n \mathbb{I}[S_k > n] \]

        Subtract the two equations,
        \[ X_{T_k \wedge n} - X_{S_k \wedge n} = b\mathbb{I}[T_k \le n] - a\mathbb{I}[S_k \le n] + X_n \left( \mathbb{I}[T_k > n] - \mathbb{I}[S_k > n] \right) \]

        Recall that by our definition, $S_k > T_k$ (if $S_k > n$, then $T_k >n$), so the latter term is non-negative and can thus be safely dropped.

        \[ X_{T_k \wedge n} - X_{S_k \wedge n} \ge b\mathbb{I}[T_k \le n] - a\mathbb{I}[S_k \le n]  \]

        Take expectation on both sides,
        \[ \mathbb{E}[X_{T_k \wedge n} - X_{S_k \wedge n}] \ge b\mathbb{P}[T_k \le n] - a\mathbb{P}[S_k \le n] \]

        By result from the OST we know that $\mathbb{E}[X_{S_k \wedge n}] - \mathbb{E}[X_{T_k \wedge n}] > 0$. Therefore
        \[ \mathbb{P}[T_k \le n] < \frac{a}{b}\mathbb{P}[S_k \le n] \]

        Since $n$ is arbitrary,
        \[ \mathbb{P}[T_k < \infty] < \frac{a}{b}\mathbb{P}[S_k < \infty] < \frac{a}{b}\mathbb{P}[T_{k-1} < \infty] \]

        So if $k \to \infty$, then $\mathbb{P}[T_k < \infty] \to 0$, so the super-martingale cannot oscillitate.
    \end{proof}


\section{Stochastic Approximation}
    Consider the problem of finding the root of $f(x)=0$. Usually, a binary search or the Newton's method can be applied. However, suppose we do not know what $f$ is, and we cannot observe the exact value of $f(x)$.

    $f(x)$ is sealed in a black box with noise, given an input query $x$, what we get is $\tilde{f}(x) + \eta$, where $\eta$ is some kind of random noise. Here we assume the noise has a Gaussian distribution with mean $0$ and variance $1$.

    So we hope to find a sequence of input $\{X_n\}$ such that $X_n \to x^*$ as $n \to \infty$.

    This problem can be solved using an iterative method. Let $X_n$ denote our $n$-th input, let $Y_n = f(X_n) + \eta_n$ be the output we get. In each iteration we update the input by
    \[ X_{n+1} = X_n - a_nY_n \]

    The problem is to determine $\{a_n\}$. Firstly it is obvious that $a_n$ should converge to $0$, or otherwise, suppose $a_n \to \delta$, and suppose the solution is $X^* = 0$, then even if we have reached the solution $X_n$, the algorithm will still perform an update $X_{n+1} = -\delta \eta_n$, and we moves away from the answer.

    \begin{theorem}[Stochastic Approximation]\label{Thm:StochasticApproximation}
        Let $f: \mathbb{R} \mapsto \mathbb{R}$ be a real-valued function. Suppose $\mathbb{E}[(X_0)^2] < \infty$. Consider the sequence generated by
        \[ Y_n = f(X_n) + \eta_n \]
        \[ X_{n+1} = X_n - a_n Y_n \]
        where we assume the following conditions
        \begin{enumerate}
            \item $X_0$, $\eta_1$, $\eta_2$, \dots, are independent, with $\eta_i$ having mean $0$ and variance $1$.
            \item For some $1 < |c| < \infty$, $|f(x)| \le c|x|$ for all $x$. (Notice that this implies $f(0)=0$).
            \item $\forall \delta > 0$, $\inf_{|x|>\delta} xf(x) > 0$. ($f(x) < 0$ for $x < x^*$ and $f(x) > 0$ for $x>x^*$).
            \item Each $a_n$ is non-negative and $\sum_n a_n = \infty$. ($a_n$ cannot decrease too fast)
            \item $\sum_n a_n^2 < \infty$. ($a_n$ cannot decrease too slowly)
        \end{enumerate}

        Then $X_n \to 0$ as $n \to \infty$.
    \end{theorem}
    \begin{proof}
        If we somehow proves that $X_n$ is a super-martingale, then by Theorem \ref{Thm:ConvergenceOfSuperMartingale}, we will be very close to success.

        Consider $\mathbb{E}[X_{n+1}^2 | X_{(0,n)}]$.\footnote{“不要问我为什么，我们经过一番尝试发现$X_n$的期望不太对。”--Chihao}
        \begin{align*}
            \mathbb{E}[X_{n+1}^2|X_{(0,n)}] &= \mathbb{E}[(X_n - a_n(f(X_n) + \eta_n))^2 | X_{(0,n)}]\\
            &= \mathbb{E}[X_n^2|X_{(0,n)}] - \mathbb{E}[2a_nX_n(f(X_n)+\eta_n)|X_{(0,n)}] + \mathbb{E}[a_n^2(f(X_n) + \eta_n)^2|X_{(0,n)}]\\
            &= X_n^2 - 2a_nX_nf(X_n) + a_n^2(f(X_n))^2 + a_n^2 \quad \text{($\eta_n$ has zero mean)}\\
            &\le X_n^2 + a_n^2\left(c^2 X_n^2 + 1\right) \quad \text{(all terms are non-negative and Assumption 2)}\\
            &\le X_n^2 + a_n^2\left(c^2 X_n^2 + c^2\right) \quad \text{(Assumption 2: $c > 1$)}\\
            &= \left( 1 + a_n^2c^2 \right)X_n^2 + a_n^2c^2
        \end{align*}
        
        Define
        \[ W_n \triangleq b_n(X_n^2 + 1) \]
        where
        \[ b_n = \prod_{k=1}^{n-1}(1 + a_k^2 c^2)^{-1} \]

        It can be verified that $W_n$ is a super-martingale.
        \begin{align*}
            \mathbb{E}[W_{n+1}|X_{(0,1)}] &= b_{n+1}\mathbb{E}[X_{n+1}^2|X_{(0,n)}] + b_{n+1}\\
            &\le b_{n+1}\left( X_n^2 \left( 1+a_n^2c^2 \right) + a_n^2c^2 \right) + b_{n+1}\\
            &= b_{n+1}(1+a_n^2c^2)(X_n^2+1) = b_n(X_n^2+1) = W_n
        \end{align*}

        So $W_n$ is a super-martingale and it will converge, by Theorem \ref{Thm:ConvergenceOfSuperMartingale}.

        Recall that
        \[ W_n \triangleq b_n(X_n^2 + 1) = \frac{(X_n^2 + 1)}{\prod_{k=1}^{n-1}(1 + a_k^2 c^2)} \]
        $\sum_n a_n^2 < \infty$ implies that the denominator also converges, and therefore $X_n^2$ converges.

        We then show that $X_n^2 \to 0$. Let $\delta > 0$ and let $D \triangleq \{x:|x| > \delta\}$. To show $X_n^2 \to 0$, it suffices to show that for any $m$, a ``bad event'' $B_m \triangleq \bigcap_{n \ge m}^{\infty} \{X_n \in D\}$ has probability 0.

        Suppose $m < n$.

        By Assumption 3, $\forall \delta > 0$, $\inf_{|x|>\delta} xf(x) \ge \epsilon > 0$. Therefore
        \[ X_nf(X_n) \ge \epsilon\mathbb{I}[X_n \in D] \ge \epsilon\mathbb{P}[B_m] \]

        Take expectation
        \[ \mathbb{E}[X_nf(X_n)] \ge \epsilon\mathbb{P}[X_n \in D] \]

        Notice that in the previous derivation of $\mathbb{E}[X_{n+1}^2|X_{(0,n)}]$, we dropped a term $- 2a_nX_nf(X_n)$, and if we add the term back,
        \[ \mathbb{E}[X_{n+1}^2|X_{(0,n)}] \le W_n - 2a_nb_{n+1}X_nf(X_n) \]

        Take expectations on both sides
        \[ \mathbb{E}[W_{n+1}] < \mathbb{E}[W_n] - 2a_nb_{n+1}\mathbb{E}[X_nf(X_n)] \]

        Therefore
        \[ \mathbb{E}[W_{n+1}] < \mathbb{E}[W_n] -2a_nb_{n+1}\epsilon\mathbb{P}[B_m] \]

        Using this result iteratively,
        \[ \mathbb{E}[W_{n+1}] < \mathbb{E}[W_m] - 2\epsilon\sum_{k=m}^n a_nb_{n+1}\mathbb{P}[B_m] \]

        So we have a upperbound for $\mathbb{P}[B_m]$
        \[ \mathbb{P}[B_m] \le \frac{\mathbb{E}[W_m]}{2\epsilon\sum_{k=m}^na_nb_{n+1}} \]

        The denominator goes to infinity, and therefore $\mathbb{P}[B_m]$ converges to 0, so we are done.
    \end{proof}