\chapter{Poison Processes}
\emph{“好想退休哦。”}
\newpage

\section{Poisson Distribution}
    \subsection{Poisson Distibution}
    \begin{definition}[Poisson Distribution]
        $X$ has a \textbf{Poisson distribution} with rate $\lambda$, denoted by $Poisson(\lambda)$, if
        \[ \mathbb{P}[X=k] = \frac{\lambda^k}{k!}e^{-\lambda} \]
    \end{definition}
    \begin{remark}
        If we view $\lambda$ as a constant, then the Poisson distribution can be seen as the limitation of a Binomial distribution $Bermoulli(n, p)$ with $p = \frac{\lambda}{n}$, as $n\to\infty$.
    \end{remark}
    \begin{theorem}
        If $n$ is large, then $Bernoulli(n, \lambda/n)$ is approximately $Poisson(\lambda)$.
    \end{theorem}

    Suppose we want to compute the number of customers arriving at some restaurant. If we divide the time interval to be sufficiently small s.t. at most one customer arrives at each interval and the probability is uniformly $p$, then the total number of arrivals has a Poisson distribution.

    \subsection{Expectation and Variance of Poisson Distribution}
    Let $X$ be a random variable with a Poisson distribution, then
    \[ \mathbb{E}[X] = \lambda \]
    \[ Var[X] = \lambda \]

    \subsection{Additivity of Poisson Distributions}
    \begin{theorem}[Additivity of Poisson Distributions]
        Let $X_1$, $X_2$ be two independent random variables, $X_1\sim Poisson(\lambda_1)$, $X_2\sim Poisson(\lambda_2)$, then
        \[ X_1 + X_2 \sim Poisson(\lambda_1 + \lambda_2) \]
    \end{theorem}
    \begin{proof}
        \begin{align*}
            \mathbb{P}[X_1=X_2=n] &=\sum_{m=0}^n \mathbb{P}[X_1=m, X_2=n-m]\\
            &= \sum_{m=0}^n\mathbb{P}[X_1=m]\cdot\mathbb{P}[X_2=n-m]\\
            &= \sum_{m=0}^n\frac{\lambda_1^m}{m!}e^{-\lambda_1}\cdot\frac{\lambda_2^{n-m}}{(n-m)!}e^{-\lambda_2}\\
            &= \frac{e^{-(\lambda_1+\lambda_2)}}{n!}\sum_{m=0}^n\frac{n!}{n!(n-m)!}\lambda_1^m\lambda_2^{n-m}\\
            &= \frac{e^{-(\lambda_1+\lambda_2)}}{n!}\sum_{m=0}^n\mathrm{C}_m^n \lambda_1^m \lambda_2^{n-m}\\
            &= \frac{(\lambda_1+\lambda_2)^n}{n!}e^{-(\lambda_1+\lambda_2)}
        \end{align*}
    \end{proof}


\section{Exponential Distributions}

    \subsection{Definition of Exponential Distributions}
    \begin{definition}[Exponential Distribution]
        A random variable $X$ is said to have an \textbf{exponential distribution} with rate $\lambda$, $X \sim Exp(\lambda)$ if
        \[ \forall t \ge 0 \quad \mathbb{P}[X \le t] = 1 - e^{-\lambda t} \]
    \end{definition}

    \subsection{Properties of Exponential Distribution}

        \subsubsection{Probability Density Function}
        The exponential distribution $Exp(\lambda)$ has probability density function
        \[ p(x) = \lambda e^{-\lambda x} \]

        \subsubsection{Expectation}
        \[ \mathbb{E}[X] = \int_0^{+\infty} t\lambda e^{-\lambda t}\mathrm{d}t = \frac{1}{\lambda} \]

        \subsubsection{Variance}
        \[ Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X] = \frac{1}{\lambda^2} \]

        \subsubsection{Lack of Memory}
        \[ \mathbb{P}[X>t+s|X>s] = \mathbb{P}[X \ge t] \]
        \begin{remark}
            If we have waited for $s$ units of time, then the probability that we have to wait for $t$ more units is the same as that of we have not waited at all.\footnote{白~{}等~{}了}
        \end{remark}

        \subsubsection{Exponential Races}
        Let $X_1 \sim Exp(\lambda_1)$, $X_2 \sim Exp(\lambda_2)$ be two independent random variables. Let $Y = \min(X_1, X_2)$, then
        \[ Y \sim Exp(\lambda_1 + \lambda_2) \]
        \begin{proof}
            \begin{align*}
                \mathbb{P}[Y \ge k] &= \mathbb{P}[\min\{X_1,X_2\} \ge k]\\
                &= \mathbb{P}[X_1 \ge k, X_2 \ge k]\\
                &= \mathbb{P}[X_1 \ge k]\cdot \mathbb{P}[X_2 \ge k]\\
                &= e^{-(\lambda_1 + \lambda_2)k}
            \end{align*}
        \end{proof}

        If we consider the problem of ``Who finishes first between $X_1$ and $X_2$'', then
        \[ \mathbb{P}[Y=X_1]=\sum_{s=0}^\infty \mathbb{P}[X_1=s,X_2>s] = \int_0^{+\infty}p_1(s)\mathbb{P}[X_2>s]\mathrm{d}s = \frac{\lambda_1}{\lambda_1 + \lambda_2} \]
        And an exponential race among $n$ random variables is
        \[ \mathbb{P}[Y=X_i] = \frac{\lambda_i}{\sum_j \lambda_j} \]


\section{Poisson Processes}

    \subsection{Definition of Poisson Processes}
    \begin{definition}[Poisson Process]\label{Def:PoissonProcess}
        $\{N(s)|s \ge 0\}$ is a \textbf{Poisson Process}, if
        \begin{enumerate}
            \item $N(0)=0$
            \item $\forall t, s \ge 0 \quad N(t+s) - N(s) \sim Poisson(\lambda t)$
            \item $n$ has \textbf{independent increments}: $\forall t_0 \le t_1 \le \dots \le t_n$,
            \[ N(t_1)-N(t_0), N(t_2)-N(t_1), \dots, N(t_n)-N(t_{n-1}) \]
            are \textbf{mutually independent}.
        \end{enumerate}
    \end{definition}

    \subsection{Alternative Interpretation of Poisson Process}
    We can construct a Poisson process as the sum of multiple random variables with Exponential distributions.
    \begin{proposition}\label{Prop:AltDefOfPoissonProcess}
        Let $\tau_1,\tau_2,\dots,\tau_n$ be independent random variables with exponential distribution $Exp(\lambda)$. Let $T_n=\sum_{i=1}^n\tau_i$, $N(s)=\max\{n|T_n \le s\}$

        Then $N(s)$ is a Poisson process with rate $\lambda$.
    \end{proposition}
    \begin{remark}
        If we think of $\tau_i$ as the time interval between the arrival time of customers in a store, then $N(s)$ is the number of customer arrivals before time $s$.
    \end{remark}

    To prove Proposition \ref{Prop:AltDefOfPoissonProcess}, we first introduce a theorem.

    \begin{theorem}\label{Thm:SumOfExpHasGammaDistribution}
        Let $\tau_1, \tau_2, \dots, \tau_n$ be independent random variables with Exponential distribution $Exp(\lambda)$. Let $T_n = \sum_{i=1}^n\tau_n$, then $T_n$ has a \emph{Gamma distribution} $\Gamma(n, \lambda n)$
        \[ f_{T_n}(t) = \lambda e^{-\lambda t}\frac{(\lambda t)^{n-1}}{(n-1)!} \quad \forall t \ge 0 \]
    \end{theorem}
    \begin{proof}
        Prove by induction on $n$.
        \begin{itemize}
            \item \textbf{Base. } $n=1$. $f_{T_1}(t) = \lambda e^{-\lambda t}$ obviouly holds.
            \item \textbf{Hypothesis. } Suppose it holds for $n$.
            \item \textbf{Step. }
            \begin{align*}
                f_{T_{n+1}}(t) &= \int_0^t f_{T_n}(s)\cdot\lambda e^{-\lambda(t-s)}\mathrm{d}s\\
                &= \int_0^t \lambda e^{-\lambda s}\frac{(\lambda s)^{n-1}}{(n-1)!}\lambda e^{-\lambda(t-s)}\mathrm{d}s \quad \text{(Plug in the hypothesis)}\\
                &= \frac{\lambda^{n+1}}{(n-1)!}e^{-\lambda t}\int_0^t s^{n-1}\mathrm{d}s \quad \text{(Move the constants out)}\\
                &= \frac{\lambda^{n+1}}{n!}e^{-\lambda t}t^n
            \end{align*}
            which is exactly $\Gamma(n+1, \lambda(n+1))$, so we are done.
        \end{itemize}
    \end{proof}
    \begin{remark}
        Theorem \ref{Thm:SumOfExpHasGammaDistribution} states that the arrival time of the $n$-th customer has a Gamma distribution.
    \end{remark}

    We can now prove Proposition \ref{Prop:AltDefOfPoissonProcess}.
    \subsection{Proof of Equivalence of the two Definitions}
        \begin{proof}
            To show that the two definitions are equivalent, we need to show that the three requirements in Definition \ref{Def:PoissonProcess} can be satisfied by the alternative definition \ref{Prop:AltDefOfPoissonProcess}.
            \begin{enumerate}
                \item $N(0)=0$ is trivial.
                \item We begin proving (2) of Definition \ref{Def:PoissonProcess} from the case $s=0$. Notice that $N(t)=n \Leftrightarrow T_n \le t \wedge T_{n+1} > t$, i.e. the $n$-th customer has arrived, but the $(n+1)$-th has not. So we only need to prove the latter has Poisson distribution.
                \begin{align*}
                    \mathbb{P}[N(t)=n] &= \mathbb{P}[T_n \le t, T_{n+1} > t]\\
                    &= \int_0^tf_{T_n}(s)\cdot\mathbb{P}[\tau_{n+1} > t-s]\mathrm{d}s \quad \text{(Enumerate all possible values of $t$)}\\
                    &= \int_0^t\lambda e^{-\lambda s}\frac{(\lambda s)^{n-1}}{(n-1)!}\cdot e^{-\lambda(t-s)}\mathrm{d}s \quad \text{(Theorem \ref{Thm:SumOfExpHasGammaDistribution})}\\
                    &= \frac{\lambda^n}{(n-1)!}e^{-\lambda t}\int_0^t s^{n-1}\mathrm{d}s \quad \text{(Move out the constants)}\\
                    &= \frac{(t\lambda)^n}{n!}\cdot e^{-\lambda t}
                \end{align*}
                which is exactly the expression of $Poisson(\lambda t)$, so we are done.

                When $s > 0$, by the lack of memory property, $N(t+s)-N(s)$ must have the same distribution as $N(t) - N(0) = N(t)$. Of course this can also be verified by some moderate calculations.
                \item (3) of Definitiion \ref{Def:PoissonProcess} can be proved using a similar argument. Notice that the lack-of-memory property and (2) implies that the ``number of arrivals'' after $s$ is independent of the arrivals before $s$. Therefore $N(t_n) - N(t_{n-1})$ is independent of $N(r)$ for all $r < t_{n-1}$. So the result can be proved by induction.
            \end{enumerate}
        \end{proof}


\section{Thinning}
    Thinning is associating another random variable with a Poisson Process.

    \subsection{Thinning}
        We can associate some i.i.d. random variables $Y_i$ with each arrival. For example, $Y_i$ can be the gender of the $i$-th customer arriving at some restaurant; or if customers arrive in cars, $Y_i$ can be the number of passengers in a car. Let $P_j = \mathbb{P}[Y_i = j]$. Let $N_j(t)$ be the total number of $i \le N(t)$ with $Y_i = j$.

        \begin{theorem}\label{Thm:ThinningOfPoissonProcess}
            $N_j(t)$ are \emph{independent} Poisson Processes with rate $\lambda P_j$.
        \end{theorem}
        \begin{proof}
            We will prove the result using the first definition of Poisson Distribution \ref{Def:PoissonProcess}. We only need to consider the simplest cases where $Y_i \in \{0,1\}$.
            \begin{align*}
                \mathbb{P}[N_0(t) = j, N_1(t) = k] &= e^{-\lambda t}\frac{(\lambda t)^{j+k}}{(j+k)!}\cdot C^{j+k}_j P_0^j P_1^k \\
                &= e^{-\lambda t}\frac{(\lambda t)^{j+k}}{(j+k)!}\frac{(j+k)!}{j! \cdot k!}P_0^j P_1^k\\
                &= e^{-P_0\lambda t}\frac{(P_0\lambda t)^j}{j!} \cdot e^{-P_1\lambda t} \frac{(P_1 \lambda t)^k}{k!}
            \end{align*}
            This is exactly the product of two Poisson Processes, with rate $P_0\lambda$ and $P_1\lambda$ respectively. Therefore we have completed our proof.
        \end{proof}
        \begin{remark}
            One tricky fact behind this theorem is that, suppose we are interested in the gender of customers arriving at a restaurant. Assume that the probability that the customer is male or female are equal. If one day 40 males came to the restaurant, this does not give any implication to the number of females coming to the restaurant because they are independent.
        \end{remark}

    \subsection{Examples}
        \subsubsection{Finding typos in a book.} Two editors read a 300-page manuscript. Editor $A$ finds $100$ typos, Editor $B$ finds $120$ typos, and $80$ of these typos are the same. Suppose $A$ and $B$ have probability $P_A$ and $P_B$ of discovering a typo, and suppose the typos in the book is a rate $\lambda$ Poisson Process. How can we estimate $\lambda$, $P_A$ and $P_B$?
    
        We can associate a random variable with each typos appearing in the manuscript.
        \begin{enumerate}
            \item Neither $A$ or $B$ found it. $(1-P_A)(1-P_B)$
            \item Only $A$ found it. $P_A(1-P_B)$
            \item Only $B$ found it. $P_B(1-P_A)$
            \item Both $A$ and $B$ found it. $P_AP_B$
        \end{enumerate}
        From Theorem \ref{Thm:ThinningOfPoissonProcess}, we know that the four cases follow four independent Poisson Processes. So we can estimate the parameters by solving
        \[
        \begin{cases}
            300P_A(1-P_B)\lambda &= 20\\
            300P_B(1-P_B)\lambda &= 40\\
            \lambda P_AP_B &= 80
        \end{cases}
        \quad \Longrightarrow \quad
        \begin{cases}
            P_A &= 2/3\\
            P_B &= 4/5\\
            \lambda &= 1/2
        \end{cases}
        \]
    
        \subsubsection{Coupon Collector Once Again.}
        We revisit the Coupon Collector's problem, but this time we assume the probability of the $j$-th coupon is $P_j$.
        \begin{itemize}
            \item Let $N_j$ be the number of gachas before we first get a coupon of the $j$-th category.
            \item Let $N = \max_{1 \le j \le n}\{N_j\}$ is the total number of gachas we need to collect all categories of coupons.
        \end{itemize}
        We want to find $\mathbb{E}[N]$.

        We re-formulate the problem into the following: suppose we owns a restaurant, and the customers coming to the restaurant is a Poisson Process of rate $\lambda = 1$. Each customer brings a coupon, let $Y_j$ be the category of the coupon brought by the $j$-th customer. Therefore $N_j(t)$ is a Poisson Process with rate $P_j$. Let $X_j$ be the random variable \emph{in this Poisson Process setting} denoting the first time to meet a customer with a category $j$ coupon, and we now alternatively consider $X = \max \{X_j\}$.

        We are interested in $\mathbb{P}[X \le t]$, i.e. the probability that we have got all categories of coupons before time $t$.
        \begin{align*}
            \mathbb{P}[X \le t] &= \mathbb{P}[X_1 \le t, X_2 \le t, \dots X_n \le t]\\
            &= \mathbb{P}[X_1 \le t] \cdot \mathbb{P}[X_2 \le t] \cdot \cdots \cdot \mathbb{P}[X_n \le t]
        \end{align*}
        By the alternative definition of Poisson Process \ref{Prop:AltDefOfPoissonProcess}, $X_j$ has an Exponential Distribution with rate $P_j$. Therefore
        \[ \mathbb{P}[X \le t] = \prod_{i=1}^n (1-e^{-P_jt}) \]
        \[ \mathbb{P}[X > t] = 1 - \prod_{i=1}^n (1-e^{-P_jt}) \]
        \begin{proposition}\label{Prop:AltComputationOfExpectation}
            If $X \ge 0$, then
            \[ \mathbb{E}[X] = \sum_{t=0}^{\infty} \mathbb{P}[X \ge t] = \sum_{i=0}^{\infty}i\cdot\mathbb{P}[X=i] \]
            \[ \mathbb{E}[X] = \int_0^{\infty} \mathbb{P}[X > t]\mathrm{d}t\]
        \end{proposition}
        By Proposition \ref{Prop:AltComputationOfExpectation},
        \[ \mathbb{E}[X] = \int_0^{\infty} \mathbb{P}[X > t]\mathrm{d}t = \int_0^{\infty}1 - \prod_i (1-e^{-P_it})\mathrm{d}t \]
        We have figured out $\mathbb{E}[X]$, but how is $\mathbb{E}[X]$ related to $\mathbb{E}[N]$?

        Notice that $X$ denotes the exact time at which the customer carrying the $N$-th coupon arrives. If $\tau_i$ denotes the time interval between the arrival of the $(i-1)$-th and the $i$-th customers, then
        \[ X = \sum_{i=1}^N \tau_i \]
        Take expectation on both sides
        \[ \mathbb{E}[X] = \mathbb{E}\left[ \sum_{i=1}^N \tau_i \right] = \mathbb{E}[N]\cdot\mathbb{E}[\tau_1] = \mathbb{E}[N] \]
        Notice that $N$ is a random variable, but we somehow still exchanged the summation and expectation in the second equation, this holds because of Wald's Equation, which will be covered in the future. For now, notice that once $N$ is determined, $\mathbb{E}[X|N]$ is determined.
        \[ \mathbb{E}[X|N] = N\mathbb{E}[\tau_1] = N \]
        Take expectations on both sides again
        \[ \mathbb{E}[\mathbb{E}[X|N]] = \mathbb{E}[X] = \mathbb{E}[N] \]
        \begin{remark}
            The original Coupon Collector's problem is a special case where $P_j$ are uniform. It can be proved\footnote{“我也不会积，但是我用Mathematica跑了一下他们确实相等” -- Chihao} that the integral equals to the sum of harmonic series.\footnote{“这个积分我会积” -- Kuan}\footnote{“不愧是常州数学帝” -- Chihao}
        \end{remark}

    \subsection{Nonhomogeneous Thinning}
        Now consider another case. Suppose $P_i$ now depends on the arrival time $s$.
        \begin{theorem}[Nonhomogeneous Thinning]
            \label{Thm:NonhomogeneousThinning}
            $N_i(t)$'s have \emph{independent} Poisson distribution with mean $\lambda\int_0^t P_i(s)\mathrm{d}s$.
        \end{theorem}
        \begin{proof}
            Suppose there are $k$ categories. Suppose until time $t$, each type $i$ have total number $n_i$.
            \begin{align*}
                &\mathbb{P}[N_1(t)=n_1,\dots,N_k(t)=n_k]\\
                &=\mathbb{P}\left[ N_1(t)=n_1,\dots,N_k(t)=n_k|N(t)=\sum_i n_i \right]\mathbb{P}\left[ N(t) = \sum_i n_i \right] \quad \text{Always holds}\\
                &= \frac{\left(\sum_{i=1}^kn_i\right)!}{n_1!\cdots n_k!}p_1^{n_1}\cdots p_k^{n_k} \cdot e^{-\lambda t}\frac{(\lambda t)^{\sum_in_i}}{(\sum_i n_i)!}\\
                &= \prod_{i=1}^k e^{-\lambda t P_i} \frac{(\lambda t P_i)^{n_i}}{n_i!}
            \end{align*}
            where
            \[ P_i = \frac{1}{t}\int_0^t P_i(s)\mathrm{d}s \]
        \end{proof}
        \begin{remark}
            Actually it is a non-homogeneous Poisson Process with rate
            \[ \lambda(t) = \lambda\cdot P_i(t) \]
        \end{remark}

        \begin{definition}[Nonhomogeneous Poisson Process]\label{Def:NonhomogeneousPoissonProcess}
            $\{ N(t) | t \ge 0 \}$ is a \textbf{nonhomogeneous Poisson Process} with rate $\lambda(s)$ if
            \begin{enumerate}
                \item $N(0)=0$
                \item $N(t)$ has independent increments.
                \item $N(t)-N(s)$ has distribution $Poisson\left(\int_s^t \lambda(\tau)\mathrm{d}\tau\right)$
            \end{enumerate}
        \end{definition}
        \begin{remark}
            In this case, the time intervals are not exponential distributions, and they are not independent.
        \end{remark}

    \subsection{Applications of Nonhomogeneous Thinning}

        \subsubsection{M/$G$/$\infty$ Queue}
        Consider a queue at some counters in a bank. M stands for a Poisson Process representing the incoming flow of customers. $G$ is a distribution of the time to serve the customers, $G(t) = \mathbb{P}[ServiceTime \le t]$. $\infty$ is the number of counters. Let
        \begin{itemize}
            \item $X(t)$: the number of customers completed services before time $t$.
            \item $Y(t)$: the number of customers being served at time $t$.
        \end{itemize}

        Notice that
        \[ p_1(s) = G(t-s) \quad p_2(s) = 1 - G(t-s) = \bar{G}(t-s) \]
        Therefore
        \[ \mathbb{E}[X(t)] = \mathbb{E}[N_1(t)] = \lambda\int_0^t G(t-s)\mathrm{d}s = \lambda\int_0^t G(s)\mathrm{d}s \]
        \[ \mathbb{E}[Y(t)] = \mathbb{E}[N_2(t)] = \lambda\int_0^t \bar{G}(s)\mathrm{d}s \]

        \subsubsection{Cars in a Tunnel}
        Consider a tunnel\footnote{“刚才Kuan提醒我隧道内不能超车” -- Chihao} of length $l$. Suppose we are driving a car, once the car enters the tunnel, its speed cannot change. Assume other NPC cars enter the tunnel with a Poisson distribution of rate $\lambda$. Suppse the speeds of NPC cars have a distribution $G$, and are also fixed once an NPC car enters the tunnel.

        We want to choose an initial speed $x$ to minimize the time we 超车 or 被超车。

        Let $t_0 = l/x$. If we enter the tunnel at time $s$, then we must exit the tunnel at time $s + t_0$. For an NPC car with speed $X$, the time interval that the NPC car stays in the tunnel is $T=l/X$.

        \[ F(t) = \mathbb{P}[T < t] = \mathbb{P}[l/X < t] = \mathbb{P}[X > l/t] = \bar{G}[l/t] \]

        Let $t$ denote the time when an NPC car enters the tunnel. We divide NPC cars into $3$ categories:
        \begin{enumerate}
            \item 超车。$t > s$ and $t + T < s + t_0$
            \item 被超车。$t < s$ and $t + T > s + t_0$
            \item No thing happens. Omitted.
        \end{enumerate}
        At any given time $t$, we have
        \[
            p(t) =
            \begin{cases}
                \mathbb{P}[t + T > s + t_0] = \bar{F}(s+t_0-t) \quad &(t < s)\\
                \mathbb{P}[t + T < s + t_0] = \bar{F}(s+t_0-t) \quad &(s < t < s+t_0)\\
                0 \quad &(t > s + t_0)
            \end{cases}
        \]
        Notice that only $T$ in the above equation is a random variable.
        \begin{align*}
            \mathbb{E}[N_{encountered}(s+t_0)] &= \lambda \left( \int_0^s \bar{F}(s-t_0-t)\mathrm{d}t + \int_s^{s+t_0} F(s+t_0-t)\mathrm{d}t \right)\\
            &= \lambda \left( \int_{t_0}^{s+t_0} \bar{F}(t)\mathrm{d}t + \int_0^{t_0} F(t)\mathrm{d}t \right)
        \end{align*}
        To maximize the expectation, we take the derivative w.r.t. $t_0$.
        \[ \frac{\mathrm{d}}{\mathrm{d}t_0} = \lambda \left( \bar{F}(s+t_0) - \bar{F}(t_0) + F(t_0) \right) = 0 \]
        If the system has been running for a long time, $\bar{F}(s+t_0) \approx 0$, so
        \[ F(t_0) = \bar{F}(t_0) = \frac{1}{2} \]
        and
        \[ G\left(\frac{l}{t_0}\right) = \frac{1}{2} \]
        So we choose an initial speed $x$ such that
        \[ G(x) = \frac{1}{2} \]

        \subsubsection{HIV Patients}
        We want to estimate the number of HIV patients, suppose a patient may or may not show symptoms. Assume the number of patients with HIV virus has a Poisson distribution with an unknown rate $\lambda$. Suppose we know a distribution $G$ representing ``how long a patient with HIV virus shows symptoms''.

        We divide people into 2 types by whether they show symptoms or not.
        \begin{equation}\label{Eq:NumOfPatientsWithSymptoms} \mathbb{E}[N_1(t)] = \lambda \int_0^t G(t-s)\mathrm{d}s = \lambda\int_0^t G(s)\mathrm{d}s \end{equation}
        \[ \mathbb{E}[N_2(t)] = \lambda \int_0^t \hat{G}(t-s)\mathrm{d}s = \lambda\int_0^t \hat{G}(s)\mathrm{d}s \]
        where $\lambda$ is unknown.

        However, we do know the number of patients who have already shown symptoms. Therefore we can estimate $\lambda$ with Eq (\ref{Eq:NumOfPatientsWithSymptoms}), and estimate $\mathbb{E}[N_2(t)]$ with $\hat{\lambda}$.

\section{Conditioning}

    \subsection{Conditioning}
        Let $T_1, T_2, \dots, T_n$ be the arrival times of a Poisson Process with rate $\lambda$. Let $U_1, U_2, \dots, U_n$ be independent uniform random variables on $[0,t]$. Let $V_1, V_2, \dots, V_n$ be sequence $U_i$ re-arranged in increasing order, then
        \begin{theorem}[Conditioning]\label{Thm:ConditioningOfPoissonProcess}
            If we condition on $N(t)=n$, then the distribution of $T_1, \dots, T_n$ is the same as the distribution of $V_1, \dots, V_n$.
        \end{theorem}
        \begin{sketchproof}~{}
            \begin{itemize}
                \item The joint distribution of $T_1,\dots,T_n$ given $N(t)=n$ is $n!/t^n$ (by brutal force calculation).
                \item The resulting distribution is uniform over $[0,t]$ because the space has volume $t^n$ and $n!$ possible orderings.
            \end{itemize}
        \end{sketchproof}
        \begin{remark}
            This property is useful when computing $\mathbb{P}[N(s)=m|N(t)=n]$
            \[ \mathbb{P}[N(s)=m|N(t)=n] = C^n_m \left(\frac{s}{t}\right)^m\left(1-\frac{s}{t}\right)^{n-m} \]
        \end{remark}


\section{Poisson Approximation}
    \subsection{Motivating Example: Max Load}\label{subsec:m-balls-in-n-bins}
        Consider a m-ball-into-n-bin problem. Suppose we throw balls randomly into several bins, and we want to know how many balls there are in the bin with the most balls.

        If we consider the number of balls in each bin, the number follows a Bernoulli distribution. However, since the number of balls in different bins are not independent, analyzing the problem in this way can be complicated.

        Alternatively, we can analyze the problem by constructing a Poisson Process.

        \begin{itemize}
            \item Let $X_i$ be the number of balls in the $i$-th bin.
            \item $X_i \sim Bernoulli(m, \frac{1}{n})$.
            \item $\sum_i X_i = m$.
            \item We are interested in $X = \max X_i$.
        \end{itemize}

    \subsection{Poisson Approximation}
        \begin{theorem}[Poisson Approximation]\label{Thm:PoissonApproximation}
            Let $(X_1,\dots,X_n)$ be a sequence of random variables where each $X_i$ has a Bernoulli distribution, then its distribution is the same as $(Y_1, \dots, Y_n)$ conditioned on $\sum_i Y_i = m$, where $Y_i \sim Poisson(\lambda)$ are \emph{independent} Poisson variables with rate $\lambda$.
        \end{theorem}
        \begin{proof}
            Let $a_1, \dots, a_n \ge 0$ be $n$ positive integers s.t. $\sum_i a_i = m$. We calculate $\mathbb{P}[X_1 = a_1, \dots, X_n = a_n]$ and $\mathbb{P}[Y_1 = a_1, \dots, Y_n = a_n|\sum_i Y_i=m]$ to show that they are equal to each other.

            We can think of $a_1,\dots,a_n$ as several bins on an axis, and we throw balls into these bins. There are $n^m$ different kind of throws and $m!$ different permutations of different balls. However, the order of balls in the same bin does not matter, so we shoud divide $m!$ by $a_1!\cdot \cdots \cdot a_n!$.
            \begin{align*}
                \mathbb{P}[X_1 = a_1, \dots, X_n = a_n]
                &= \frac{1}{n^m} \cdot \frac{m!}{a_1! \dots a_n!}
            \end{align*}

            \begin{align*}
                \mathbb{P}[Y_1 = a_1, \dots, Y_n = a_n | \sum_iY_i = m]
                &= \frac{\mathbb{P}[Y_1=a_1,\dots,Y_n=a_n]}{\mathbb{P}[\sum_i Y_i = m]}\\
                &= \frac{\prod_{i=1}^n \mathbb{P}[Y_i = a_i]}{\mathbb{P}[\sum_i Y_i = m]} \quad \text{(Independence of $Y_i$)}\\
                &= \frac{\prod_{i=1}^n e^{-\lambda}\frac{\lambda^{a_i}}{a_i!}}{e^{-\lambda n} \cdot \frac{(\lambda n)^m}{m!}} \quad \text{(Denominator is a sum of Poisson)}\\
                &= \frac{1}{n^m} \cdot \frac{m!}{a_1! \dots a_n!} \quad \text{(Mafs)}
            \end{align*}

        \end{proof}

    \subsection{m-balls-in-n-bins: MaxLoad Revisited}
        With Theorem \ref{Thm:PoissonApproximation}, we can solve the motivating example in Section \ref{subsec:m-balls-in-n-bins}.
        \begin{theorem}
            Let $m=n$. Let $X = \max_i X_i$. Then there exist constants $c_1$, $c_2$ such that
            \[ \mathbb{P}\left[\frac{c_1\log n}{\log\log n} < X < \frac{c_2\log n}{\log\log n}\right] = 1 - O\left(\frac{1}{n}\right) \]
        \end{theorem}
        \begin{proof}
            We first proof the \emph{upper bound}. Proving the upper bound is equivalent to proving
            \[\mathbb{P}\left[ X \ge \frac{c_2\log n}{\log\log n} \right] = O\left(\frac{1}{n}\right) \]
            \begin{align*}
                \mathbb{P}\left[ X \ge \frac{c_2\log n}{\log\log n} \right] &= \mathbb{P}[\exists i: X_i \ge k]\\
                &\le n\mathbb{P}[X_1 \ge k] \quad \text{(Union Bound)}\\
                &\le n C^k_n\left(\frac{1}{n}\right)^k\\
                &\le n\cdot \left(\frac{en}{k}\right)^k n^{-k} \quad \text{(Mafs)}\\
                &= n\cdot\left(\frac{e}{k}\right)^k
            \end{align*}
            Therefore it is sufficient to show that $n\cdot e^k / k^k < 1/n^{1+\epsilon}$ for some $\epsilon > 0$. Take logarithm on both sides,
            \[ (2+\epsilon)\log n + k - k\log k < 0 \]
            Hence $k\log k > 3\log n$ suffices. Plug in $k = \frac{c_2\log n}{\log\log n}$.
            \begin{align*}
                k\log k &= \frac{c_2\log n}{\log\log n} (\log\log n - \log\log\log n + \log c_2)\\
                &> c_2\log n\left(1- \frac{\log\log\log n}{\log\log n}\right)\\
                &> \frac{c_2}{2}\log n
            \end{align*}
            So we can choose $c_2 = 6$.
            
            The proof of \emph{lower bound} requires Theorem \ref{Thm:BadEventBound}. Let $X < \frac{c_1\log n}{\log\log n}$ be a ``bad event''. This is equivalent to
            \[ \forall i, \quad X_i < \frac{c_1\log n}{\log\log n} \]
            Denote this bad event by $\mathcal{B}$, let $f = \mathbb{I}[\mathcal{B}|X_1,\dots,X_n]$ be an indicator of the bad event. By Theorem \ref{Thm:BadEventBound},
            \begin{align*}
                \mathbb{E}[f] &= \mathbb{P}[\mathcal{B}]\\
                &\le e\sqrt{n} \mathbb{E}[\mathbb{I}[\mathcal{B}|(Y_1,\dots,Y_n)]]\\
                &= e\sqrt{n} \mathbb{P}[\forall i \quad Y_i < \frac{c_1\log n}{\log\log n}] \quad \text{(Independence of $Y_i$)}\\
                &= e\sqrt{n}\prod_{i=1}^n \mathbb{P}[Y_i < RHS]\\
                &= e\sqrt{n}(1-\mathbb{P}[Y_i \ge RHS])^n\\
                &\le e\sqrt{n}(1-\mathbb{P}[Y_i = RHS])^n\\
                &= e\sqrt{n}\left(1-\frac{1}{ek!}\right)^n\\
                &\le e\sqrt{n} e^{-\frac{n}{ek!}} < \frac{1}{n} \quad \text{(Mafs)}
            \end{align*}
        \end{proof}
        \begin{theorem}\label{Thm:BadEventBound}
            Let $X_i$ be Bernoulli random variables, and let $Y_i$ be independent Poisson random variables with rate $\lambda = m/n$. For any function $f$ from $\mathbb{N}^n$ to $\mathbb{N}$,
            \[ \mathbb{E}[f(X_1, \dots, X_n)] \le e\sqrt{m}\cdot\mathbb{E}[f(Y_1,\dots,Y_m)] \]
        \end{theorem}
        \begin{proof}
            \begin{align*}
                \mathbb{E}[f(Y_1,\dots,Y_n)] &= \sum_{k=0}^{\infty}\mathbb{E}[f(Y_1,\dots,Y_n) | \sum_i Y_i = k] \cdot \mathbb{P}[\sum_i Y_i = k] \quad \text{(Total Probability)}\\
                &\ge \mathbb{E}[f(Y_1,\dots,Y_n)|\sum Y_i = m]\cdot \mathbb{P}[\sum Y_i = m]\\
                &= \mathbb{E}[f(X_1,\dots,X_m)]\cdot\mathbb{P}[\sum Y_i = m] \quad \text{(Thm \ref{Thm:PoissonApproximation})}
            \end{align*}
            If we let each $Y_i$ has a Poisson distribution of rate $m/n$, by additivity of Poisson variables we have $\sum Y_i \sim Poisson(m)$, and then
            \begin{align*}
                \mathbb{E}[f(Y_1,\dots,Y_n)] &= \mathbb{E}[f(X_1,\dots,X_n)]\cdot e^{-m}\frac{m^m}{m!}\\
                &\ge \mathbb{E}[f(X_1,\dots,X_n)]\cdot\frac{1}{e\sqrt{m}}
            \end{align*}
        \end{proof}
        \begin{remark}~{}
            \begin{itemize}
                \item $f$ is usually chosen to be some indicator function $\mathbb{I}$ of some ``bad event''. (In order to show that the probability that this ``bad event'' happens is very small.)
                \item Choosing $\lambda = m/n$ is to maximize $\mathbb{P}[\sum_i Y_i=m]$.
            \end{itemize}
        \end{remark}
