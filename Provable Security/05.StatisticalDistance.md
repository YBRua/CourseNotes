# Statistical Distance/Indistinguishability

## Security Definition via Indistinguishability Experiment

### Adversarial Indistinguishability Experiment

Assume an encryption scheme $\Pi = ï¼ˆ\mathrm{Gen}, \mathrm{Enc}, \mathrm{Dec})$. We can define a security game between an adversary $A = (A_1, D)$ and a challenger $C$.

$$\begin{matrix}
    C & & A=(A_1, D)\\
    k \leftarrow \mathrm{Gen}(1^n) & \xleftrightarrow{(m_0, m_1)} & (m_0, m_1) \leftarrow A_1(1^n)\\
    b\leftarrow U_1; c = \mathrm{Enc}_k(b_b) & \xrightarrow{c} & b' \leftarrow D(c)
\end{matrix}$$

The adversarial indistinguishability experiment $\mathrm{PrivK}^{eav}$ outputs $1$ iff $b = b'$.

The advantage of the adversary is given by

$$ \mathrm{Pr}\left[ \mathrm{PrivK}^{eav}_{A,\Pi} = 1 \right]  - \frac{1}{2}.$$

Note that randomly guessing would result in a probability of $b = b'$ of $1/2$.

Intuitively, the adversary selects two messages $m_0,m_1$, the challenger randomly choose one of them, and the adversary aims to find out to which plaintext the ciphertext belong.

### Statistical Security

**Statistical Security.** An encryption scheme $\Pi$ over message space $\mathcal{M}$ is $\epsilon$-secure (in presence of eavesdroppers) if for every adversary $A$ it holds that

$$ \mathrm{Pr}\left[ \mathrm{PrivK}^{eav}_{A,\Pi} = 1 \right] \le \frac{1}{2} + \frac{\epsilon}{2}. $$

If $\epsilon = 0$, we say that $\Pi$ is **perfectly secure**.

Note that $\epsilon$-security implies

$$ \frac{1}{2} - \frac{\epsilon}{2} \le \mathrm{Pr}\left[ \mathrm{PrivK}^{eav}_{A,\Pi} = 1 \right] \le \frac{1}{2} + \frac{\epsilon}{2}. $$

The lower bound implies that otherwise the adversary could gain more advantage by inverting its guesses.

**Alternative Definition of Statistical Security.** An encryption scheme $\Pi$ over message space $\mathcal{M}$ and ciphertext space $\mathcal{C}$ is $\epsilon$-secure if for every distinguisher $D: \mathcal{C} \mapsto \{0, 1\}$ and every pair of $m_0 \neq m_1 \in \mathcal{M}$ it holds that

$$ \left| \mathrm{Pr}_{k \leftarrow \mathrm{Gen}(1^n)} \left[ D(\mathrm{Enc}_k(m_0)) = 1 \right] - \mathrm{Pr}_{k \leftarrow \mathrm{Gen}(1^n)} \left[ D(\mathrm{Enc}_k(m_1)) = 1 \right] \right| \le \epsilon.$$

#### Example: One-Time-Pad (OTP)

The **one-time-pad (OTP)** scheme is defined as

1. For any security parameter$n$, $\mathcal{M} = \mathcal{C} = \mathcal{K} = \{0, 1\}^n$.
2. On input $1^n$,.

**Proposition.** One-time-pad is perfectly secure.

*Sketch of proof.*

- $\mathrm{Enc}_k(m_0) = m_0 \oplus k \sim U_n, \mathrm{Enc}_k(m_1) = m_1 \oplus k \sim U_n$.

## Statistical Distance

The **statistical distance** between $X$ and $Y$, denoted by $\mathrm{SD}(X,Y)$, is defined by

$$ \mathrm{SD}(X, Y) = \frac{1}{2}\sum_{x} | \mathrm{Pr}[X=x] - \mathrm{Pr}[Y = x]|. $$

For joint random variables $(X, Z)$ and $(Y,Z)$ with a common $Z$, we use the shorthand

$$ \mathrm{SD}(X,Y|Z) \triangleq \mathrm{SD}((X,Z), (Y,Z)). $$

### Upper Bound of Distinguishing Advantage

The statistical distance $\mathrm{SD}(X,Y)$ gives an upper bound for the advantage of a distinguisher.

For random variables $X$ and $Y$ defined over $\mathcal{S}$ and any distinguisher $D: \mathcal{S} \mapsto \{0, 1\}$, it holds that

$$ | \mathrm{Pr}[D(X) = 1] - \mathrm{Pr}[D(Y) = 1] | \le \mathrm{SD}(X, Y). $$

## Entropy

### Renyi Entropy

The **Renyi Entropy** of order $\alpha$, where $\alpha \ge 0$ and $\alpha \neq 1$, is defined as

$$ \mathrm{H}_\alpha(X) = \frac{1}{1 - \alpha} \log \left( \sum_{i=1}^n p_i^\alpha \right). $$

Note that in general we have

$$ \mathrm{H}_0(X) \ge \cdots \ge \mathrm{H}_\infty(X). $$

### Hartley or Max-Entropy

The **Hartley Entropy** is the logarithm of the *cardinality* of $X$

$$ \mathrm{H}_0(X) = \log n = \log |X|. $$

### Shannon Entropy

The **Shannon Entropy** is defined as the limiting value of $\mathrm{H}_\alpha$ as $\alpha \to 1$,

$$ \mathrm{H}_1(X) = \lim_{\alpha \to 1} \mathrm{H}_\alpha(X) = \sum_{i=1}^n p_i \log p_i. $$

### Collision Entropy

The **Collision Entropy**, or sometimes called **Renyi Entropy**, is defined as

$$ \mathrm{H}_2(X) = -\log \sum_{i=1}^n p_i^2 = -\log \mathrm{Pr}(X = Y), $$

where $X$ and $Y$ are i.i.d. variables.

### Min-Entropy

The limiting value of $\mathrm{H}_\alpha$ as $\alpha \to \infty$.

$$ \mathrm{H}_\infty(X) = \min_i(-\log p_i) = -\log \max_i p_i. $$

- In cryptography the min-entropy is commonly used.
- But the Shannon entropy is not used as much because there are cases where $\mathrm{H}_\infty(X)$ is small but $\mathrm{H}_1(X)$ is large.
  - The implication is that, while in average (Shannon entropy), the entropy seems to be large, in extreme cases (Min-entropy), the entropy is actually small.
  - The adversary usually targets the most vulnerable (most likely, i.e., highest prob / lowest entropy) cases.

## Two More Lemmas on Indistinguishability

### Indistinguishability Amplification

For independent bitstrings $\mathcal{S}_1,\dots,\mathcal{S}_l$, distributed over $\mathbb{F}_2^n$, we have the following bound on the statistical distance of their XOR sum from uniform,

$$ \mathrm{SD}\left( \bigoplus_{i=1}^l S_i, U_n \right) \le \frac{1}{2}\prod_{i=1}^l (2\delta_i) $$

where $\delta_i = \mathrm{SD}(S_i, U_n)$ for every $1 \le i \le l$.

### Piling-Up Lemma

For $0 < \mu < 1/2$ and $l \in \mathbb{N}^+$,

$$ \mathrm{Pr}\left[ \bigoplus_{i=1}^l E_i = 0:E_1,\dots,E_l \leftarrow \mathcal{B}_u \right] = \frac{1}{2}(1 + (1-2\mu)^l). $$

## Unpredictability

### Unpredictability

A random variable $X$ is $\epsilon$-unpredictable if for every algorithm $A$ it holds that

$$ \mathrm{Pr}[A(1^n) = X] \le \epsilon .$$

**Unpredictability and Min-Entropy.** A random variable $X$ is $\epsilon$-unpredictable iff $\mathrm{H}_\infty(X) \ge \log(1/\epsilon)$. The proof is straightforward as the best guess for $A$ is to output $X$ with the maximum probability.

### Conditional Unpredictability

### Average Min-Entropy

### High Min-Entropy does not Imply Security

In general, encryptions are not secure by using keys that have high min-entropy entropy.

**Example.** Consider a OTP scheme, where the first key is always $0$ and we only encrypt the last $n-1$ bits.

- $\mathrm{H}_\infty(K) = n - 1$.
- However, the adversary can choose $m_0$ and $m_1$ to start with 0 and 1 respectively and always win the game (because the first bit is never encrypted).

### Randomness Extractors

> A randomness extractor is a function, which being applied to output from a weak entropy source, together with a short, uniformly random seed, generates a highly random output that appears independent from the source and uniformly distributed.

A randomness extractor is used to "recycle" random bits from high-entropy sources.

An $(n,k,m,d,\epsilon)$-randomness extractor is a function $\mathrm{Ext}: \{0,1\}^n \times \{0,1\}^d \mapsto \{ 0,1 \}^m$ such that for every $X$ of length $n$ and min-entropy at least $k$ we have

$$ \mathrm{SD}((\mathrm{Ext}(X, U_d), U_d), U_{m+d}) \le \epsilon, $$

where the $d$-bit string is a uniform random seed.

#### Importance of Random Key

Consider a *deterministic* version,

An $(n, k, m, \epsilon)$-randomness extractor $\mathrm{Ext}: \{ 0, 1 \}^n \mapsto \{ 0, 1 \}^m$ is a function that for every $X$ of length $n$ and min-entropy at least $k$ we have

$$ \mathrm{SD}(\mathrm{Ext}(X), U_m) \le \epsilon. $$

Assume such extractors exist, and we consider the most simple case, where $k=n-1$ and $m=1$.

Define $S_b = \{ X: \mathrm{Ext}(X) = b \}$. We have $S_0 \cap S_1 = \emptyset$ and $S_0 \cup S_1 = \{ 0, 1 \}^n$. From the definition we have $\exists S_b: |S_b| \ge 2^{n-1}$.

Let $X$ be a uniform distribution on $S_b$.

## Leftover Hash Lemma

### Finite Field Arithmetics

The **finite field** (a.k.a. Galois Field) with $p^n$ elements is denoted $\mathrm{GF}(p^n)$

### Universal Hash Functions

$H \subseteq \{ h: \{0, 1\}^L \mapsto \{0, 1\}^t \}$ is a family of universal has functions for any distinct $x_1, x_2 \in \{0, 1\}^L$,

$$ \mathrm{Pr}[h(x_1) = h(x_2): h \leftarrow H] \le \frac{1}{2^t}. $$

### The Leftover Hash Lemma

For any integers $d \le k \le l$, let $\mathcal{H} \subseteq \{ h: \{0, 1\}^l \mapsto \{0,1\}^{k-d} \}$ be a family of universal hash functions. Then for any random variable $X$ defined over $\{0,1\}^l$ with min-entropy $\mathrm{H}_\infty(X) \ge k$ it holds that

$$ \mathrm{SD}(\mathrm{H}(X), U_{k-d} | \mathrm{H}) = \mathrm{SD}((\mathrm{H}(X), H), (U_{k-d}, H)) \le 2^{-\frac{d}{2} - 1}. $$

where $H$ is uniformly distributed over all members of $\mathcal{H}$.

We refer to $d$ as the *entropy loss* (the difference between the amount of entropy and the number of random bits extracted) and $H$ as the *random seed*.
