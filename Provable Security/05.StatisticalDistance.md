# Statistical Distance/Indistinguishability

## Security Definition via Indistinguishability Experiment

### Adversarial Indistinguishability Experiment

Assume an encryption scheme $\Pi = ï¼ˆ\mathrm{Gen}, \mathrm{Enc}, \mathrm{Dec})$. We can define a security game between an adversary $A = (A_1, D)$ and a challenger $C$.

$$\begin{matrix}
    C & & A=(A_1, D)\\
    k \leftarrow \mathrm{Gen}(1^n) & \xleftrightarrow{(m_0, m_1)} & (m_0, m_1) \leftarrow A_1(1^n)\\
    b\leftarrow U_1; c = \mathrm{Enc}_k(b_b) & \xrightarrow{c} & b' \leftarrow D(c)
\end{matrix}$$

The adversarial indistinguishability experiment $\mathrm{PrivK}^{eav}$ outputs $1$ iff $b = b'$.

The advantage of the adversary is given by

$$ \mathrm{Pr}\left[ \mathrm{PrivK}^{eav}_{A,\Pi} = 1 \right]  - \frac{1}{2}.$$

Note that randomly guessing would result in a probability of $b = b'$ of $1/2$.

Intuitively, the adversary selects two messages $m_0,m_1$, the challenger randomly choose one of them, and the adversary aims to find out to which plaintext the ciphertext belong.

### Statistical Security

**Statistical Security.** An encryption scheme $\Pi$ over message space $\mathcal{M}$ is $\epsilon$-secure (in presence of eavesdroppers) if for every adversary $A$ it holds that

$$ \mathrm{Pr}\left[ \mathrm{PrivK}^{eav}_{A,\Pi} = 1 \right] \le \frac{1}{2} + \frac{\epsilon}{2}. $$

If $\epsilon = 0$, we say that $\Pi$ is **perfectly secure**.

Note that $\epsilon$-security implies

$$ \frac{1}{2} - \frac{\epsilon}{2} \le \mathrm{Pr}\left[ \mathrm{PrivK}^{eav}_{A,\Pi} = 1 \right] \le \frac{1}{2} + \frac{\epsilon}{2}. $$

The lower bound implies that otherwise the adversary could gain more advantage by inverting its guesses.

**Alternative Definition of Statistical Security.** An encryption scheme $\Pi$ over message space $\mathcal{M}$ and ciphertext space $\mathcal{C}$ is $\epsilon$-secure if for every distinguisher $D: \mathcal{C} \mapsto \{0, 1\}$ and every pair of $m_0 \neq m_1 \in \mathcal{M}$ it holds that

$$ \left| \mathrm{Pr}_{k \leftarrow \mathrm{Gen}(1^n)} \left[ D(\mathrm{Enc}_k(m_0)) = 1 \right] - \mathrm{Pr}_{k \leftarrow \mathrm{Gen}(1^n)} \left[ D(\mathrm{Enc}_k(m_1)) = 1 \right] \right| \le \epsilon.$$

#### Example: One-Time-Pad (OTP)

The **one-time-pad (OTP)** scheme is defined as

1. For any security parameter$n$, $\mathcal{M} = \mathcal{C} = \mathcal{K} = \{0, 1\}^n$.
2. On input $1^n$,.

**Proposition.** One-time-pad is perfectly secure.

*Sketch of proof.*

- $\mathrm{Enc}_k(m_0) = m_0 \oplus k \sim U_n, \mathrm{Enc}_k(m_1) = m_1 \oplus k \sim U_n$.

## Statistical Distance

The **statistical distance** between $X$ and $Y$, denoted by $\mathrm{SD}(X,Y)$, is defined by

$$ \mathrm{SD}(X, Y) = \frac{1}{2}\sum_{x} | \mathrm{Pr}[X=x] - \mathrm{Pr}[Y = x]|. $$

### Upper Bound of Distinguishing Advantage

The statistical distance $\mathrm{SD}(X,Y)$ gives an upper bound for the advantage of a distinguisher.

For random variables $X$ and $Y$ defined over $\mathcal{S}$ and any distinguisher $D: \mathcal{S} \mapsto \{0, 1\}$, it holds that

$$ | \mathrm{Pr}[D(X) = 1] - \mathrm{Pr}[D(Y) = 1] | \le \mathrm{SD}(X, Y). $$

## Entropy

### Renyi Entropy

The **Renyi Entropy** of order $\alpha$, where $\alpha \ge 0$ and $\alpha \neq 1$, is defined as

$$ \mathrm{H}_\alpha(X) = \frac{1}{1 - \alpha} \log \left( \sum_{i=1}^n p_i^\alpha \right). $$

### Hartley or Max-Entropy

### Shannon Entropy

The **Shannon Entropy** is defined as the limiting value of $\mathrm{H}_\alpha$ as $\alpha \to \infty$,

$$ \mathrm{H}_1(X) = \lim_{\alpha \to 1} \mathrm{H}_\alpha(X) = \sum_{i=1}^n p_i \log p_i. $$

### Collision Entropy

The **Collision Entropy**, or sometimes called **Renyi Entropy**, is defined as

$$ \mathrm{H}_2(X) = -\log \sum_{i=1}^n p_i^2 = -\log \mathrm{Pr}(X = Y), $$

where $X$ and $Y$ are i.i.d. variables.

### Min-Entropy

## Two More Lemmas on Indistinguishability

### Indistinguishability Amplification
