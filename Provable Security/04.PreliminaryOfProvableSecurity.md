# Preliminaries of Provable Security

## Introduction

### Writing a proof

- **Conditionally.** By a reduction
- **Unconditionally.**
- **Contradiction.**

### Examples

#### Binary Linear Code

A **binary linear code** $(m,n)$-code is a set of codewords $\mathcal{C} \subseteq \mathbb{F}_2^m$ with $|\mathcal{C}| = 2^n (n < m)$, and a binary linear $[m,n]$-code is a binary $(m,n)$-code that is the row span of some generator matrix $\mathbf{C} \in \mathbb{F}_2^{n \times m}$.

- Basically we encode a bitstring of length $n$ into one with length $m$.
- The encoding process introduces redundancy since it is encoded from a shorter string to a longer one.

**Nearest Codeword Problem (NCP).** The nearest codeword problem $\mathtt{NCP}_{n,m,w}$ for $n,m,w \in \mathbb{N}$ refers to that given the input of a matrix $\mathbf{C} \in \mathbb{F}_2^{n \times m}$ of a binary linear code $\mathcal{C}$ and a noisy codeword $\mathbf{t}^T = \mathbf{s}^T\mathbf{C} + \mathbf{x}^T$ for some $\mathbf{s} \in \mathbf{F}_2^n$ and $\mathbf{x} \in \mathbb{R}_w^m$. The challenge is to find out a solution $\mathbf{s}'$ such that $\mathbf{s}^T\mathbf{C} + \mathbf{x} = \mathbf{s}'^T\mathbf{C} + \mathbf{x}'^T$ for some $\mathbf{x}' \in \mathbb{R}_w^m$.

- Intuitively, due to lengths and noise/corruptions, we might have multiple possible solutions at decoding stage.
- As long as we are able to recover one of the possible decodings, we consider it a success.

**Proposition (Unique Decoding of NCP).** Most linear codes have a unique decoding when codeword length $m$ is long enough. For $w/m < 1/4$,

$$ \mathrm{Pr}_{\mathbf{C} \sim \mathbb{F}_{2}^{m \times n}} [\exists \mathbf{s}_1 \neq \mathbf{s}_2 \in \mathbb{F}_2^n, \exists \mathbf{t}_1, \mathbf{t}_2 \in \mathbb{F}_2^m: |\mathbf{t}_1|, |\mathbf{t}_2| \le w \land (\mathbf{s}_1^T \mathbf{C} + \mathbf{x}_1^T = \mathbf{s}_2^T \mathbf{C} + \mathbf{x}_2^T)] $$

is upper bounded by $2^{m + n + w2 \log m}$.

*Sketch of Proof.*

Let $\mathbf{s} = \mathbf{s}_1 - \mathbf{s}_2$

## Mathematical Background

### Probability

#### Random Variables, Values, Sets

We use capital letters ($X, Y, A$) for random variables, standard letters $(x,y,a)$ for values, and calligraphic letters $\mathcal{X}, \mathcal{Y}, \mathcal{A}$ for sets and events.

#### Set and Set Operations

In this course we assume discrete and finite set (unless stated otherwise). A set can be defined by enumeration.

$$ \mathcal{S} = \{ 0,1,\dots,99 \} $$

- $|\mathcal{S}|$ denotes the cardinality of $\mathcal{S}$.
- $\mathcal{S}_1 \times \mathcal{S}_2$ denotes the cartesian product of $\mathcal{S}_1$ and $\mathcal{S}_2$.

#### Probability Distributions

For any non-empty set $\mathcal{X}$ (the sample space), a (discrete) probability distribution $X$, defined over $\mathcal{X}$, refers to the rule that assigns a numeric value (i.e., probability that $X = x$) to each outcome $x \in \mathcal{X}$.

#### Uniform Distribution

For a distribution $X$ defined over $\mathcal{X}$, $X$ is uniform (or flat) if for every possible outcome $x \in \mathcal{X}$ it holds that

$$ \mathrm{Pr}[X = x] = \frac{1}{|\mathcal{X}|}. $$

- $U_\mathcal{X}$ denotes the uniform distribution over $\mathcal{X}$.
- $U_n$ denotes the uniform distribution over $\{0, 1\}^n$.

#### Random Variables

A **random variable** is a function that maps elements of the sample space to another set (usually, but not necessarily, $\mathbb{R}$).

- Note that we often use *random variable* and *probability distribution* interchangeably.

#### Events and Independence of Events

An **event** is a (finite) set of outcomes (a subset of the sample space) to which a probability is assigned. Typically, when the sample space is finite, any subset of the sample space is an event.

The event that an outcome of $U_n$ falls into subset $\mathcal{X}$, denoted by $\mathcal{E}$ has probability

$$ \mathrm{Pr}[\mathcal{E}] = \mathrm{Pr}[U_n \in \mathcal{X}] = \sum_{x \in \mathcal{X}} \mathrm{Pr}[U_\mathcal{X} = x] = \frac{1}{2^n}\mathcal{|X|} $$

#### Bayes Theorem

Let $\mathcal{E}_1, \mathcal{E}_2$ be two events over the sample space and $\mathrm{Pr}[\mathcal{E}_2] \neq 0$,

$$ \mathrm{Pr}[\mathcal{E}_1 | \mathcal{E}_2] = \frac{\mathrm{Pr}[\mathcal{E}_1] \cdot \mathrm{Pr}[\mathcal{E_2 | \mathcal{E}_1}]}{\mathrm{Pr}[\mathcal{E}_2]}. $$

### Polynomials

#### Polynomial

**Polynomial.** A function $\mathrm{poly}(\cdot)$ is a **polynomial** (in a single indeterminate) iff it can be represented as

$$ \mathrm{poly}(n) = a_cn^c + a_{c-1}n^{c-1} + \cdots + a_0n_0,$$

where $c$ is called the **degree** of the polynomial.

#### Others

**Super-polynomial.** A function $\mathrm{superpoly}(\cdot)$ is **super-polynomial** if for every constant $c>0$, $\mathrm{superpoly}(n) > n^c$ for all sufficiently large $n$'s.

**Negligible.** A function $\mathrm{negl}(\cdot)$ is **negligible** if for every constant $c>0$ it holds

**Overwhelming.** A function $\mathrm{overwhelm}(\cdot)$ is **overwhelming** if $\mu(n) = 1 - \mathrm{overwheml}(n)$ is negligible.

**Non-negligible.** A function $\mu(\cdot)$ is **non-negligible** if there exists a constant $c>0$ such that

$$ \mu(n) \ge n^{-c} $$

for *infinitely many* $n$'s.

**Noticable.** A function $\mu(\cdot)$ is **noticable** if there exists a constant $c>0$ such that

$$ \mu(n) \ge n^{-c} $$

for *all sufficiently large* $n$'s.

#### Asymptotic Functions

**Function.** Informally, $f(\cdot)$ denotes a function that takes one argument. A more formal notation will specify the domain and range of the function,

$$ f(x) \mapsto ax + b \quad g: \{0,1\}^n \mapsto \{0,1\}^n $$

### Some Useful Inequalities

#### Union Bound

Let $\mathcal{S}$ be a sample space and $\mathcal{E}_1, \mathcal{E}_2$ be two events over $\mathcal{S}$,

$$ \mathrm{Pr}[\mathcal{E}_1 \cup \mathcal{E}_2] \le \mathrm{Pr}[\mathcal{S}_1] + \mathrm{Pr}[\mathcal{S}_2]. $$

#### Markov Inequality

Let $X$ be any random variable that takes non-negative real numbers. For any $\delta > 0$,

$$ \mathrm{Pr}[X \ge \delta \mathbb{E}[X]] \le \frac{1}{\delta}. $$

#### Chebyshev's Inequality

Let $X$ bge a random variable with expectation $\mu$ and standard deviation $\sigma$. For any $\delta > 0$,

$$ \mathrm{Pr}[|X - \mu| \ge \delta\sigma] \le 1/\sigma^2. $$

#### Chernoff and Hoeffding Bounds

**Chernoff Bound.** Let $X_1,\dots,X_n$ be independent variable swith $0 \le X_i \le 1$. Denote $\mu = \mathbb{E}[\sum_i X_i/n]$. Then for any $\epsilon > 0$ or any $0 < \delta < 1$,

$$ \mathrm{Pr}\left[ \left| \frac{\sum_{i=1}^n X_i}{n} - \mu \right| > \epsilon \right]  < 2^{-\epsilon^2n},$$

#### Piling-up Lemma