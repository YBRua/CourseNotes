# 规划与学习

## 回顾：策略评估与策略提升

### 值函数估计

$$ V^{\pi}(s) = \mathbb{E}_{a\sim\pi(s)}[Q^{\pi}(s,a)] $$

$$ Q^{\pi}(s,a) = R(s,a) + \gamma \sum_{s'} P_{s\pi(s)}(s')V^{\pi}(s') $$

### 策略提升

对于两个策略 $\pi$ 和 $\pi'$，如果满足如下条件

$$ \forall s \quad Q^{\pi}(s, \pi'(s)) \ge V^{\pi}(s) $$

则 $\pi'$ 是 $\pi$ 的策略提升

特别地，如果

1. 在某个状态下，两个策略输出不同，且 $Q^{\pi}(s,\pi'(s)) > Q^{\pi}(s,\pi(s))=V^{\pi}(s)$
2. 在其他所有状态下，两个策略输出相同

则 $\pi'$ 是 $\pi$ 地一种策略提升

#### 策略提升定理

如果 $\pi'$ 是 $\pi$ 的策略提升，则对任何状态

$$ V^{\pi'}(s) \ge V^{\pi}(s) $$

因此有一个策略 $\pi$后，可以迭代地更新策略，使新策略的价值更高

1. 评估策略价值 $V^{\pi}(s)$
2. 寻找新策略 $\pi'$ 使 $\pi'$ 满足策略提升

## 规划与学习入门

### 模型

给定一个状态和动作，模型能够预测下一个状态和奖励的分布，即 $\mathbb{P}[s',r|s,a]$

#### 分类

- 白盒模型（分布模型）
  - 描述了轨迹的所有可能性及其概率
- 黑盒模型（样本模型）
  - 根据概率进行采样，只产生一条可能的轨迹

#### 作用

得到模拟的模拟数据（simulated experience）

### 规划

输入一个模型，输出一个策略的搜索过程

#### 分类

- 状态空间的规划
  - 在状态空间搜索最佳策略
- 规划空间的规划
  - 在规划空间搜索最佳策略
  - 此时，一个规划就是一个动作集合以及动作顺序的约束
  - 此时的状态是一个规划，目标状态是能完成任务的规划

#### 通用框架

- 通过模型得到模拟数据
- 利用模拟数据更新值函数，从而改进策略

### Q-Planning

#### 与学习的异同

- 不同点
  - 规划使用模型产生的模拟经验
  - 学习使用环境产生的真实经验
- 相同点
  - 通过回溯更新值函数估计
  - 学习的方法也可以用在模拟经验上

#### 框架

- 重复以下步骤
  1. 随机选择状态 $s$  和动作 $a$
  2. 将 $s$ 和 $a$ 输入采样模型，获得奖励 $r$ 和 $s'$
  3. 根据 $(s,a,r,s')$ 进行 Q学习

环境模型是否准确会影响模型的实际性能，因此通常需要应用在环境非常好学习的应用场景中

## Dyna 算法

> ウルトラマンダイナ

Dyna算法使用经验更新值函数和恶略，同时更新模型

### 框架

- 和环境交互产生真实经验
- 使用真实经验直接强化学习更新
- 同时使用真实经验学习模型
- 使用模型产生的模拟经验规划更新策略

### 算法

- 初始化值函数 $Q(s,a)$ 和模型 $Model(s,a)$
- 重复以下步骤
  1. $s$ 更新为当前非终止状态
  2. $a = greedy(s, Q)$
  3. 执行动作 $a$，获得奖励 $r$ 和 新状态 $s'$
  4. 使用 $Q$-Learning 更新值函数 $Q$
  5. 使用 $r, s'$ 更新模型
  6. 重复以下步骤 $n$ 次
     1. 随机采样之前观测到的状态 $s$（每轮新循环重新采样，不是在上一步基础上采样）
     2. 随机采样在 $s$ 状态下做过的状态 $a$
     3. 根据模型 $Model(s,a)$ 获得 $r$, $s'$
     4. 使用 $Q$-Planning 规划更新值函数

### 小结

- 综合了规划、决策和学习
- 在环境模型比较准确的情况下，可以较多地依赖规划（$n$ 选大一点）
- 否则可以选择较小的 $n$，在训练过程中学习到较准确的环境模型

### 应对环境变化

#### 模型可能不准确

- 环境是随机的，但是我们只观测到了有限的样本
- 模型使用了泛化性不好的环境估计
- 环境改变了，但是模型没有发现环境改变

#### Dyna-Q+

- 应对环境不准确或环境在变化的情况
- 将奖励从 $r$ 更改为 $r+\mathcal{K}\sqrt{\tau}$
  - $r$: 原本的奖励
  - $\mathcal{K}$: 权重参数，通常较小
  - $\tau$: 某个状态多久没有到达过了
  - 鼓励模型探索一些长时间没到达的状态

!!!note Reward Shaping
    对 Reward 进行一些修改以达到一些特定的目的（例如让模型尽可能探索未知状态等）。但是这种修改需要满足一些特定条件，使最终学到的策略和本来的策略一致。

    - 例如 $1/n_s$ 可以鼓励智能体在行动时尽可能覆盖到所有状态

## :robot:采样方法

> 通过模拟场景让智能体更快地学到一些真实环境里不容易见到的经验

### 优先级采样

定义优先级队列，其中优先级 $P$ 定义为

$$ P \leftarrow |R + \gamma\max_a Q(s',a) - Q(s,a)| $$

#### 使用优先级采样的Dyna-Q

- 初始化值函数 $Q(s,a)$ 、模型 $Model(s,a)$和优先级队列 $Pq$
- 重复以下步骤
  1. $s$ 更新为当前非终止状态
  2. $a = \epsilon greedy(s, Q)$
  3. 执行动作 $a$，获得奖励 $r$ 和 新状态 $s'$
  4. 更新优先级 $P$
  5. 如果 $P > \theta$，则将 $(s,a)$ 以优先级 $P$ 插入 $Pq$
  6. 重复以下步骤 $n$ 次
     1. $(s,a)$ 为优先级队列 $Pq$ 首个元素
     2. 根据模型 $Model(s,a)$ 获得 $r$, $s'$
     3. 更新 $Q$
     4. 对于所有能够到达 $s$ 的 $\bar{s}$, $\bar{a}$
        1. 令 $\bar{r}$ 为模型对于 $\bar{s},\bar{a},s$ 预测的奖励
        2. $P = |\bar{r} + \gamma\max_a Q(s,a) - Q(\bar{s},\bar{a})|$
        3. 若 $P > \theta$，则将 $(\bar{s}, \bar{a})$ 以优先级 $P$ 插入

### 期望更新和采样更新

- 期望更新
  - $Q(s,a) = \sum_{s',r} \mathbb{P}[s',r|s,a](r + \gamma\max_{a'}Q(s',a'))$
  - 需要分布模型
  - 需要更大计算量
  - 没有偏差，更准确
- 采样更新
  - $Q(s,a) =Q(s,a) + \alpha [r + \gamma\max_{a'}Q(s',a') - Q(s,a)] $
  - 只需要采样模型
  - 计算量需求更低
  - 受采样误差影响，准确率相对较低
