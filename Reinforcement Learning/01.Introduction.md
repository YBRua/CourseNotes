# 导论

> 序列决策问题

## 基础概念

> 通过从交互中学习来实现目标的计算方法

- 智能体（Agent）
  - 感知状态
  - 采取行动：影响状态或达到目标
  - 达到目标：最大化累计奖励
- 交互过程
  - Agent不同，交互出的数据也不同

> “要在第1分钟到第90分钟都换下C罗。”
> “结果发现C罗根本就不work。”

`Placeholder`

> “你第一次越线超车”
> “你发现”
> “欸，还挺爽啊。”

### 强化学习系统要素

#### 历史（History）

- **历史**是观察、行动和奖励的序列

$$ H_t = [O_1, R_1, A_1, \dots, O_{t-1}, R_{t-1}, A_{t-1}, O_t, R_t] $$

- 即到 $t$ 未知所有可观测的变量

#### 状态（State）

- **状态**是用于确定接下来会发生的事情的信息，是历史的函数

$$ S_t = f(H_t) $$

#### 策略（Policy）

- **策略**是智能体在特定时间的行为方式，是从状态到行动的映射
- **确定性策略**：$a = \pi(s)$
- **随机策略**：$\pi(a|s) = \mathbb{P}(A_t=a|S_t=s)$

#### 奖励（Reward）

- **奖励** $R(s, a)$ 是定义强化学习目标的**标量**

#### 价值函数（Value Function）

- 标量，用于定义对于长期来说什么是“好”的
- 是对于未来累计奖励的预测
- 给定Policy，才有对应的价值

##### 例子

$$\begin{aligned}
  Q_{\pi}(s,a) &= \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \cdots | S_t=s, A_t=a]\\
  &=\mathbb{E}_{\pi}[R_{t+1} + \gamma Q_{\pi}(s',a')|S_t=s, A_t=a]
\end{aligned}$$

#### 模型（Model）

- **环境的模型**用于模拟环境的行为
  - 预测下一个状态：$\mathcal{P}_{ss'}^{a} = \mathbb{P}[S_{t+1}=s'|S_t=s, A_t=a]$
  - 预测下一个奖励：$\mathcal{R}_{s}^{a}=\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$

### 方法

- 基于价值
  - 没有策略
  - 有价值函数
- 基于策略
  - 没有价值函数
  - 直接优化策略
- Actor-Critic
  - 有策略
  - 有价值函数
