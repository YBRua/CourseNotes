# Supervised Learning

## Notations

- $X$: Input feature vector
- $x_i$: Denotes the $i$-th element in $X$
- $Y$: Output response. A scalar or a vector of binary/real values
- $G$: Qualitative response that takes value from a discrete set $\mathcal{G}$, e.g., $\{red, green\}$. $G$ is often transformed to $Y$.

## Problem

- 200 points drawn from two (unknown) distributions in $\mathbb{R}$.
  - 100 from each of $\mathcal{G} = \{ green, red \}$
- Build a model to predict the color of unknown points

## Linear Regression and K-Nearest Neighbours

### Linear Regression

Let $Y = 1$ if $G$ is red and $Y=0$ otherwise.

- Model $Y$ as a linear function of $X$

$$ \bm{Y} = \beta_0 + \sum_{j=1}^{p} x_j \beta_j = \bm{X}^{T}\bm{\beta} $$

- Obtain $\bm{\beta}$ by least squares.

$$ RSS(\bm{\beta}) = \sum_{I=1}^N (y_i - \bm{x}_i^T\bm{\beta})^2 = \| \bm{X}\bm{\beta} - \bm{y} \|^2 $$

$$ \min_{\bm{\beta}} \| \bm{X}\bm{\beta} - \bm{y} \|^2 \Rightarrow \bm{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}$$

- Note that we assume $\bm{X}^T\bm{X}$ is invertible.
- For linear regression (in classification tasks), the decision boundry is $\bm{x}^T\bm{\beta} = 0.5$.
  - Note that this decision boundary is also linear.

### K-Nearest Neighbours

Look at the neighbours of a data point and take a vote

$$ Y_k(x) = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i. $$

- $N_k(x)$ is a neighbourhood of $x$ containing exactly $k$ neighbours.
- The classification rule is the majority voting of members in $N_k(x)$.
  - If there is a clear dominance of one class in the neighbourhood of $x$, then $x$ is likely to belong to that class.

#### Trade-off on $k$

- Small neighbourhood: Might not be accurate enough.
- Large neighbourhood: Does not make sense to use too many neighbours because different points would have the same neighbourhood

## Other Classification Methods

There are other modern classification methods, which can be considered variants of linear regression or K-Nearest Neighbours.

- Kernel smoothers,
- Local linear regression,
- Linear basis expansion,
- ...

## Statistical Decision Theory

### Quantitative Output $Y$

- Let $X \in \mathbb{R}^p$ be a real-valued vector, and $Y \in \mathbb{R}$ be a real valued random output variable, with joint distribution $Pr(X, Y)$.
- We seek a function $f(X)$ for predicting $Y$ given the inputs $X$.
- The method requires a loss function $L(X, f(Y))$ for penalizing errors in prediciton.
- The most common and convenient loss is the squared error loss.
  - $L(Y, f(X)) = (Y - f(X))^2$
- This leads to a criterion for choosing $f$, the **expected (squared) prediction error (EPE)**.

$$ EPE(f) = \mathbb{E}[Y - f(X)]^2 = \int (y-f(x))^2 \mathrm{Pr}(\mathrm{d}x, \mathrm{d}y). $$

We can re-write $EPE$ by conditioning on $X$

$$\begin{align*}
  EPE(f) &= \mathbb{E}[Y - f(X)]^2 \\
  &= \int (y-f(x))^2 p(x, y) \mathrm{d}x\mathrm{d}y \\
  &= \int (y - f(x))^2 p(y|x)p(x) \mathrm{d}x \mathrm{d}y\\
  &= \mathbb{E}_X \mathbb{E}_{Y|X} [Y - f(X)]^2.
\end{align*}$$

It suffices to minimize EPE *pointwise*

$$ f(x) = \arg\min_c \mathbb{E}_{Y|X} [(Y-c)^2 | X = x]. $$

The solution is given by

$$ f(x) = \mathbb{E}[Y | X = x], $$

which is also known as the **regression function**.

When *"best" is measured by average squared error*, the best prediction of $Y$ at any point $X=x$ is the conditional mean.

#### Interpretation of kNN

For a kNN classifier

$$ \hat{f}(x) = \mathrm{Avg}(y_i | x_i \in N_k(x)) $$

where we make two approximations

1. Expectation is approximated by averaging over a set of sampled data.
2. Conditioning at a point is relaxed to conditioning on some region "close to" the point.

#### Interpretation of Linear Regression

The linear regression is a *model-based* approach: we specify a model for the regression function.

$$ f(x) \approx x^T \beta. $$

Plug this model into the EPE function, we can solve for an analytical solution for $\beta$

$$ \beta = (\mathbb{E}[XX^T])^{-1} \mathbb{E}[XY]. $$

The least squares solution can be acquired by approximating the expectation with averaging over the training data.

#### Remarks on the Criterion

We have been using $L_2$ loss functions. If we use $L_1$ loss instead, the solution will be the conditional median,

$$ \hat{f}(x) = \mathrm{median}(Y | X = x). $$

### Qualitative Output $G$

Assume the prediction rule is $\hat{G}(X) \in \mathcal{G}$, with $|\mathcal{G}| = K$.

- Let $L(k, l)$ be a loss function for penalizing prediction errors. I.e., the "price" paid for misclassifying $\mathcal{G}_k$ as $\mathcal{G}_l$.
  - Most often we use the **zero-one loss function**, where all misclassifications are charged one unit.
- The expected prediction error is
  - $EPE = \mathbb{E}[L(G, \hat{G}(X))]$
- Again we can condition and write $EPE$ as
  - $ EPE = \mathbb{E}_X \sum_{k=1}^K L(\mathcal{G}_k, \hat{G}(X)) p(\mathcal{G}_k | X) $
- It suffices to minimize EPE pointwise. The solution is
  - $\hat{G}(x) = \arg\min_{g\in\mathcal{G}} \sum_{k=1}^K L(\mathcal{G}_k, g) P(\mathcal{G}_k|X = x)$
- With 0-1 loss it simplifies to
  - $ \hat{G}(x) = \arg\min_{g \in \mathcal{G}} (1 - p(g | X = x)) $

#### Case Study: Binary Classification

For simplicity, assume there are only two classes $\mathcal{G}_1$ and $\mathcal{G}_2$.

$$ L(\mathcal{G}_1, \mathcal{G}_2) = 1 \quad L(\mathcal{G}_2, \mathcal{G}_1) = 1 \quad L(\mathcal{G}_1, \mathcal{G}_1) = 0 \quad L(\mathcal{G}_2, \mathcal{G}_2) = 0 $$

The predicted label $g$ can only be either $\mathcal{G}_1$ or $\mathcal{G}_2$

- If $g = \mathcal{G}_1$, $E = P(\mathcal{G}_2|X = x)$
- If $g = \mathcal{G}_2$, $E = P(\mathcal{G}_1|X = x)$

Therefore,

$$ \hat{G}(X) = \arg\max_{g\in\mathcal{G}} P(g|X=x) $$

This is known as the **Bayes Classifier**. Intuitively, it says we should choose the class with the maximum probability at the input $x$.

#### Bayes Classifier

Assume the two classes (RED and GREEN) are drawn from two distributions parametrized by $\bm{\theta}_1$ and $\bm{\theta}_2$. Further assume $P(g=1) = P(g=2)$ (the prior are the same)

$$\begin{align*}
    P(g=1 | X = x) &= \frac{P(X=x|g=1)P(g=1)}{P(X=x)} \\
    &= \frac{P(X=x | g=1) P(g=1)}{P(X=x|g=1)P(g=1) + P(X=x|g=2)P(g=2)} \\
    &= \frac{P(X=x|\bm{\theta}_1)}{P(X=x|\bm{\theta}_1) + P(X=x|\bm{\theta}_2)}
\end{align*}$$

## Curse of Dimensionality

> The drawbacks and problems caused by high dimensionality.

- Data.
  - Requires more storage space and computational complexity.
  - Requires more data samples to fill in the space.
- Model.
  - High model complexity (feature dimension), few training samples $\to$ overfitting
  - Low model complexity, abundant training samples $\to$ underfitting

### Geographic Interpretation

> For a high-dimensional object, most of its volumn is near its surface.

Consider a hypercube with radius $r$. $V(r) = r^p$ where $p$ is the number of dimensions.

Assume we shrink $r$ by a small fraction $\epsilon$,

$$ \lim_{p \to \infty} \frac{V_p(1-\epsilon)}{V_p(1)} = \lim_{p \to \infty} (1-\epsilon)^p = 0, $$

i.e., the shrinked hypercube is much more smaller in volume.

Conversely, given a $p$-dim hypercube, to capture a fraction $s$ of its volume, the edge length $r$ needs to be

$$ s = r^p \Rightarrow r = s^{1/p}. $$

Assume $s=0.1$. For $p = 10$, $r = 0.8$.

### Distance Metrics in High Dimensionality

- Let $x$, $y$ be two independent random variables with uniform distribution on $[0,1]^p$.
- The expected L2 distance $\|x-y\|^2 = p/6$
- As $p$ increases, the mean value becomes larger, but the variance becomes smaller
- Distances between near and far neighbours become more and more similar with increasing dimensionality.
- *Distance metrics start losing their effectiveness to measure dissimilarity in high dimensional space*.
- It is more difficult to learn a good classifier that depends on such metrics.

### Avoiding the Curse of Dimensionality

1. Sample sufficient training data.
2. Reduce the dimension of features.

## Function Approximation

Given $N$ training samples of some unknown function (system) $y = f(x)$

$$ \{ (x_1, f(x_1)), \dots, (x_N, f(x_N)) \} $$

We aim to find an approximation $\hat{f}(x)$ to predict $y' = \hat{f}(x')$ where $x'$ is not in the training set.

### Two Types of Supervised Learning

- **Classification.** Model output is a prediction of some discrete classes.
- **Regression.** Model output has infinitely many values.

### Formulation of Function Approximation

We assume a parametric form $f_{\theta}(x)$ of $f(x)$, and a loss function for measuring the quality of the approximation.

$$ RSS(\theta) \sum_{i=1}^N (y_i - f_{\theta}(x_i))^2, $$

where "RSS" stands for **Residual Sum of Squares**.

- If a function passes through all data points, we will have $RSS = 0$.
- Such an approximation is *cursed* because it tends to be unsmooth.

#### Restricted Estimators

- Roughness Penalty and Bayesian Methods.
  - The functions are explicitly controlled by penalizing the loss with an extra **regularization term** $J(f)$.
  - $MSE(y, f(x)) + \lambda J(f)$
- Kernel Methods and Local Regression.
  - $\sum_{i=1}^N \sum_{k \in N(x_i)} K_\lambda (x_k, x_i) (y_i - f_\theta(x_k))^2$
  - where $K_\lambda()$ is a **kernel function** that usually measures the similarity between $x_k$ and $x_i$
  - intuitively, if $x_k$ and $x_i$ are similar, then we hope $y_i$ to be close to $f_\theta(x_k)$.
- Basis Functions and Dictionary Functions
  - $ \sum_{i=1}^N (y_i - \sum_{m=1}^M \theta_m h_m(x_i))^2 $
  - Use a combination of basis functions $h_m()$, weighted by parameters $\theta_m$, to approximate $f()$.

## Bias and Variance

### Bias, Variance vs. Underfitting, Overfitting

- Underfitting / High Bias
- Overfitting / High Variance
- The turning point depends on the data distribution difference between training set and test set.
