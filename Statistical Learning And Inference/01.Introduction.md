# 01. Introduction

## Function Estimation Model

![function-estimation-model](figs/function-estimation-model.png)

- **Generator ($G$).** Generates observations $x$, which are independently sampled from a certain distribution $F(x)$.
- **Supervisor ($S$).** Labels each input $x$ with an output value $y$ according to some fixed distribution $F(y|x)$.
- **Learning Machine ($f()$).** "Learns" from $(x, y)$ pairs from $G$ and $S$, by choosing a function that *best approximates* $S$ from a parametrized function class $f(x; \alpha) = \hat{y}$.

### Risk Minimization

- **Loss Functional.** The loss functional $(L, Q)$ is the error of a given function on a given example.
  - $L(x, y, f_{\alpha}) \rightarrow L(y, f(x, \alpha))$.
  - $Q(z, \alpha) \rightarrow L(z_y, f(z_x, \alpha))$, where $z = (x,y)$ (if we want to merge $x$ and $y$)
- **Risk Functional.** The risk functional $R$ is the expected loss of a given example when drawn from $F(x, y)$.
  - **Generalization Error.** The generalization error of a given function is usually defined as $R(\alpha) = \int Q(z, \alpha) \mathrm{d}F(z)$. (Integral over all pairs of possible $(x,y)$)

#### Three Main Learning Problems

> All aim to minimize certain kinds of risks

- **Pattern Recognition.**
  - $y \in \{ 0, 1 \}$ and $L(y, f(x, \alpha)) = \mathbb{I}[y \neq f(x,a)]$.
- **Regression Estimation.**
  - $y \in \mathbb{R}$ and $L(y, f(x, \alpha)) = (y - f(x, \alpha))^2$.
- **Density Estimation.**
  - $L(p(x, \alpha)) = -\log p(x, \alpha)$.

#### General Formulation of Learning

- Given an i.i.d. $k$ samples, $z_1, \dots, z_k$ where $z_i = (x_i, y_i)$ drawn from a distribution $F(z)$.
- We wish to find a function $\alpha^*$ such that the risk is minimized
  - $\arg\min_{\alpha} R(\alpha)$

##### Emprical Risk Minimization

Calculating the integral is usually not practical.

$$ R(\alpha) = \int Q(z, \alpha) \mathrm{d}F(z) \Rightarrow R_{emp}(\alpha) = \frac{1}{k} \sum_{i = 1}^k Q(z_i, \alpha) $$

- **Emprical Risk Minimizer.** $\alpha_k = \arg\min_{\alpha} R_{emp}(\alpha)$.
  - ERM approximates $Q(z, \alpha^*)$ with $Q(z, \alpha_k)$

#### Error Bounds

The **error bound** is the maximum possible error of a model on an unseen test set.

$$ R_T(h) \le R_S(h) + \frac{1}{2} d_{H\Delta{}H}(S,T) + \lambda \le R_S(h) + \frac{1}{2} d_H(S, T) + \lambda $$

- Basically "test error = training error + distance between training and testing set"
