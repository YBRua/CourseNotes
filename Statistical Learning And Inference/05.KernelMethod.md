# Kernel Method

Consider a fitting problem on samples $\{(x_i, y_i)\}$. The naive KNN regression

$$ \hat{f}(x_0) = \mathrm{Avg}(y_i | x_i \in \mathcal{N}_k(x) ) $$

produces a bumpy curve, since $\hat{f}(x_0)$ is not continuous on $x$.

Kernel smoothing methods aim to solve this problem.

- Fit a different but simple model separately at each point $x_0$.
- Using only the observations close to the target point $x_0$ for fitting.
- And in such a way that result in a smooth function in $\mathbb{R}^p$.
- Localization is done via a weighting function, or **kernel** $K_{\lambda}(x_0, x_i)$ that assigns a weight to $x_i$ based on its distance to $x_0$.

$$ \hat{f}(x_0) = \frac{\sum_{i=1}^N K_{\lambda}(x_0, x_i)y_i}{\sum_{i=1}^N K_{\lambda}(x_0, x_i)} $$

## One-Dimensional Kernel Smoothers

The general notation of a kernel function is

$$ K_{\lambda}(x_0, x) = D\left( \frac{|x - x_0|}{h_{\lambda}(x_0)} \right)$$

where

- $h_{\lambda}(x_0)$ is a **width function** that determines the width of the neighborhood at $x_0$.
  - If we set $h_{\lambda}(x_0) = \lambda$, then the width function is a constant.
  - Larger $\lambda$ averages over more observations and implies lower variance, but higher bias.

### Popular Kernels for Local Smoothing

#### Gaussian Kernel

$$ D(t) = \phi(t) = \exp\left( -\frac{1}{2}t^2 \right) $$

- Noncompact kernel.
- The "standard deviation" plays the role of the window size.
- Has infinite support.

#### Epanechchnikov Kernel

$$ D(t) = \begin{cases}
    \frac{3}{4}(1-t^2) &\quad |t| < 1\\
    0 &\quad o.w.
\end{cases} $$

- Compact kernel (needed when used with nearest neighbour window size).

#### Tri-Cube Kernel

$$ D(t) = \begin{cases}
    (1-|t|^3)^3 &\quad |t| < 1\\
    0 &\quad o.w.
\end{cases} $$

- Compact kernel.
- Flatter on the top.
- Differentiable at the boundary of its support.

## Local Regression

- Locally weighted averages can be *badly biased on the boundaries* of the domain, due to the asymmetry of the kernel in that region.
  - Such bias can be present in the interior of the domain as well, if the values are not equally spaced (though less severe).
- Fitting a straight line locally instead of using constants can remove the first-order bias.

### Locally Weighted Regression

Locally weighted regression solves a separate weighted least squares problem at each target point $x_0$

$$ \min_{\alpha(x_0), \beta(x_0)} \sum_{i=1}^N K_{\lambda}(x_0, x_i)[y_i - \alpha(x_0) - \beta(x_0)x_i]^2 $$

The estimate is then

$$ \hat{f}(x_0) = \hat{\alpha}(x_0) + \hat{\beta}(x_0)x_0 $$

Denote $b(x)^T = (1, x)$, $B \in \mathbb{R}^{N \times 2}$ with the $i$-th row being $b(x)^T$; let $W(x_0) = \mathrm{diag}(K_{\lambda}(x_0, x_i))$,

$$ \hat{f}(x_0) = b(x_0)^T (B^TW(x_0)B)^{-1}B^TW(x_0)y = \sum_{i=1}^Nl_i(x_0)y_i $$

which shows that the estimate $\hat{f}$ is linear in $y$.

#### Analysis

Consider the expansion for $\hat{f}$ around $x_0$,

$$ \begin{align*}
  \hat{f}(x_0) = \sum_{i=1}^Nl_i(x_0)y_i &= f(x_0)\sum_{i=1}^N l_i(x_0) + f'(x_0)\sum_{i=1}^N(x_i - x_0)l_i(x_0) \\&+ \frac{1}{2}f''(x_0)\sum_{i=1}^N(x_i-x_0)^2l_i(x_0) + R
\end{align*} $$

It can be proved that

$$ \sum_{i=1}^N l_i(x_0) = 1 \quad \text{and} \quad \sum_{i=1}^N(x_i - x_0)l_i(x_0)= 0 $$

Therefore

$$ \hat{f}(x_0) = f(x_0) + + \frac{1}{2}f''(x_0)\sum_{i=1}^N(x_i-x_0)^2l_i(x_0) + R $$

That is, the bias $\hat{f}(x_0) - f(x_0)$ depends only on the quadratic and higher-order terms in the expansion of $f$.

### Local Polynomial Regression

We can also fit local polynomials of any degree $d$

$$ \min_{\alpha(x_0), \beta_j(x_0)} \sum_{i=1}^N K_{\lambda}(x_0, x_i)\left[ y_i - \alpha(x_0) - \sum_{j=1}^d \beta_j(x_0)x_i^j \right]^2 $$

with solution

$$ \hat{f}(x_0) = \hat{\alpha}(x_0) + \sum_{j=1}^d \hat{\beta}_j (x_0)x_0^j $$

- Reduced bias, but increased variance.

### Selection of Kernel Width

- The parameter $\lambda$ in $K_{\lambda}$ controls the width of the kernel.
  - $\lambda$ takes the *radius* of supporting region for compact supporting kernel.
  - $\lambda$ takes the *variance* for Gaussian kernel.
- Kernel width is related to model selection.
  - Wide width leads to large bias and small variance.
  - Narrow width leads to small bias and large variance.

### Structured Local Regression

When $x$ is multi-dimensional, we can enforce some structured assumptions on the kernel.

In general, we use a positive semidefinite matrix $A$ to weigh different coordinates of $x$

$$ K_{\lambda, A}(x_0, x) = D\left( \frac{(x - x_0)^TA(x-x_0)}{\lambda} \right) $$

By imposing appropriate restrictions on $A$, some coordinates can be downgraded or omitted.

## Local Likelihood

The concept of local regression can be easily extended: any parametric model can be made local if the fitting method accomodates observation weights. For example,

Associate each observation $y_i$ with a parameter $\theta_i = \theta(x_i) = x_i^T\beta$. Assume inference for $\beta$ is based on log-likelihood,

$$ l(\beta) = \sum_{i=1}^N l(y_i, x_i^T\beta) $$

We can model $\theta(X)$ more flexibly by using the likelihood local to $x_0$,

$$ l(\beta(x_0)) = \sum_{i=1}^N \sum_{i=1}^N K_{\lambda}(x_0, x_i)l(y_i,x_i^T\beta(x_0)) $$

## Kernel Density Estimation

Kernel density esitmation is an unsupervised learning procedure.

Assume we have a set of random samples $x_1,\dots,x_N$ drawn from a probability density $f_X(x)$, and we wish to estimate $f_X$ at $x_0$.

A natural local estiamte has the form

$$ \hat{f}_X(x_0) = \frac{\#x_i \in N(x_0)}{N\lambda} $$

This estimate is bumpy, a smooth Parzen estimate is preferred

$$ \hat{f}_X(x_0) = \frac{1}{N\lambda}\sum_{i=1}^N K_{\lambda}(x_0, x_i) $$

A popular choice for $K_\lambda$ is the Gaussian kernel $K_\lambda(x_0, x) = \phi(|x-x_0|/\lambda)$. Let $\phi_\lambda$ denote the Gaussian density with zero mean and standard deviation $\lambda$,

$$ \hat{f}_X(x_0) = \frac{1}{N}\sum_{i=1}^N \phi_\lambda(x-x_i) = \frac{1}{N(2\lambda^2\pi)^{p/2}}\sum_{i=1}^N\exp\left( -\frac{1}{2}\left( \left\| x_i - x_0 \right\| / \lambda \right)^2 \right) $$

### Kernel Density Classification

The kernel density estiamtes can be used for non-parametric classification using the Bayes theorem. Assume for a problem with $J$ classes, we fit density estimates $\hat{f}_j(X)$ for each class, and assume we also have the estimates for class priors $\hat{\pi}_j$,

$$ \mathrm{Pr}[G=j|X=x_0]=\frac{\hat{\pi_j}\hat{f}_j(x_0)}{\sum_{k=1}^J \hat{\pi}_k \hat{f}_k(x_0)} $$

## Naive Bayes

> Refer to AI2611 Notes (Machine Learning) for extra information on Naive Bayes.

The Naive Bayes model assumes that, given a class $G=j$, the features $X_k$ are independent,

$$ f_j(X) = \prod_{k=1}^p f_{jk}(X_k) $$

where $p$ is the feature dimension. While this assumption is generally not true, it does dramatically simplify the estimation.

- The densities $f_{jk}$ can be separately estiamted using one-dimensional kernel density estimates.
- If a component $X_j$ of $X$ is discrete, a histogram estimate could be used, allowing mixed variable types in a feature vector.

We can derive the logit-transform, using class $J$ as the base,

$$ \begin{align*}
  \log\frac{\mathrm{Pr}[G=l|X]}{\mathrm{Pr}[G=J|X]} &= \log\frac{\pi_l f_l(X)}{\pi_J f_J(X)}\\
  &= \frac{\pi_l \prod_{k=1}^p f_{lk}(X_k)}{\pi_J \prod_{k=1}^p f_{Jk}(X_k)} \\
  &= \log \frac{\pi_l}{\pi_J} + \sum_{k=1}^p \log\frac{f_{lk}(X_k)}{f_{Jk}(X_k)}\\
  &= \alpha_l + \sum_{k=1}^p g_{lk}(X_k)
\end{align*} $$

which is of the form of a *generalized additive model* (not covered in this course).

## Radial Basis Functions

- Kernel methods achieve flexibility by fitting simple models in a region local to the target point $x_0$.
- Localization is achieved via a weighting kernel $K_{\lambda}$.
- Radial basis functions combine the ideas, by treating the kernel functions $K_\lambda(\xi, x)$ as basis functions.

$$f(x) = \sum_{j=1}^M K_{\lambda_j}(\xi_j, x)\beta_j = \sum_{j=1}^M D\left( \frac{\| x - \xi_j \|}{\lambda_j} \right)\beta_j $$

where each basis element is indexed by a location or prototype parameter $\xi_i$ and a scale parameter $\lambda_j$.

### RBF Network Model

For simplicity, we focus on least squares methods for regression and use the Gaussian kernel for $D()$.

$$ \min_{\lambda,\xi,\beta} \sum_{i=1}^N \left( y_i - \beta_0 - \sum_{j=1}^M \beta_j \exp\left( -\frac{(x_i - \xi_j)^T(x_i - \xi_j)}{\lambda_j^2} \right) \right)^2 $$

The model is commonly referred to as an RBF network.

- $\{ \xi, \lambda \}$ are estimated by using GMM to fit the training data, which are respectively the mean and standard deviation of each Gaussian model in GMM.
- Given $\xi$ and $\lambda$, $\beta_j$ is solved by least-squares regression.

### Gaussian Mixture Model

> https://stephens999.github.io/fiveMinuteStats/intro_to_em.html

We assume $x_1,\dots,x_N$ follows a Gaussian Mixture Model, i.e., a weighted sum of Guassian distributions,

$$ p(X_i = x) = \sum_{k=1}^K P(Z_i = k)P(X_i=x | Z_i = k) = \sum_{i=1}^K \pi_k P(X_i = x | \mu_k, \sigma_k) $$

where $Z_i$ is a *latent variable* indicating "$x_i$ belongs to the $k$-th Gaussian", $K$ is the number of Guassians, $\pi_k$ are the priors and $\mu_k, \sigma_k$ are the parameters.

We aim to obtain the maximum likelihood estimation for $\pi, \mu, \sigma$.

The log-likelihood is

$$ l(\theta) = \sum_{i=1}^N \log\left( \sum_{k=1}^K \pi_kN\left( x_i;\mu_k,\sigma_k^2 \right) \right) $$

which is too complicated to solve for analytical solutions. For example, if we try taking the derivative w.r.t. $\mu_k$,

$$\tag{1} \frac{\partial l}{\partial \mu_k} = \sum_{n=1}^N\frac{\pi_k\mathcal{N}(x_i;\mu_k,\sigma_k^2)}{\sum_{j=1}^K\pi_j\mathcal{N}(x_i|\mu_j,\sigma_j^2)}\cdot\frac{(x_i - \mu_k)}{\sigma_k^2} = 0 $$

and å¯„.

#### Expectation Maximization for GMM

However, if we knew the latent variables $Z_i$, we can simply gather all samples $X_i$ such that $Z_i = i$ and estimate the parameters for $k$-th Gaussian.

Therefore we first attempt to compute the probability that the $i$-th sample belongs to the $k$-th Gaussian, denoted by $\gamma_i(k)$.

$$ \gamma_i(k) = P(Z_i = k|X=x_i) = \frac{P(X=x_i|Z_i = k)P(Z_i=k)}{\sum_{j=1}^{K}P(X=x_i|Z_i=j)P(Z_i = j)} = \frac{\pi_k p(x_i|\theta_k)}{\sum_{j=1}^K \pi_j p(x_i|\theta_j)}$$

where $\theta_j = (\mu_j, \sigma_j^2)$. With $\gamma_j(k)$ we can easily estimate $\mu$ using (1), if we pretend the $\mu_k$ terms in $\gamma_i(k)$ does not exist for the moment.

$$ \frac{\partial l}{\partial \mu_k} = \sum_{i=1}^{N} \gamma_i(k) \frac{(x_i - \mu_k)}{\sigma_k^2} = 0  $$

which yields

$$ \mu_k = \frac{\sum_{i=1}^N \gamma_i(k)x_i}{\sum_{i=1}^N \gamma_i(k)} $$

A similar estimation can be made on $\pi_k$ and $\sigma_k$,

$$ \pi_k = \frac{1}{N}\sum_{i=1}^N \gamma_i(k), \quad \sigma_k^2 = \frac{\sum_{i=1}^N \gamma_i(k)(x_i - \mu_k)^2}{\sum_{i=1}^N \gamma_i(k)} $$

The EM algorithm for GMM estimates the parameters iteratively

- **E-Step.** Estimate the hidden variable $\gamma_i(k) = P(Z | X, \theta)$
- **M-Step.** Use maximum likelihood estimation to estimate $\pi, \mu, \sigma^2$

#### Expectation Maximization for K-Means

GMM can be used as a probablistic clustering or soft clustering. A similar hard clustering algorithm is the K-Means algorithm. In each iteration of K-Means,

- **E-Step.** Estimate the hidden variable

$$ \gamma_i(k) = \begin{cases}
  1 &\quad k=\arg\min_k(x_i-\mu_k)^2\\
  0 &\quad o.w.
\end{cases} $$

- **M-Step.** Maximum likelihood estimation

$$ \mu_k = \frac{\sum_{i=1}^N \gamma_i(k) x_i}{\sum_{i=1}^N \gamma_i(k)} $$

#### EM Algorithm in General

The Expectation-Maximization algorithm can generally be applied to situations where we have *an observation and a hidden variable*.
