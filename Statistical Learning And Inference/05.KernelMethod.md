# Kernel Method

Consider a fitting problem on samples $\{(x_i, y_i)\}$. The naive KNN regression

$$ \hat{f}(x_0) = \mathrm{Avg}(y_i | x_i \in \mathcal{N}_k(x) ) $$

produces a bumpy curve, since $\hat{f}(x_0)$ is not continuous on $x$.

Kernel smoothing methods aim to solve this problem.

- Fit a different but simple model separately at each point $x_0$.
- Using only the observations close to the target point $x_0$ for fitting.
- And in such a way that result in a smooth function in $\mathbb{R}^p$.
- Localization is done via a weighting function, or **kernel** $K_{\lambda}(x_0, x_i)$ that assigns a weight to $x_i$ based on its distance to $x_0$.

$$ \hat{f}(x_0) = \frac{\sum_{i=1}^N K_{\lambda}(x_0, x_i)y_i}{\sum_{i=1}^N K_{\lambda}(x_0, x_i)} $$

## One-Dimensional Kernel Smoothers

The general notation of a kernel function is

$$ K_{\lambda}(x_0, x) = D\left( \frac{|x - x_0|}{h_{\lambda}(x_0)} \right)$$

where

- $h_{\lambda}(x_0)$ is a **width function** that determines the width of the neighborhood at $x_0$.
  - If we set $h_{\lambda}(x_0) = \lambda$, then the width function is a constant.
  - Larger $\lambda$ averages over more observations and implies lower variance, but higher bias.

### Popular Kernels for Local Smoothing

#### Gaussian Kernel

$$ D(t) = \phi(t) = \exp\left( -\frac{1}{2}t^2 \right) $$

- Noncompact kernel.
- The "standard deviation" plays the role of the window size.
- Has infinite support.

#### Epanechchnikov Kernel

$$ D(t) = \begin{cases}
    \frac{3}{4}(1-t^2) &\quad |t| < 1\\
    0 &\quad o.w.
\end{cases} $$

- Compact kernel (needed when used with nearest neighbour window size).

#### Tri-Cube Kernel

$$ D(t) = \begin{cases}
    (1-|t|^3)^3 &\quad |t| < 1\\
    0 &\quad o.w.
\end{cases} $$

- Compact kernel.
- Flatter on the top.
- Differentiable at the boundary of its support.

## Local Regression

- Locally weighted averages can be *badly biased on the boundaries* of the domain, due to the asymmetry of the kernel in that region.
- Fitting a straight line locally instead of using constants can remove the first-order bias.

## Local Likelihood

## Kernel Density Estimation

## Naive Bayes

## Radial Basis Functions
