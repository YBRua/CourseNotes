# Few-Shot Learning

## Problem Setting

- We are given a base category set $\mathcal{C}^b$ (each category has sufficient labeled samples)
- and a novel category set $\mathcal{C}^n$ (each category has only a few samples)
- Want to transfer knowledge to the novel set.

## Few-Shot Learning Methods

- **Meta-learning.** Learn a learning strategy to adjust well to a new few-shot learning task.
- **Metric learning.** Learn a semantic embedding space using a distance loss function.
- **Synthetic method.** Synthesize more data from the novel classes to facilitate regular learning.

### Meta-Learning

> *"Learn to learn."*

In meta-learning, we construct many tasks to train the model and hope it would generalize better when encountering new few-shot tasks.

I.e., we want to train and transfer *a good initial model*.

#### N-way K-Shot Task

Construct multiple tasks to simulate the few-shot training process. Each task contains

- A training (support) set
  - $N$ categories
  - $K$ samples per category
- A test (query) set
  - Query samples in $N$ categories

#### MAML

> The high-level idea is to learn a good initial model that can quickly adapt to future tasks.

**Goal.** Optimize the model parameters such that *a small number of gradient steps on a new task will produce maximally effective behavior on that task.*

1. Randomly initialize $\theta$.
2. While not converged
   1. Sample batch of tasks $T_i \sim p(\mathcal{T})$
   2. For all $T_i$ do
      1. Evaluate $\nabla_{\theta} \mathcal{L}_{\mathcal{T_i}}(f_{\theta})$ w.r.t. $K$ samples
      2. Compute adapted parameters with gradient descent $\theta_i' = \theta - \alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T_i}}$
   3. Update $\theta = \theta - \beta \nabla_{\theta} \sum_{T_i \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T_i}}(f_{\theta_i'})$.

- **Explanation.**
  - When adapting to a new task $T_i$, the model parameter $\theta$ is updated via gradient descent. $\theta_i' = \theta - \alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T_i}}$.
  - The meta-objective loss is $\min_{\theta} \sum_{T_i \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T_i}}(f_{\theta_i'})$

### Metric Learning

Compare each testing data with training sample, and check whether the two samples belong to the same category.

I.e., we have *a good metric* as prior and we want to transfer this metric.

#### Relation Network

- Concatenate embeddings of query and support samples.
- A relation module is trained to produce score 1 for correct class and 0 for others.

#### Matching Network

$$ P(\hat{y} | \hat{x}, S) $$

#### Prototypical Netowrk

- Learns a mapping function $f_\phi$ for extracting features.
- Construct class-level prototypes by using mapped representations, which is the centroid of each category.
  - $ c_k = \frac{1}{|S_k|} \sum_{(x_i, y_i) \in S} f_{\phi}(x_i) $
- At inference time, it performs non-parametric matching via distance $d$.
  - $p( y=k |x ) = \frac{\exp(-d(f_\phi(x), c_k))}{\sum_j \exp(-d(f_\phi(x), c_j))}$

### Synthetic Method

- Train a synthesizer sampling from class distribution.
- Use the trained synthesizer to create more data instances for training.
- Generally we are trying to transfer some sort of *delta* between samples.

#### [CVPR 18] Low-shot learning from imaginary data

- Use one sample plus random noise to generate new samples in the same category.
- The synthesizer is jointly trained with the classifier.

#### [NeurIPS 18] Delta-encoder: an effective sample synthesis method for few-shot object recognition

- Use a variant of autoencoder to capture the intra-class differences.
- Transfer intra-class differences to novel classes.

## Few-Shot Image Generation

- Base categories to novel categories (fine-grained, instant adaptation)
- From large dataset to small dataset (finetune on small dataset)
- Only small dataset (directly train on small dataset)

### Base Categories to Novel Categories

> We assume the categories are fine-grained (e.g., chihuahua, labrador retriever instead of simply "dog")
> *Instant adaptation* means we can directly apply the model to novel categories.

- [MM 20] F2GAN: Fusing-and-Filling GAN for Few-shot Image Generation
- [ECCV 22] DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta
- [MM 22] Few-shot Image Generation Using Discrete Content Representation

### From Large Datasets to Small Datasets

- Train a model on a large dataset.
- Finetune the model on a small dataset.

### Only a Small Dataset

- By "small" we mean a few tens or hundreds of data.
- Want to avoid overfitting.
  - Reduce model parameters
  - Data augmentation.
