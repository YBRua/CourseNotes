# Weakly Supervised Learning

**Weakly supervised learning** refers to cases where the supervision signal is weak. The exact definition of *weak* might vary from different tasks.

- E.g., for image classification, *weakly supervised* typically means *noisy labels*; for segmentation, it usually means *coarse-grained annotations*.

It is usually much more easier and cheap to get weakly supervised training data. Weak supervision provides lower degree of (and therefore cheaper) annotation at train time than the required output at test time.

- E.g., for object detection, the model needs to output a bounding box at test time, but the training set does not provide bounding boxes.

## Methods

### Label Flipping Method

#### [ICLR 15] Training Convolutional Networks with Noisy Labels

##### Label Transition Matrix

Consider image classification tasks. Images and labels are usually crawled from the Internet, which could be noisy. Assume $\tilde{y}$ is the noisy label, and $y$ is the predicted label.

$$ p(\tilde{y} = j | x) = \sum_i p(\tilde{y} = j | y=i) p(y=i | x) = \sum_i q_{ij}p(y=i | x) $$

where $q_{ij} = p(\tilde{y} = j | y=i)$ denotes the **transition probability** from class $i$ to class $j$

- We can combine the model output $p(y|x)$ and the transition matrix $Q$ to get the final noisy labels $\tilde{y}$.
- We can then use the noisy labels $\tilde{y}$ to train the classification model and learn $Q$.

##### Training and Inference

For a standard classification task with clean labels,

$$ x \xrightarrow{f(\cdot)} y $$

However, assume we now only have noisy labels $\tilde{y}$ as supervision.

$$ x \xrightarrow{f(\cdot)} y \xrightarrow{Q} \tilde{y} $$

At test time we only use $f(\cdot)$ to output the clean label.

#### [ICCV 15] Webly Supervised Learning of Convolutional Networks

Derives a similar "transition matrix", but in a different way.

- First train a model using easy images.
- Then calculate the confusion matrix of this model to get the transition matrix.

$$ q_{ij} = \frac{\sum_{k \in C_i} p(y=j | x)}{|C_i|} $$

- Transfer the matrix $Q$ to another model trained on noisy labels.

#### [CVPR 15] Learning from massive noisy labeled data for image classification

Further unifies the source of noise into another random variable $z$.

Assumes there are three sources of noises.

### Curriculum Learning

Start from easy tasks and move on to hard tasks.

#### [ECCV 18] Curriculumnet

- Divide training samples into clusters based on local density.
- The clusters with high local density are easy clusters.
- First train the model with easy clusters, then move on to hard clusters
