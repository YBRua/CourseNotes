# Weakly Supervised Learning

**Weakly supervised learning** refers to cases where the supervision signal is weak. The exact definition of *weak* might vary from different tasks.

- E.g., for image classification, *weakly supervised* typically means *noisy labels*; for segmentation, it usually means *coarse-grained annotations*.

It is usually much more easier and cheap to get weakly supervised training data. Weak supervision provides lower degree of (and therefore cheaper) annotation at train time than the required output at test time.

- E.g., for object detection, the model needs to output a bounding box at test time, but the training set does not provide bounding boxes.

## Label Flipping Method

### [ICLR 15] Training Convolutional Networks with Noisy Labels

#### Label Transition Matrix

Consider image classification tasks. Images and labels are usually crawled from the Internet, which could be noisy. Assume $\tilde{y}$ is the noisy label, and $y$ is the predicted label.

$$ p(\tilde{y} = j | x) = \sum_i p(\tilde{y} = j | y=i) p(y=i | x) = \sum_i q_{ij}p(y=i | x) $$

where $q_{ij} = p(\tilde{y} = j | y=i)$ denotes the **transition probability** from class $i$ to class $j$

- We can combine the model output $p(y|x)$ and the transition matrix $Q$ to get the final noisy labels $\tilde{y}$.
- We can then use the noisy labels $\tilde{y}$ to train the classification model and learn $Q$.

#### Training and Inference

For a standard classification task with clean labels,

$$ x \xrightarrow{f(\cdot)} y $$

However, assume we now only have noisy labels $\tilde{y}$ as supervision.

$$ x \xrightarrow{f(\cdot)} y \xrightarrow{Q} \tilde{y} $$

At test time we only use $f(\cdot)$ to output the clean label.

### [ICCV 15] Webly Supervised Learning of Convolutional Networks

Derives a similar "transition matrix", but in a different way.

- First train a model using easy images.
- Then calculate the confusion matrix of this model to get the transition matrix.

$$ q_{ij} = \frac{\sum_{k \in C_i} p(y=j | x)}{|C_i|} $$

- Transfer the matrix $Q$ to another model trained on noisy labels.

### [CVPR 15] Learning from massive noisy labeled data for image classification

Further unifies the source of noise into another random variable $z$.

Assumes there are three sources of noises.

## Curriculum Learning

Start from easy tasks and move on to hard tasks.

### [ECCV 18] Curriculumnet

- Divide training samples into clusters based on local density.
- The clusters with high local density are easy clusters.
- First train the model with easy clusters, then move on to hard clusters

## Multi-Instance Learning

- A special form of weakly-supervised learning.
- Training instances are arranged in sets, called **bags**.
- A label is provided for bags, but not for instances.
  - **Negative bags** do not contain positive instances.
    - $y_{l,i} = -1$ for $Y_l = -1$.
  - **Positive bags** contain at least one positive instance (might contain both positive and negative instances).
    - $\sum_{i \in 1,\dots,|\mathcal{B}_l|} \frac{1}{2}(y_{l,i}+ 1) \ge 1$ for $Y_l = 1$.
- Relaxed assumptions of multi-instance learning
  - A bag is positive when it contains a sufficient number of positive instances.
    - $\sum_{i \in 1,\dots,|\mathcal{B}_l|} \frac{1}{2}(y_{l,i} + 1) \ge \sigma_p |\mathcal{B}_l|$ for $Y_l = 1$.
  - A bag is negative when it contains a certain number of negative instances.
    - $\sum_{i \in 1,\dots,|\mathcal{B}_l|} \frac{1}{2}(y_{l,i} + 1) \le \sigma_n |\mathcal{B}_l|$ for $Y_l = -1$.
  - Positive and negative bags differ by their distributions.

### Applications

#### Image Search

- Consider searching images using *boat* as the query
  - **Positive bags.** Relevant images (might contain irrelevant instances)
  - **Negative bags.** Irrelevant images.

#### Object Localization

- **Objective.** Find and locate objects in an image.
- **Bags.** Collections of candidate annotation boxes.
- **Instance.** Sub-image corresponding to candidate bounding boxes.

#### Medical Diagnosis

- **Objective.** Determine if a subject is diseased or healthy.
- **Bags.** Collections of image patches extracted from a medical image.
- **Instances.** Image segments or patches.

#### Paragraph-level Sentiment Analysis

- **Objective.** Determine the sentiment of a paragraph of text.
- **Bags.** Paragraphs.
- **Instances.** Sentences.

### Methods

#### Bag-level Methods

- Group instances into bags.
- Treat one bag as a sample.
- Bags can be constructed by clustering or uniform partitioning.
- Represents a bag with a single feature vector, thus transforming the task to a standard supervised learning task.
- **Pros.**
  - Can model distributions and relations between instances.
  - Can deal with unclassifiable instances.
  - Can be faster than instance-level methods.
  - More accurate for bag classification tasks.
- **Cons.**
  - Cannot be used for instance-level classification.

##### sMIL

Based on SVM

$$ \begin{align*}
    \min_{w,b,\xi_i} &\quad \frac{1}{2}\|w\|^2 + C\sum_{l=1}^L \xi_l\\
    \mathrm{s.t.} &\quad Y_l(w^T\phi(z_i)+b) \ge \rho - \xi_l\\
    &\quad \xi_l \ge 0
\end{align*} $$

where $\rho = \sigma - (1-\sigma) = 2\sigma - 1$, $\sigma$ denotes the positive ratio, $z_i = \frac{1}{|\mathcal{B}_l|} \sum x_{l,i}$ is the averaged bag feature.

#### Instance-level Methods

- Try to identify the labels for each instance.
- **Pros.**
  - Can be directly used for instance classification tasks.
- **Cons.**
  - Do not work if the instance does not have a precise class.
  - Usually less accurate than bag-level methods.

##### Instance-level SVM

$$ \begin{align*}
    \min_{w,b,\xi_i,y_{l,i}} &\quad \frac{1}{2}\|w\|^2 + C\sum_{l=1}^L\sum_{i=1}^{|\mathcal{B}_l|} \xi_{l,i}\\
    \mathrm{s.t.} &\quad y_{l,i}(w^T\phi(x_{l,i})+b) \ge 1 - \xi_l\\
    &\quad \xi_{l,i} \ge 0
\end{align*} $$

Note that $y_{l,i}$ is also included as optimization variables, i.e., we infer the labels of each instance.

The optimization problem is solved by alternatingly optimizing $w,b,\xi$ and $y$.

1. Fix $w, b$, update $y_{l,i}$ by $\tilde{y}_{l,i} = w^T\phi(x_{l,i}) + b$. Determine $y_{l,i}$ by the constraints of MIL.
2. Fix $y_{l,i}$, update $w, b$ by solving a standard SVM.

##### Deep Multi-Instance Learning Method

1. (bag-level) Average the instance scores.
2. (bag-level) Average the instance features.
3. (somewhere between bag and instance level) Learn instance weights using an attention model.
   - Tend to assign higher weights to positive instances, and lower weights to negative instances.
   - The weights could be used for instance-level classification.
   - Can be extended to image pixels and learn spatial attention.

## Outlier Detection

Detect and remove outliers (noisy samples) and retain clean samples. Performance depends on the accuracy of the outlier detection algorithm.

- **Typical methods.**
  - Use a model (e.g., Gaussian) to fit the distribution of all training samples.
    - Remove samples with low probability.
  - Use the density within a neighborhood or the distance from a nearest neighbor.
    - A point is likely to be an outlier if its local density is low, or if it is too far from a neighbor.
  - Use clustering.
    - The smallest cluster is likely to contain outliers.
  - Use a one-class classifier.
    - Use predictio scores to identify outliers.
  - Use the reconstruction error of an autoencoder.
    - If the large reconstruction error is large, the point is likely to be an outlier.

## Applications

### Weakly Supervised Object Detection

- Only know whether an image contains an object, without knowledge of the exact bounding boxes.

#### Formulation

- Assume a binary task (we want to detect only one type of object).
- Each image is a bag.
  - A positive bag is one that contains the target object.
  - A negative bag is one that does not contain the object.
- The bounding boxes are instances.
  - Want to find the true positive instances (bounding boxes, a.k.a. windows)
- Train a window classifier that identifies positive windows.

#### Bag Generation

> How do we generate the bounding boxes?

- **Sliding windows.**
  - Shift a window over an image.
  - Dense windows (e.g., 100k per image)
  - Can cover different translation, scale and aspect ratio.
- **Object proposals.**
  - Selective search bounding boxes.
  - Sparse (~2k per image).
  - Commonly used.

#### Method Overview

- **Retrain & Relocalize**
  - Initialize a set of bounding boxes.
  - Train a model.
  - Use the model to update the boxes.
  - Retrain the model.
- **Curriculum Learning.**
  - Start with easy samples and move on to difficult ones.
  - Difficulty is measured by the confidence of max-scoring windows or window sizes.

### Weakly Supervised Semantic Segmentation

- Only have image-level labels (e.g., image contains person and bicycle).

#### Class Activation Map (CAM)

Assign a weight vector for each class. Use the weighted sum of feature maps to identify objects.

- Coarse-grained, but can be a good starting point.

#### Learning from Web Data

- Web images often have dominant objects and help learn boundaries.
- Target domain images have accurate class labels and help learn classifiers.
