# Model Selection

## Bias, Variance and Model Complexity

### Error Measurement

Assume the loss function for measuring errors between $Y$ and $\hat{f}(X)$ is denoted by $L(Y, \hat{f}(X))$. Typical choices are

- **Squared Loss.** $(Y - \hat{f}(X))^2$.
- **Absolute Loss.** $|Y-\hat{f}(X)|$.
- **0-1 Loss.** $\mathbb{I}[\hat{f}(X) \neq Y]$.

**Training error.** The average loss over the training samples.

$$ \overline{err} = \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i)) $$

**Test error (Generalization error).** The prediction error over an independent test sample.

$$ err_{\tau} = \mathbb{E}[L(Y, \hat{f}(X))|\tau] $$

where $\tau$ denotes the training set.

$$\tag{1} err = \mathbb{E}[L(Y, \hat{f}(X))] = \mathbb{E}[err_{\tau}] $$

Note that (1) takes expectation over anything that is random, including the randomness of the training set that produces $\hat{f}$.

- We would like to know the *expected test error* of the model.
- As the model becomes more complex, its bias decreases while variance increases.
- The training error is NOT a good estimate for test error, as training error usually consistently decreases with model complexity.
  - However, a model with (extremely) low training error is usually an overfit and tends to generalize poorly.

### Model Selection vs. Model Assessment

- **Model selection** estimates the performance of different models in order to choose the best one (performend on the validation set).
- **Model assessment** estimates the prediction error of the final model on new data (performed on the test set).

## Bias-Variance Decomposition

We assme

$$ Y = f(X) + \epsilon \quad \epsilon \sim \mathcal{N}(0,\sigma_{\epsilon}^2) $$

The expected prediction error of a regression fit $\hat{f}(X)$ on a sample $x_0$ is given by

$$ \begin{align*}
    err(x_0) &= \mathbb{E}\left[ \left( Y - \hat{f}(x_0) \right)^2 | X=x_0 \right] \\
    &= \sigma_{\epsilon}^2 + \left[ \mathbb{E}\hat{f}(x_0) - f(x_0) \right]^2 + \mathbb{E}\left[ \mathbb{E}\hat{f}(x_0) - \hat{f}(x_0) \right]^2\\
    &= \sigma_{\epsilon}^2 + \mathrm{Bias}\left( \hat{f}(x_0) \right)^2 + \mathrm{Var}\left( \hat{f}(x_0) \right)
\end{align*} $$

For a fixed $err(x_0)$, as the model becomes more complex, the squared bias becomes lower but the variance becomes higher.

### kNN Estimator

$$ \hat{f}_k(x_0) = \frac{1}{k}\sum_{j=1}^k f(x_j) $$

For the kNN regression fit,

$$ err(x_0) = \mathbb{E}\left[ \left( Y - \hat{f}_k(x_0) \right)^2 | X=x_0 \right] = \sigma_{\epsilon}^2 + \left[ f(x_0) - \frac{1}{k}\sum_{j=1}^k f(x_j) \right]^2 + \frac{\sigma_{\epsilon}^2}{k} $$

- $k$ is inversely related to the model complexity.
  - As $k$ increases, the bias will typically increase, while the variance decreases.

### Linear Regression

$$ \hat{f}_d(x) = \beta^Tx, \quad \beta = (X^TX)^{-1}X^Ty $$

$$ \hat{f}_d(x_0) = x_0^T\beta = x_0^T(X^TX)^{-1}X^Ty \coloneqq h(x_0)^Ty $$

where $d$ is the number of dimensions.

Therefore,

$$ err(x_0) = \sigma_{\epsilon}^2 +  \left[ f(x_0) - \mathbb{E}\hat{f}_d(x_0) \right]^2 + \|h(x_0)\|^2 \sigma_{\epsilon}^2 $$

The variance $\|h(x_0)\|^2 \sigma_\epsilon$ changes with $x_0$, its average over all training samples $x_i$ is $(d/N)\sigma_\epsilon^2$ and the **in-sample** error, is

$$ \frac{1}{N}\sum_{i=1}^N err(x_i) = \sigma_\epsilon^2 + \frac{1}{N}\sum_{i=1}^N [f(x_i) - \mathbb{E}\hat{f}(x_i)]^2 + \frac{d}{N}\sigma_\epsilon^2 $$

The model complexity is directly related to the number of parameters $d$.

## Optimism of the Training Error

Given a training set $\tau = \{ (x_i, y_i) \}_{i=1}^N$,

the generalization error of a model $\hat{f}$ is,

$$ err_{\tau} = \mathbb{E}_{X^0, Y^0} [L(Y^0, \hat{f}(X^0)) | \tau] $$

Averaging over the training set $\tau$ yields the expected error,

$$ err = \mathbb{E}_\tau\mathbb{E}_{X^0, Y^0} [L(Y^0, \hat{f}(X^0)) | \tau] $$

Typically, the training error,

$$ \overline{err} = \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i)) $$

The training error $\overline{err}$ is typically smaller than the generalization error $err_\tau$. Therefore $\overline{err}$ is an overly optimistic estimator for $err_\tau$.

### Extra-Sample and In-Sample Error

The error $err_\tau$ can be thought of as **extra-sample error**, as the input do not coincide with the training sample.

Since we assume $Y = f(X) + \epsilon$, even for the same $x$, we might get a different response $Y$ due to the random error term.

The **in-sample error** is defined as

$$ err_{in} = \frac{1}{N}\sum_{i=1}^N \mathbb{E}_{Y^0}\left[ L\left( Y^0_i, \hat{f}(x_i) \right) \right] $$

where $Y^0_i$ indicates that we observe new response values at training points $\{ x_0,\dots,x_N \}$.

#### Optimism

We define the **optimism** as the difference between the in-sample error and the training error.

$$ op \coloneqq err_{in} - \overline{err} $$

$op$ is usually nonnegative because $err_{in}$ is typically larger than or equal to $\overline{err}$.

For square loss, 0-1 loss or other loss functions, it can be shown that generally

$$ op = \frac{2}{N}\sum_{i=1}^N \mathrm{cov}(\hat{y}_i, y_i) $$

- The harder we fit the training data, the greater $\mathrm{cov}(\hat{y}_i, y_i)$ would be, and therefore the larger the optimism.

If $\hat{y}_i$ is obtained by a linear fit with $d$ basis functions, a simplification is

$$ \sum_{i=1}^N \mathrm{cov}(\hat{y}_i, y_i) = d\sigma_{\epsilon}^2 $$

- If the dimension / number of basis functions increases, optimism will increase too.
- If the number of training samples increases, optimism will decrease.

### Estimates of In-Sample Prediction Error

The general form of the in-sample estimate is

$$ \widehat{err}_{in} = \overline{err} + \widehat{op} $$

When $d$ parameters are fitted with squared error loss, a version of the so-called $C_p$ statistic is

$$ C_p = \overline{err} + 2\cdot\frac{d}{N}\hat{\sigma}_\epsilon^2 $$

where $\hat{\sigma}_\epsilon^2$ is an estimation of the noise variance, obtained from the mean-squared error of a low-bias model.

### Akaike Information Criterion (AIC)

The **Akaike Information Criterion** is a similar but more generally applicable estimate of $err_{in}$.

Given a set of models indexed by a tuning parameter $\alpha$, denote by $\overline{err}(\alpha)$ and $d(\alpha)$ the training error and number of parameters for each model, then

$$ \mathrm{AIC}(\alpha) = \overline{err} + 2\cdot\frac{d(\alpha)}{N}\hat{\sigma}_\epsilon^2 $$

$\mathrm{AIC}(\alpha)$ provdies an estimate of the test error curve, and we find the tuning parameter $\alpha$ that minimizes it.

$$ \{ \hat{f}(\alpha) | \alpha: \min\mathrm{AIC}(\alpha) \} $$

## The Bayesian Approach and BIC

### Bayesian Infromation Criterion (BIC)

The Bayesian Information Criterion (BIC) is defined as

$$ \mathrm{BIC} = -2\mathrm{loglik} + (\log N)d $$

It is applicable to settings where the fitting is carried out by maximizing the log likelihood.

Under the Gaussian model, assuming $\sigma_{\epsilon}^2$ is known, $-2\mathrm{loglik}$ equals to (up to a constant) $\sum_i(y_i - \hat{f}(x_i))^2/\sigma_{\epsilon}^2$, which is $N\overline{err}/\sigma_{\epsilon}$ for a squared error loss. Therefore

$$ \mathrm{BIC} = \frac{N}{\sigma_{\epsilon}^2}\left[ \overline{err} + (\log N)\frac{d}{N}\sigma_\epsilon^2 \right] $$

#### AIC vs. BIC

$$ AIC = \overline{err} + 2\cdot\frac{d(\alpha)}{N}\hat{\sigma}_\epsilon^2 $$

$$ BIC \propto \overline{err} + (\log N)\frac{d}{N}\sigma_\epsilon^2 $$

- BIC is proportional to AIC, with $2$ replaced by $\log N$.
- When $N \to \infty$, BIC tends to select simple models due to its heavy penalty on complexity.

#### Bayesian Model Selection

Despite the similarities of AIC and BIC, BIC is motivated by the Bayesian approach to model selection.

Assume we have a set of $m$ candidate models $M_m$, and corresponding parameters $\theta_m$, and we want to choose the best model among them.

The posterior of a given model is

$$\begin{align*}
  \mathrm{Pr}(M_m |Z) &\propto \mathrm{Pr}(M_m)\mathrm{Z|M_m}\\
  &\propto \mathrm{Pr}(M_m)\int \mathrm{Pr}(Z|\theta_m, M_m)\mathrm{Pr}(\theta_m|M_m)\mathrm{d}\theta_m
\end{align*}$$

where $Z$ represents the training data.

To compare two models $M_m$ and $M_l$,

$$ \frac{\mathrm{Pr}(M_m|Z)}{\mathrm{Pr}(M_l|Z)} = \frac{\mathrm{Pr}(M_m)}{\mathrm{Pr}(M_l)} \frac{\mathrm{Pr}(Z|M_m)}{\mathrm{Pr}(Z|M_l)} $$

If the oddes are greater than 1, model $m$ will be chosen, or otherwise we choose model $l$. The rightmost quantity

$$ \frac{\mathrm{Pr}(Z|M_m)}{\mathrm{Pr}(Z|M_l)} $$

is called the **Bayes factor**, the contribution of the data to the posterior odds.

If the model prior $\mathrm{Pr}(M)$ is uniformly distributed, so that $\mathrm{Pr}(M_m)$ is a constant,

$$ \log\mathrm{Pr}(Z|M_m) = \log\mathrm{Pr}\left( Z|\hat{\theta}_m, M_m \right) - \frac{d_m}{2}\log N + O(1) $$

where $\hat{\theta}_m$ is the maximum likelihood estimator, $d_m$ is the model dimension.

If we define the loss function to be

$$ -2\log\mathrm{Pr}\left( Z |\hat{\theta}_m, M_m \right) $$

It is equivalent to the BIC criterion. Minimizing BIC is equivalent to maximizing posterior

$$ BIC = -2\log lik + (\log N)d $$

## Minimum Description Length (MDL)

The Minimum Description Length (MDL) approach gives a selection criterion similar to BIL, but is derived from the view of optimal coding. If we think of our data as a message to be encoded and the model as an encoder, we want to choose the shortest code for transmission.

### Optimal Coding

The strategy is to use the shortest coding length for the most frequent message. In general, if messages $z_i$ are sent with probabilities $\mathrm{Pr}(z_i)$, then we should use length $-\log\mathrm{Pr}(z_i)$. The average message length satisfies

$$ \mathbb{E}[length] \ge -sum \mathrm{Pr}(z_i)\log\mathrm{Pr}(z_i)$$

The equality holds iff $p_i = A^{-l_i}$, where $A$ is the length of the alphabet ($A=2$ if we are using binary encoding).

### MDL

Back to model selection. The message length required to transmit the output is

$$ length = - \log\mathrm{Pr}(y|\theta, M, X) - \log\mathrm{Pr}(\theta |M) $$

where the first term is the average code length for transmitting the discrepancy between the model and actual target values, and the second term is the average code length for transmitting the model parameters.

Assume $y \sim N(\theta,\sigma^2)$ with $\theta\sim N(0,1)$ and no input,

$$ length = Const + \log\sigma + \frac{(y-\theta)^2}{\sigma^2} + \frac{\theta^2}{2} $$

The smaller $\sigma$ is, the shorter on average is the message length.

The MDL principle states that the model should minimize

$$ length = - \log\mathrm{Pr}(y|\theta, M, X) - \log\mathrm{Pr}(\theta |M) $$

## Vapnik-Chrnovenkis Dimension (VC Dimension)

A difficulty in using estimates of in-sample error is the need to specify the number of parameters (i.e., model complexity) $d$ used in the fit.

The Vapnik-Chernovenkis theory provides a general measure for model complexity.

The VC dimension of a function family $\{f(x, \alpha)\}$ is defined to be the largest number of points that can be shattered by members of $\{f(x, \alpha)\}$.

- **Examples.**
  - The VC dimension of the family of linear functions $\mathbb{I}(\alpha_0 + \alpha_1^Tx > 0)$ is 3.
    - A linear function can separate at most 3 points (however the points are distributed and whatever their labels are).
    - A linear function cannot separate 4 points in some cases.
  - The VC dimension of $\mathbb{I}(\sin\alpha x)$ is infinite.

The VC dimension of a class of real-valued functions $\{ g(x, \alpha) \}$ is defined to be the VC dimension of the indicator class

$$ \{ \mathbb{I}(g(x, \alpha) - \beta > 0) \} $$

where $\beta$ takes values over the range of $g$.

The VC Dimension can be used for constructing an estimate of (extra-sample) prediction error. For example, if we fit $N$ points using a class of functions $\{ f(x;\alpha) \}$ having VC dimension $h$, then with probability of at least $1-\eta$

$$ err_\tau \le \overline{err} + \frac{\epsilon}{2}\left( 1 + \sqrt{1 + \frac{4\overline{err}}{\epsilon}} \right) $$

for binary classifcation and

$$ err \le \frac{\overline{err}}{(1-c\sqrt{\epsilon})_+} $$

where $\epsilon = a_1 \frac{h[\log(a_2)N/h + 1] - \log (\eta /4)}{N}$

where $a_1 \in (0,4]$ and $a_2 \in (0, 2]$.
