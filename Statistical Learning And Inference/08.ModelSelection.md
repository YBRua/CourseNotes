# Model Selection

## Bias, Variance and Model Complexity

### Error Measurement

**Training error.** The average loss over the training samples.

$$ \bar{err} = \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i)) $$

**Test error (Generalization error).** The prediction error over an independent test sample.

$$ err_{\tau} = \mathbb{E}[L(Y, \hat{f}(X))|\tau] $$

where $\tau$ denotes the training set.

$$ err = \mathbb{E}[L(Y, \hat{f}(X))] = \mathbb{E}[err_{\tau}] $$

Note that the second formula does not condition on the training set $\tau$, so it is the expected test error over all possible training sets.

### Model Selection vs. Model Assessment

- **Model selection** estimates the performance of different models in order to choose the best one (performend on the validation set).
- **Model assessment** estimates the prediction error of the final model on new data (performed on the test set).

## Bias-Variance Decomposition

We assme

$$ Y = f(X) + \epsilon \quad \epsilon \sim \mathcal{N}(0,\sigma_{\epsilon}^2) $$

The expected prediction error of a regression fit $\hat{f}(X)$ on a sample $x_0$ is given by

$$ \begin{align*}
    err(x_0) &= \mathbb{E}\left[ \left( Y - \hat{f}(x_0) \right)^2 | X=x_0 \right] \\
    &= \sigma_{\epsilon}^2 + \left[ \mathbb{E}\hat{f}(x_0) - f(x_0) \right]^2 + \mathbb{E}\left[ \mathbb{E}\hat{f}(x_0) - \hat{f}(x_0) \right]^2\\
    &= \sigma_{\epsilon}^2 + \mathrm{Bias}\left( \hat{f}(x_0) \right)^2 + \mathrm{Var}\left( \hat{f}(x_0) \right)
\end{align*} $$

For a fixed $err(x_0)$, as the model becomes more complex, the squared bias becomes lower but the variance becomes higher.

### kNN Estimator

$$ \hat{f}_k(x_0) = \frac{1}{k}\sum_{j=1}^k f(x_j) $$

For the kNN regression fit,

$$ err(x_0) = \mathbb{E}\left[ \left( Y - \hat{f}_k(x_0) \right)^2 | X=x_0 \right] = \sigma_{\epsilon}^2 + \left[ f(x_0) - \frac{1}{k}\sum_{j=1}^k f(x_j) \right]^2 + \frac{\sigma_{\epsilon}^2}{k} $$

### Linear Regression

$$ \hat{f}_d(x) = \beta^Tx, \quad \beta = (X^TX)^{-1}X^Ty $$

$$ \hat{f}_d(x_0) = x_0^T\beta = x_0^T(X^TX)^{-1}X^Ty \coloneqq h(x_0)^Ty $$

Therefore,

$$ err(x_0) = \sigma_{\epsilon}^2 +  \left[ f(x_0) - \mathbb{E}\hat{f}_d(x_0) \right]^2 + \|h(x_0)\|^2 \sigma_{\epsilon}^2 $$

## Optimism of the Training Error

The training error $\bar{err}$ is typically smaller than the generalization error $err_\tau$. Therefore $\bar{err}$ is an overly optimistic estimator for $err_\tau$.

### Optimism

#### In-Sample Error

Since we assume $Y = f(X) + \epsilon$, even for the same $x$, we might get a different response $Y$ due to the random error term.

The **in-sample error** is defined as

$$ err_{in} = \frac{1}{N}\sum_{i=1}^N \mathbb{E}_{Y^0}\left[ L\left( Y^0_i, \hat{f}(x_i) \right) \right] $$

where $Y^0_i$ indicates that we observe new response values at training points $\{ x_0,\dots,x_N \}$.

#### Optimism

We define the optimism as the difference between the in-sample error and the training error.

$$ op \coloneqq err_{in} - \overline{err} $$

$op$ is usually nonnegative because $err_{in}$ is typically larger than or equal to $\overline{err}$.

For square loss, 0-1 loss or other loss functions, it can be shown that generally

$$ op = \frac{2}{N}\sum_{i=1}^N \mathrm{cov}(\hat{y}_i, y_i) $$

If $\hat{y}_i$ is obtained by a linear fit with $d$ basis functions, a simplification is

$$ \sum_{i=1}^N \mathrm{cov}(\hat{y}_i, y_i) = d\sigma_{\epsilon}^2 $$

### Estimates of In-Sample Prediction Error

The general form of the in-sample estimate is

$$ \widehat{err}_{in} = \overline{err} + \widehat{op} $$

### Akaike Information Criterion (AIC)

## Effective Number of Parameters

## The Bayesian Approach and BIC

### Bayesian Infromation Criterion (BIC)

The Bayesian Information Criterion (BIC) is defined as

$$ \mathrm{BIC} = -2\mathrm{loglik} + (\log N)d $$

It is applicable to settings where the fitting is carried out by maximizing the log likelihood.

Under the Gaussian model, assuming $\sigma_{\epsilon}^2$ is known, $-2\mathrm{loglik}$ equals to (up to a constant) $\sum_i(y_i - \hat{f}(x_i))^2/\sigma_{\epsilon}^2$, which is $N\overline{err}/\sigma_{\epsilon}$ for a squared error loss. Therefore

$$ \mathrm{BIC} = \frac{N}{\sigma_{\epsilon}^2}\left[  \right] $$

## Minimum Description Length (MDL)

## Vapnik-Chrnovenkis Dimension (VC Dimension)

A difficulty in using estimates of in-sample error is the need to specify the number of parameters (i.e., model complexity) $d$ used in the fit.

The Vapnik-Chernovenkis theory provides a general measure for model complexity.

The VC dimension of a function family $\{f(x, \alpha)\}$ is defined to be the largest number of points that can be shattered by members of $\{f(x, \alpha)\}$.

- **Examples.**
  - The VC dimension of the family of linear functions $\mathbb{I}(\alpha_0 + \alpha_1^Tx > 0)$ is 3.
    - A linear function can separate at most 3 points (however the points are distributed and whatever their labels are).
    - A linear function cannot separate 4 points in some cases.
  - The VC dimension of $\mathbb{I}(\sin\alpha x)$ is infinite.

The VC dimension of a class of real-valued functions $\{ g(x, \alpha) \}$ is defined to be the VC dimension of the indicator class

$$ \{ \mathbb{I}(g(x, \alpha) - \beta > 0) \} $$

where $\beta$ takes values over the range of $g$.
