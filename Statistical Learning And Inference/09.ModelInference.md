# Model Inference

## Bootstrap and Maximum Likelihood Methods

### Bootstrap

## Bayes Methods

## The EM Algorithm

The expectation-maximization (EM) algorithm is a popular tool for simplifying difficult maximum likelihood problems.

### Two-Component Gaussian Mixture Model

$$ f(x) = \sum_{m=1}^M \alpha_m \phi(x; \mu_m, \Sigma_m) $$

Given $n$ samples, the log-likelihood is

$$ l(y, \theta) = \sum_{i=1}^N \log[\alpha \phi_{\theta_1}(x_i) + (1-\alpha)\phi_{\theta_2}(x_i)] $$

Suppose we observe the latent binary $z$

$$ L(x,z,\theta) = \sum_{z_i=1}\log[\phi_{\theta_1}(x_i)] + \sum_{z_i=0}\log[\phi_{\theta_2}(x_i)] $$

where

$$ z_i = 1 \Rightarrow x_i \sim \phi_{\theta_1} \quad z_i = 0 \Rightarrow \sim \phi_{\theta_2} $$

### The EM Algorithm in General

Consider the log-likelihood of a discriminative model

$$ \begin{align*}
    L(\theta) &= \log p(y | x; \theta) \\
    &= \sum_z q(z|x,y) \log p(y|x;\theta) \\
    &= \sum_z q(z|x,y) \log \frac{p(y,z|x;\theta)}{p(z|x,y;\theta)} \\
    &= \sum_z q(z|x,y) \log \left[ \frac{p(y,z|x;\theta)}{p(z|x,y;\theta)} \frac{q(z|x,y)}{q(z|x,y)} \right] \\
    &= \sum_z q(z|x,y) \log \frac{p(y,z|x;\theta)}{q(z|x,y)} + \sum_z q(z|x,y) \log \frac{q(z|x,y)}{p(z|x,y;\theta)} \\
    &= l(\theta, q) + \mathrm{KL}[q(z|x,y) \| p(z|x,y;\theta)]
\end{align*} $$

Note that the KL-Divergence is nonnegative, and $l$ is a lower bound of $L$.

$$ l(\theta,q) \le L(\theta), \quad l(\theta, q) = L(\theta) \Leftrightarrow q(z|x,y) = p(z|x,y;\theta) $$

#### The E-Step

In the E-Step, we find $q(z|x,y) = p(z|x,y;\theta^{(t)})$ so that $L(\theta^{(t)}) = l(\theta^{(t)}, q)$.

Given model parameters $\theta^{(t)}$ from the $t$-th iteration.

$$ q(z|x,y) = p(z|x,y;\theta^{(t)}) $$

#### The M-Step

In the M-Step, we optimize $l(\theta, q)$ over $\theta$, so that $L(\theta^{(t+1)}) \ge l(\theta^{(t+1)}) \ge L(\theta^{(t)})$.

Since we only care about $\theta$ in the M-Step, we can omit $p(z|x,y;\theta)$ on the demonimator, and the lower bound $l(\theta, q)$ can be written as

$$ Q(\theta; \theta^{(t)}) = \sum_z q(z|x,y) \log p(y, z|x;\theta) $$

We can update $\theta$ by taking its derivative

$$ \nabla_{\theta} Q = \sum_z (z|x,y) \nabla_{\theta}\log p(y,z|x;\theta) $$

#### EM Algorithm for Generative Model

Cannot deriveå•Š

## Markov Chain Monte Carlo

- Given a set of samples, we can fit a distribution on the samples. (E.g., maximum likelihood estimation).
- Having fit a distribution, we can draw samples from it.

The Markov chain Monte Carlo algorithm draws samples from a target distribution $p(x)$ by performing a biased random walk to explore the distribution.

### Metropolis-Hastings Algorithm (Univariate)

1. Initialize
   1. Pick an initial state $x_0$
   2. Set $t=0$
2. Iterate
   1. Randomly generate a candidate state $x'$ according to $g(x'|x_t)$, where $g$ is a pre-defined hyperfunction.
   2. Calculate the acceptance probability $A(x', x_t) = \min\left( 1, \frac{p(x')}{p(x_t)}\frac{g(x_t|x')}{g(x'|x_t)} \right)$
   3. Generate $u \in [0,1]$ uniformly at random.
   4. If $u \le A(x', x_t)$, then we accept $x_{t+1} = x'$. Otherwise we reject the candidate $x_{t+1} = x_t$.
   5. $t=t+1$

### Gibbs Sampling (Multivariate)

Consider a multivariate sampling problem where we want to sample

$$ x = (x_1,\dots,x_d) $$

The idea of the Gibbs sampler is to divide elements in $x$ into two groups and update each group iteratively

1. Initialize
   1. Pick an initial state $x^{(0)}$.
   2. Set $t= 0$.
2. Iterate
   1. $j = \mathrm{mod}(t, d)$.
   2. Generate $x_{j}^{(t+1)}$ according to $p(x_{j}|x_1,\dots,x_{j-1},x_{j+1},\dots,x_d)$.
   3. $x^{(t+1)} = (x^{(t)}_1,\dots,x^{(t+1)}_j,\dots,x^{(t)}_d)$. Note that only one element $x_j$ in $x^{(t+1)}$ is changed.

## Model Averaging and Stacking

In the above algorithm, we choose one element as one group, and all other elements as the other group.
