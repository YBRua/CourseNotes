# Linear Classifiers

## Linear Regression as Classification

- Model the problem $X \mapsto Y$ as a linear regression, where $Y$ are one-hot vectors corresponding to label classes.
- **Drawback.** Linear models are simple and only utilizes the label information.
  - Cannot handle complicated cases.
- **Solutions.**
  - Use higher order features (e.g., quadratic)
  - Leverage more information (e.g., spatial distribution)

## Linear and Quadratic Discriminant Functions

### Linear Discriminant Analysis

According to the Bayes optimal classification, we want to model the posterior distribution $\mathrm{Pr}(G|X)$.

- Assumptions
  - $f_k(x) = \mathrm{Pr}[X=x|G=k]$ denotes the class-conditional density of $X$ in class $k$.
  - $\pi_k = \mathrm{Pr}[G=k]$ denotes the prior probability of class $k$.
- Assume $f_k(X)$ follows a multivariate Gaussian distribution.
 
$$ f_k(X) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)} $$

#### Prior and Posterior

By the Bayes Theorem, the posterior of given $X=x$, the probability of it belonging to some class $G=k$ is given by

$$ \mathrm{Pr}(G=k|X) = \frac{\mathrm{Pr}(X|G=k)\mathrm{P}(G=k)}{\mathrm{Pr}(X)} \sim \mathrm{Pr}(X|G=k)\mathrm{P}(G=k) $$

Note that we are trying to find the most likely $G=k$, so we can omit the demoninator (as it does not contain $G$).

When comparing two classes $k$ and $l$, assume the covariance matrices are the same for $k$ and $l$ $\Sigma_k = \Sigma_l = \Sigma$,

$$ \log\frac{\mathrm{Pr}(G=k|X=x)}{\mathrm{Pr}(G=l|X=x)} = \log\frac{\pi_k}{\pi_l} - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \frac{1}{2}\mu_l^T\Sigma^{-1}\mu_l + x^T\Sigma^{-1}(\mu_k-\mu_l) $$

The above equation indicates that the decision boundary of class $k$ and $l$ is linear in $x$.

#### Linear Discriminant Function

The **linear discriminant functions** are an equivalent description for the decision rule.

$$ \delta_k(x) = \log\mathrm{Pr}(G=k|X=x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k $$

$$ G(k) = \argmax_k \delta_k(x) $$

i.e., we assign $x$ to the class with the highest posterior.

In practice, we do not know the parameters of the Gaussian distributions, so we need to estimate the parameters using the training data.

- $\hat{\pi}_k = N_k / N$
- $\hat{\mu}_k = \sum_{x\in G(k)} x_i/N_k$
- $\hat{\Sigma}_k = \sum_{x\in G(k)} (x_i - \mu_k)(x_i -\mu_k)^T / (N - 1)$

#### LDA

- Rule: $G(x) = \argmax_k \delta_k(x)$
- Boundary: $\{ x | \delta_k(x) = \delta_l(x) \}$
  - The decision boundary is linear

#### LDA Classification vs. Dimensionality Reduction

- **Dimensionality Reduction.** $\hat{x} = v^Tx, v \propto S_W^{-1}(\mu_1 - \mu_2)$
- **Classification.** $w^Tx = c, w = S_W^{-1}(\mu_1-\mu_2)$

### Quadratic Discriminant Analysis

- The assumption in LDA that two classes have the same covariance matrix is too strong.
- When the covariances of class $k$ and $l$ are different, the problem becomes a **quadratic discriminant function**.

$$ \delta_k(x) = -\frac{1}{2}\log|\Sigma_k| - \frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log\pi_k $$


- The decision boundary is described by a quadratic function.

### Regularized Discriminant Analysis

- The difference between LDA and QDA is whether different classes share the same covariance matrix.
- We can add regularizations on the covariance matrix to design models "between" LDA and QDA.

1. Shrink the separate covariances of QDA towards a common covariance as in LDA
   - $\Sigma_k(\alpha) = \alpha\Sigma_k + (1-\alpha)\Sigma$
2. Or shrink the covariances toward a scalar covariance
   - $\Sigma_k(\gamma) = \gamma\Sigma_k + (1-\gamma)\sigma^2I$

### Computation for LDA and QDA

$$ \delta_k(x) = -\frac{1}{2}\log|\Sigma_k| - \frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log\pi_k $$

The computations for LDA and QDA can be simplified by diagonalizing $\Sigma$. Consider the EVD of $\Sigma$ (or $\Sigma_k$).

$$ \hat{\Sigma}_k = U_kD_k{U_k}^{T} $$

where $U_k$ is a $p \times p$ orthonormal matrix, and $D_k$ is a diagonal matrix of positive eigenvalues $d_{kl}$.

The ingredients for $\delta_k(x)$ are

$$(x - \hat{\mu}_k)^T \hat{\Sigma}^{-1}_k(x - \hat{\mu}_k) = \left[ \left( x - \hat{\mu}_k^T \right)U_k \right] {D_k}^{-1}\left[ {U_k}^{T} \left( x - \hat{\mu}_k \right) \right]$$

$$ \log\left| \hat{\Sigma}_k \right| = \sum_l \log d_{kl} $$

For LDA, $\Sigma = UD{U}^{T}$,

$$ {(x - \hat{\mu}_k)}^{T} \hat{\Sigma}^{-1} (x - \hat{\mu}_k) = [U^T (x - \hat{\mu}_k)]^T D^{-1} [U^{T}(x - \hat{\mu}_k)] = \left\| D^{-1/2}U^Tx - D^{-1/2}U^T\hat{\mu}_k \right\|^2 $$

Denote

$$ x^* = D^{-1/2}U^Tx, \quad \hat{\mu}_k^* = D^{-1/2}U^T\hat{\mu}_k $$

Then

$$ \delta_k(x) = -\frac{1}{2}(x - \mu_k)^T \Sigma^{-1}(x - \mu_k) = -\frac{1}{2}\| x^* - \hat{\mu}_k^* \|^2 $$

### Reduced Rank LDA

$$ \delta_k(x) = -\frac{1}{2}\| x^* - \hat{\mu}_k^* \|^2 $$

- The matrix of samples $X^* \in \mathbb{R}^{p \times N}$.
- The matrix of centroids $U^* \in \mathbb{R}^{p \times K}$.

Perform PCA on $U^*$ and obtain a projection matrix $V^*$. We can also project the inputs to this centroid-spanning subspace.

$$ \tilde{X} = {V^*}^{T} X^*, \quad \tilde{U} = {V^*}^T U^* $$

## Logistic Regression

The Logistic Regression model assumes that the posterior probability $\mathrm{Pr}[G = k | X = x]$ could be modeled by a linear model.

$$ \mathrm{Pr}[G=k | X = x] = \frac{\exp(\beta_k^Tx)}{1 + \sum_{l=1}^{X-1} \exp(\beta_l^Tx)} \quad k=1,\dots,K-1 $$

$$ \mathrm{Pr}[G=K | X = x] = \frac{1}{1 + \sum_{l=1}^{X-1} \exp(\beta_l^Tx)}$$

### Fitting Logistic Regression

Logistic Regression models are usually fitted by maximum likelihood,

$$ \hat{\beta} = \arg\max \sum_{i=1}^N \log \mathrm{Pr}_{\beta}[y_i | x_i] $$

It can be solved by the Newton's method. For simplicity we only consider the binary classification case.

$$p(x, \beta) = \mathrm{Pr}_{\beta} [y=1|x] = \frac{\exp(\beta^T x)}{1 + \exp(\beta^T x)} $$

$$ 1 - p(x, \beta) = \mathrm{Pr}_{\beta}[y=0|x] = \frac{1}{1 + \exp(\beta^Tx)} $$

$$ l(\beta) = \sum_{i=1}^N y_i \log p(x_i, \beta) + (1-y_i)\log(1-p(x_i, \beta)) = \sum_{i=1}^N \left( y_i\beta^Tx_I - \log\left( 1 + e^{\beta^Tx_i} \right) \right) $$

Set the gradient of $l(\beta)$ w.r.t. $\beta$ to zero,

$$ \nabla l = \sum_{i=1}^N x_i (y_i - p(x_i, \beta)) = 0 $$

Use Newton's method to find the zero point of $\nabla l$ by iteratively updating

$$ x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)} $$

## Separating Hyperplane

### Resenblatt's Perceptron Learning Algorithm

The **perceptron learning algorithm** tries to find a separating hyperplane by inimizing the distance of misclassified samples to the decision boundary.

If $y_i = 1$ is misclassified, then $x_i^T \beta + \beta_0 < 0$, and vice versa for a misclassified $y_i = -1$.

The goal is to minimize

$$ D(\beta, \beta_0) = - \sum_{i \in \mathcal{M}} y_i(x_i^T\beta + \beta_0) $$

where $\mathcal{M}$ is the set of misclassified samples.

Using stochastic gradient descent, the gradients are given by

$$ \nabla D_{\beta} = -\sum_{i \in \mathcal{M}} y_ix_i \quad \nabla D_{\beta_0} = -\sum_{i\in\mathcal{M}} y_i $$

### Optimal Separating Hyperplane

The **optimal separating hyperplane** separates the two classes and maximizes the distance to the closest point from either class.

Consider

$$ \begin{align*}
  \max &\quad M \\
  \mathrm{s.t.} &\quad y_i(x_iT\beta + \beta_0) \ge M\\
  &\quad \|\beta\| = 1
\end{align*} $$

The constraints ensure that all points are at least a signed distance $M$ from the decision boundary defined by $\beta$ and $\beta_0$, we seek the largest $M$.

We can eliminate the $\|\beta\|= 1$ constraint by (redefining $\beta_0$),

$$ \frac{1}{\|\beta\|}  y_i(x_i^T\beta + \beta_0) \ge M \Longrightarrow y_i(x_i^T\beta + \beta_0) \ge \|\beta\|M$$

Since for any $\beta$ and $\beta_0$ satisfying the inequalities, any positively scaled multiple satisfies too, we can arbitrarily set $\left\| \beta \right\| = 1/M$,

$$ \begin{align*}
  \max &\quad \frac{1}{2}\|\beta\|^2 \\
  \mathrm{s.t.} &\quad y_i(x_iT\beta + \beta_0) \ge 1
\end{align*} $$

We can solve this by Lagrangian multipliers and the solution for $\beta$ is

$$ \beta^* = \sum_{i=1}^N \alpha_i^* y_i x_i $$

where $\alpha_i^* \ge 0$ is the Lagrangian multiplier. $\alpha_i > 0$ if $x_i$ is on the boundary, else $0$.
