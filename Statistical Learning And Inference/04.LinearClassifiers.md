# Linear Classifiers

## Linear Regression as Classification

- Model the problem $X \mapsto Y$ as a linear regression, where $Y$ are one-hot vectors corresponding to label classes.
- **Drawback.** Linear models are simple and only utilizes the label information.
  - Cannot handle complicated cases.
- **Solutions.**
  - Use higher order features (e.g., quadratic)
  - Leverage more information (e.g., spatial distribution)

## Linear and Quadratic Discriminant Functions

### Linear Discriminant Analysis

According to the Bayes optimal classification, we want to model the posterior distribution $\mathrm{Pr}(G|X)$.

- Assumptions
  - $f_k(X)$ denotes the conditional density of $X$ in class $k$.
  - $\pi_k$ denotes the prior probability of class $k$.
- Assume $f_k(X)$ follows a multivariate Gaussian distribution.
 
$$ f_k(X) = \frac{1}{(2^\pi)^{p/2}|\Sigma_k|^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_l^{-1}(x-\mu_k)} $$

#### Prior and Posterior

The posterior of given $X=x$, the probability of it belonging to some class $G=k$ is given by

$$ \mathrm{Pr}(G=k|X) = \frac{\mathrm{Pr}(X|G=k)\mathrm{P}(G=k)}{\mathrm{Pr(X)}} \sim \mathrm{Pr}(X|G=k)\mathrm{P}(G=k) $$

Note that we are trying to find the most likely $G=k$, so we can omit the demoninator (as it does not contain $G$).

When comparing two classes $k$ and $l$, assume the covariance matrix $\Sigma_k = \Sigma$,

$$ \log\frac{\mathrm{Pr}(G=k|X=x)}{\mathrm{Pr}(G=l|X=x)} = \log\frac{\pi_k}{\pi_l} - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \frac{1}{2}\mu_l^T\Sigma^{-1}\mu_l + x^T\Sigma^{-1}(\mu_k-\mu_l) $$

The above equation indicates that the boundary of class $k$ and $l$ is linear in $x$.

#### Linear Discriminant Function

Let

$$ \delta_k(x) = \log\mathrm{Pr}(G=k|X=x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k $$

$$ G(k) = \argmax_k \delta_k(x) $$

i.e., we assign $x$ to the class with the highest posterior.

In practice, we do not know the parameters of the Gaussian distributions, so we need to estimate the parameters using the training data.

- $\hat{\pi}_k = N_k / N$
- $\hat{\mu}_k = \sum_{x\in G(k)} x_i/N_k$
- $\hat{\Sigma}_k = \sum_{x\in G(k)} (x_i - \mu_k)(x_i -\mu_k)^T / (N - 1)$

#### LDA

- Rule: $G(x) = \argmax_k \delta_k(x)$
- Boundary: $\{ x | \delta_k(x) = \delta_l(x) \}$
  - The decision boundary is linear

### Quadratic Discriminant Analysis

- The assumption in LDA that two classes have the same covariance matrix is too strong.
- When the covariances of class $k$ and $l$ are different, the problem becomes a **quadratic discriminant function**.

$$ \delta_k(x) = -\frac{1}{2}\log|\Sigma_k| - \frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log\pi_k $$

### Regularized Discriminant Analysis

- The difference between LDA and QDA is whether different classes share the same covariance matrix.
- We can add regularizations on the covariance matrix to design models "between" LDA and QDA.

1. Shrink the separate covariances of QDA towards a common covariance as in LDA
   - $\Sigma_k(\alpha) = \alpha\Sigma_k + (1-\alpha)\Sigma$
2. Or shrink the covariances toward a scalar covariance
   - $\Sigma_k(\gamma) = \gamma\Sigma_k + (1-\gamma)\sigma^2I$

## Logistic Regression

The Logistic Regression model assumes that the decision boundary is linear.
