# Zero-Shot Learning

## Training/Testing Categories

Consider a classification problem,

- Training categories $\mathcal{C}^s$
- Testing categories $\mathcal{C}^t$.

In the traditional setting, we assume $\mathcal{C}^s = \mathcal{C}^t$.

In a more generalized setting, $\mathcal{C}^s \neq \mathcal{C}^t$. The testing set might contain labels not seen in the training set (and vice versa).

1. Testing category is a subset of the training category. $\mathcal{C}^t \subseteq \mathcal{C}^s$. This case is trivial.
2. Testing category contains labels not seen in the training category.
   - In the extremee case, $\mathcal{C}^s$ and $\mathcal{C}^t$ have no overlap. We have no training samples for the testing set.

## Zero-Shot Learning

- It is impossible to collect ample annotated examples for all categories.
- **Long-tail categories.** Categories with few examples in the training set.

## Category-level Semantic Information

- Use **category-level semantic information** to bridge the gap between seen and unseen categories.
- Semantic information can be in various forms, such as attributes or category descriptions.

### Attribute Vectors

Each category $c$ is associated with an attribute vector $a_c$, which is manually annotated by human experts.

- **Pros.** Accurate and informative.
- **Cons.** Manual annotation efforts.

Summarize attribute vectors from the training set, and given the attributes of an unseen category, we can predict its label by its attributes.

### Free Category-level Information

Crawled from webpages (e.g., wikipedia) or word vectors, etc.

- **Pros.** Automatic and almost free.
- **Cons.** Less informative than attributes.

## Methods for Zero-Shot Learning

In zero-shot learning, we have 3 spaces: the input space, the category space and the semantic space.

Zero-shot learning methods essentially describes the relationships between the 3 spaces, and can be roughly divided into

1. Semantic relatedness methods.
2. Semantic embedding methods.
3. Synthetic methods.

### Semantic Relatedness Methods

We measure the semantic relatedness between different categories in the semantic space by computing a similarity matrix $\mathbf{S}$ where $\mathbf{S}_{ij} = s_{i,j}$ is the similarity between category $i$ and $j$.

Denote the classifier by $f_c(\cdot)$.

- Classifiers for seen categories: $\{ f_1(\cdot),\dots,f_{c_s}(\cdot) \}$
- Classifiers for unseen categories: $f_{j}(\cdot) = \sum_{i=1}^{c_s} s_{i,j}f_i(\cdot)$

Intuitively, we borrow more scores from a seen classifier $f_i$ if it is semantically similar to the unseen category $j$.

### Semantic Embedding Methods

The semantic embedding methods consider the relationship between the semantic space and input space.

#### Projection to Embedding Space

We learn a mapping matrix $P$ from the input space $x$ to the semantic space $a$.

$$ a = Px $$

During training, $P$ is learned by minimizing the loss

$$ \min_P \sum_i \| Px_i - a_{c(i)} \|^2 $$

Note that $a_{c(i)}$ is the attribute vector of category to which sample $i$ belong and it is known in advance.

During testing,

$$ y_j = \arg\min_{c \in \mathcal{C}^t} \| Px_j - a_c \|^2 $$

#### Projection to Compatibility Space

We learn a compatibility matrix $M$ that projects both $x$ and $a$ into another latent space, where a compatibility score is computed.

During training,

$$ \min_M \sum_{i}^{n_s}\sum_{c'=1}^{c_s} \max[ 0, \delta(c' \neq c(i)) + x_i^TMa_{c'} - x_i^TMa_{c(i)} ] $$

The interpretation is, consider $c' \neq c(i)$, the indicator function is 1, and we want to minimize the term. So we want $x_i^TMa_{c'} - x_i^TMa_{c(i)}$ to be small, which means $x_i^TMa_{c(i)}$ should be larger than $x_i^TMa_{c'}$.

During testing, we find the most compatible class

$$ y_j = \arg\max_{c\in\mathcal{C}^t} x_j^TMa_c $$

!!!note ""
    Another similar formulation for learning $M$ is

    $$ \min_M \| X^TMA - Y \|^2_F + \lambda_1\|MA\|^2_F + \lambda_2\|X^TM\|^2_F + \lambda_1\lambda_2\|M\|^2_F $$

    where $X = [x_1,\dots,x_n]$ and $A = [a_1,\dots,a_{c_s}]$

    Note that the first term is the most important, and the latter 3 terms are optional regularizers to prevent overfitting.

### Synthetic Methods
