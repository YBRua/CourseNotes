# Zero-Shot Learning

## Training/Testing Categories

Consider a classification problem,

- Training categories $\mathcal{C}^s$
- Testing categories $\mathcal{C}^t$.

In the traditional setting, we assume $\mathcal{C}^s = \mathcal{C}^t$.

In a more generalized setting, $\mathcal{C}^s \neq \mathcal{C}^t$. The testing set might contain labels not seen in the training set (and vice versa).

1. Testing category is a subset of the training category. $\mathcal{C}^t \subseteq \mathcal{C}^s$. This case is trivial.
2. Testing category contains labels not seen in the training category.
   - In the extremee case, $\mathcal{C}^s$ and $\mathcal{C}^t$ have no overlap. We have no training samples for the testing set.

We first assume $\mathcal{C}_s \cap \mathcal{C}_t = \empty$. The training categories and the testing categories have no overlap.

## Zero-Shot Learning

- It is impossible to collect ample annotated examples for all categories.
- **Long-tail categories.** Categories with few examples in the training set.

## Category-level Semantic Information

- Use **category-level semantic information** to bridge the gap between seen and unseen categories.
- Semantic information can be in various forms, such as attributes or category descriptions.

### Attribute Vectors

Each category $c$ is associated with an attribute vector $a_c$, which is manually annotated by human experts.

- **Pros.** Accurate and informative.
- **Cons.** Manual annotation efforts.

Summarize attribute vectors from the training set, and given the attributes of an unseen category, we can predict its label by its attributes.

### Free Category-level Information

Crawled from webpages (e.g., wikipedia) or word vectors, etc.

- **Pros.** Automatic and almost free.
- **Cons.** Less informative than attributes.

## Methods for Zero-Shot Learning

In zero-shot learning, we have 3 spaces: the input space, the category space and the semantic space.

Zero-shot learning methods essentially describes the relationships between the 3 spaces, and can be roughly divided into

1. Semantic relatedness methods.
2. Semantic embedding methods.
3. Synthetic methods.

### Semantic Relatedness Methods

We measure the semantic relatedness between different categories in the semantic space by computing a similarity matrix $\mathbf{S}$ where $\mathbf{S}_{ij} = s_{i,j}$ is the similarity between category $i$ and $j$.

Denote the classifier by $f_c(\cdot)$.

- Classifiers for seen categories: $\{ f_1(\cdot),\dots,f_{c_s}(\cdot) \}$
- Classifiers for unseen categories: $f_{j}(\cdot) = \sum_{i=1}^{c_s} s_{i,j}f_i(\cdot)$

Intuitively, we borrow more scores from a seen classifier $f_i$ if it is semantically similar to the unseen category $j$.

### Semantic Embedding Methods

The semantic embedding methods consider the relationship between the semantic space and input space.

#### Projection to Embedding Space

We learn a mapping matrix $P$ from the input space $x$ to the semantic space $a$.

$$ a = Px $$

During training, $P$ is learned by minimizing the loss

$$ \min_P \sum_i \| Px_i - a_{c(i)} \|^2 $$

Note that $a_{c(i)}$ is the attribute vector of category to which sample $i$ belong and it is known in advance.

During testing,

$$ y_j = \arg\min_{c \in \mathcal{C}^t} \| Px_j - a_c \|^2 $$

#### Projection to Compatibility Space

We learn a compatibility matrix $M$ that projects both $x$ and $a$ into another latent space, where a compatibility score is computed.

During training,

$$ \min_M \sum_{i}^{n_s}\sum_{c'=1}^{c_s} \max[ 0, \delta(c' \neq c(i)) + x_i^TMa_{c'} - x_i^TMa_{c(i)} ] $$

The interpretation is, consider $c' \neq c(i)$, the indicator function is 1, and we want to minimize the term. So we want $x_i^TMa_{c'} - x_i^TMa_{c(i)}$ to be small, which means $x_i^TMa_{c(i)}$ should be larger than $x_i^TMa_{c'}$.

During testing, we find the most compatible class

$$ y_j = \arg\max_{c\in\mathcal{C}^t} x_j^TMa_c $$

!!!note ""
    Another similar formulation for learning $M$ is

    $$ \min_M \| X^TMA - Y \|^2_F + \lambda_1\|MA\|^2_F + \lambda_2\|X^TM\|^2_F + \lambda_1\lambda_2\|M\|^2_F $$

    where $X = [x_1,\dots,x_n]$ and $A = [a_1,\dots,a_{c_s}]$

    Note that the first term is the most important, and the latter 3 terms are optional regularizers to prevent overfitting.

    The closed form solution for this optimization problem is

    $$ M = (XX^T + \lambda_1 I)^{-1}XYA^T(AA^T + \lambda_2 I)^{-1} $$

### Synthetic Methods

Synthesize training samples for unseen categories so that the problem is reduced to a traditional classification task.

Generally, synthetic methods learn a mapping/generator $G$ from the semantic space to visual space (i.e, "generate" visual space samples from semantic space features).

Usually generating features instead of images is easier (before text-to-image diffusion models).

## Problems in Zero-Shot Learning

### Projection Domain Shift

- **Projection.** Refers to the visual-semantic projection.
- **Domain shift.** The same semantic label might correspond to different visual appearances (e.g., both zebras and whales have tails, but their tails look very different), and the projection between seen and unseen categories might be different.

**Solution.** Use unlabeled test samples to adapt visual-semantic projection to unseen categories.

### Hubness Problem

A small set of *hub* unseen categories that become the nearest neighbors to the majority of testing samples.

As a result, most of the unseen test samples will be categorized into a small set of certain unseen categories.

**Solution 1.** Normalize the distance. $ \tilde{d}_{kc} = \frac{d_{kc}}{\sqrt{\sum_i d_{ic}^2}} $.

**Solution 2.** Use rankings. $\mathrm{rank}(c, x_k) = \sum_i \mathbb{I}[d_{ic} < d_{kc}]$, $ y_k = \arg\min \mathrm{rank}(c, x_k)$.

### Semantic Gap

Visual features are visual, but semantic features are not visual. The visual space and the semantic space might not be aligned.

**Solution.** Align visual and semantic space. For example, for each $a_c$, calculate the average of its $k$-nearest $f_s(x_i)$ as the new $a_c$, where $f_s()$ is the projection model.

## Other Topics on Zero-Shot Learning

### Generalized Zero-Shot Learning

In the previous (standard) zero-shot learning, we assume the training and testing categories *have no overlap at all*. The generalized zero-shot learning assumes the training categories $\mathcal{C}_s$ is a subset of testing categories $\mathcal{C}_t$, $\mathcal{C}_s \subseteq \mathcal{C}_t$.

#### Seen Bias

The predcition of test samples is biased toward seen categories. Seen categories tend to have higher scores at test time.

- **Solutions.**
  1. Deduct the prediction scores on seen categories by a constant.
  2. Hierarchical prediction: first predict whether the sample is seen or unseen, and then predict the specific category.

#### Modern Zero-Shot Learning

- In traditional zero-shot learning settings, we do not have any training samples for unseen categories.
- However, in modern zero-shot learning settings, we might have samples for such categories.
  - Also known as open-world / open-vocabulary classification.
  - Refers to the property that the model (trained with sufficiently large datasets) can be applied to classify almost any category.
- E.g., CLIP (Contrastive Language-Image Pre-training).
