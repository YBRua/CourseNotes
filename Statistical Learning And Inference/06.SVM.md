# Support Vector Machines

## Linear SVM

Assume a set of training data $(x_1,y_1),\dots,(x_N,y_N)$ with $x_i \in \mathbb{R}^p$ and $y_i \in \{ 1, -1 \}$. Assume for the moment that the data are linearly separable.

Consider the following classification rule

$$ G(x) = \mathrm{sign}(w^T + b) $$

The goal is to find a set of appropriate parameters $w, b$.

### Maximum Margin Classifier

Define the **margin** of a linear classifier as the width that the boundary could be increased by, before hitting a data point.

The **maximum margin classifier** is the linear classifier with the maximum margin. This leads to the simplest form of SVM (Linear SVM).

- Intuitively the maximum margin design feels safe.
  - If a small error is made near the boundary, the maximum margin gives the least chance of causing a misclassification.
- Some learning theories (e.g., using VC dimension) are related to (but not exactly the same as) the proposition that maximum margin can be beneficial.
- Empirically it also works well.

### Formulation of Linear SVM

Consider the decision hyperplane

$$ \{ x | w^Tx + b = 0 \} $$

Define

- "Plus-Plane" $\{ x | w^Tx + b = 1 \}$
- "Minus-Plane" $\{ x | w^Tx + b = -1 \}$

Let $x^-$ be any point on the minus-plane, and let $x^+$ be the point on the plus-plane that is closest to $x^-$. Note that $w$ is perpendicular to the plus and minus planes, and we have

$$ x^+ = x^- + \lambda w $$

for some $\lambda$.

We can thus formulate

$$\begin{align*}
    &w^Tx^- + b = -1 &\quad(1)\\
    &w^Tx^+ + b = 1 &\quad(2)\\
    &x^+ = x^- + \lambda w &\quad(3)\\
    &\| x^+ - x^- \| = M &\quad(4)
\end{align*}$$

where $M$ is the margin width.

We can then interpret $M$ with $w$ and $b$.

Plug (3) into (2),

$$ w^T(x^- + \lambda w) + b = 1 \Longrightarrow (w^Tx^- + b) + \lambda w^Tw = 1 $$

Therefore,

$$ \lambda w^Tw = 2 \Longrightarrow \lambda = \frac{2}{w^Tw} $$ (5)

Plug (3) into (4) and using (5),

$$ M = \|x^+ - x^-\| = \lambda \|w\| = \frac{2}{\|w\|} $$

We should therefore maximize the margin $M = \frac{2}{\|w\|}$,

$$ \max_w \frac{2}{\|w\|} $$

under the constraint that all data points are correctly classified.

Note that maximizing $1/\|w\|$ is equivalent to minimizing $\|w\|$, and we thus have

$$ \begin{align*}
    \min_{w} &\quad \frac{1}{2}\|w\|^2 \\
    \mathrm{s.t.} &\quad y_i(w^Tx_i + b) \ge 1
\end{align*} $$

#### Dual Formulation of Linear SVM

Introduce Lagrangian multipliers $\alpha_i \ge 0$ and the Lagrangian function is given by

$$ \mathcal{L}(w, b, \alpha) = \frac{1}{2}\|w\|^2 + \sum_{i=1}^N \alpha_i (1 - y_i(w^Tx_i + b)) $$

Set $\nabla\mathcal{L} = 0$,

$$ \nabla_w\mathcal{L} = w - \sum_{i=1}^N \alpha_i y_i x_i = 0 \Longrightarrow w = \sum_{i=1}^N \alpha_i y_i x_i $$

$$ \nabla_b\mathcal{L} = \sum_{i=1}^N \alpha_i y_i = 0 $$

Substitute $w$, the dual function is given by

$$ \begin{align*}
    g(\alpha) &= \frac{1}{2}w^Tw + \sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i y_iw^Tx_i - \sum_{i=1}^N \alpha_i y_i b\\
    &= \sum_{i=1}^N\alpha_i + \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j y_iy_j x_i^T x_j\\
    &= \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j
\end{align*} $$

The dual form of Linear SVM is

$$\begin{align*}
    \max_{\alpha} &\quad g(\alpha)\\
    \mathrm{s.t.} &\quad \alpha_i \ge 0 \\
    &\quad \sum_{i=1}^N \alpha_i y_i = 0
\end{align*}$$

- A standard quadratic programming w.r.t. $\alpha$.
- $w$ can be recovered by $w = \sum_{i=1}^N \alpha_i y_i x_i$.

#### Remarks

By complementary slackness,

$$ \alpha_i (1- y_i(w^Tx_i + b)) = 0 $$

- If $\alpha_i > 0$, then $y_i(w^Tx_i + b) = 1$, and $x_i$ lies on the boundary.
- Otherwise, $x_i$ is not on the boundary, and $\alpha_i = 0$.
- Many of the $\alpha_i$'s would be zero.
- $x_i$ with non-zero $\alpha_i$ (i.e., lie on the boundary) are known as the **support vectors**.
- Let $\mathcal{S}$ be the index set of support vectors,
  - $w = \sum_{i \in \mathcal{S}} \alpha_i y_i x_i$
  - $b = \frac{1}{|\mathcal{S}|} \sum_{i\in\mathcal{S}} (y_s - w^Tx_s)$
- When testing with a new data $z$,
  - $w^Tz+b = \sum_{i\in\mathcal{S}} \alpha_iy_i(x_i^Tz)+b$
  - Changes in the interior points do not affect the decision boundary.

## Non-Linearly Separable Problems

### Error Penalty

Allow "error" $\xi_i$ in classification, with penalty $C$

$$\begin{align*}
    \min &\quad \frac{1}{2}\|w\|^2 + C\sum_{i=1}^N \xi_i\\
    \mathrm{s.t.} &\quad y_i(w^Tx_i + b) \ge 1 - \xi_i\\
    &\quad \xi_i \ge 0
\end{align*}$$

Similarly we can write the Lagrangian, by introducing multipliers $\alpha_i \ge 0$ and $\beta_i \ge 0$ for the two sets of inequality constraints.

$$ \mathcal{L}(w, b, \xi, \alpha, \beta) = \frac{1}{2}w^Tw + C\sum_{i=1}^N \xi_i - \sum_{i=1}^N \alpha_i[y_i(w^Tx_i + b) - (1 - \xi_i)] - \sum_{i=1}^N \beta_i\xi_i$$

The introduction of $\xi_i$ does not affect the form of $\nabla_w \mathcal{L}$ and $\nabla_b \mathcal{L}$,

$$ \nabla_w\mathcal{L} = w - \sum_{i=1}^N \alpha_i y_i x_i = 0 \Longrightarrow w = \sum_{i=1}^N \alpha_i y_i x_i $$

$$ \nabla_b\mathcal{L} = \sum_{i=1}^N \alpha_i y_i = 0 $$

Further we have

$$ \nabla_{\xi_i} \mathcal{L} = C - \alpha_i - \beta_i = 0 \Longrightarrow \alpha_i = C - \beta_i $$

Notice that $\alpha_i = C - \beta_i$ could be replaced by $0 \le \alpha_i \le C$, and therefore the dual problem is

$$\begin{align*}
    \max_{\alpha} &\quad \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j\\
    \mathrm{s.t.} &\quad \sum_{i=1}^N \alpha_i y_i = 0 \\
    &\quad 0 \le \alpha_i \le C
\end{align*}$$

### Mapping into Linearly Separable Space

For a problem that is not linearly separable, we can use a mapping $\phi$ to project data $x \to \phi(x)$ into a feature space so that they are linearly separable.

Building on top of the SVM with penalty,

$$\begin{align*}
    \min &\quad \frac{1}{2}\|w\|^2 + C\sum_{i=1}^N \xi_i\\
    \mathrm{s.t.} &\quad y_i(w^T\underline{\phi(x_i)} + b) \ge 1 - \xi_i\\
    &\quad \xi_i \ge 0
\end{align*}$$

Again we can solve with Lagrangian multipliers and KKT conditions. The details are omitted here. The solution for $w$ is given by

$$ w = \sum_{i=1}^N \alpha_iy_i\phi(x_i) $$

and the dual function is

$$ g(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j \phi(x_i)^T\phi(x_j) $$

we maximize this function subject to a similar set of constraints.

#### Kernel Trick

- Notice that in the dual problem, we only need the inner product $\phi(x_i)^T\phi(x_j)$ instead of the actual feature vectors $\phi(x_i)$ or $\phi(x_j)$.
- This allows us to bypass the computation of $\phi(x)$, allowing more flexible definitions for the kernel (e.g., allow using $\phi()$ that maps $x$ into an infinite dimension).

Replace $\phi(x_i)^T\phi(x_j)$ with an *inner-product kernel*

$$ g(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j \underline{K(x_i, x_j)} $$

and the prediction $y$ is given by

$$ y = w^T\phi(x) + b = \sum_{i=1}^N \alpha_i y_i K(x, x_i) + b $$

We can therefor use $K$ instead of explicitly computing $\phi(x)$. Some popular kernels include

- **Linear kernel.** $K(x,y) = x^Ty$.
- **Polynomial kernel.** $K(x,y) = (x^Ty + c)^d$
- **Gaussian kernel.** $K(x,y) = \exp\left( -\frac{\|x-y\|^2}{2\sigma^2} \right)$
- **Radia basis.** $K(x,y) = \exp(-\gamma\|x-y\|^2)$
- **Sigmoid kernel.** $K(x,y) = \tanh(\alpha x^Ty + c)$
- **Inverse multi-quadratic kernel.** $K(x,y) = \frac{1}{\sqrt{\|x-y\|}2\sigma^2 + c*2}$

Generally, a kernel matrix should be *positive semidefinite*. It also has some properties.

- **Symmetry.** $K(x,y) = K(y,x)$.
- **Cauchy-Schwarz inequality.** $K(x,y)^2 \le K(x,x)K(y,y)$.
- **Closure.**
  - $K(x,y) = cK_1(x,y)$
  - $K(x,y) = c + K_1(x,y)$
  - $K(x,y) = K_1(x,y) + K_2(x,y)$
  - $K(x,y) = K_1(x,y) \cdot K_2(x,y)$

### Kernelized Other Things

We can extend the kernel trick to other classification models, such as least squares and logistic regression.

#### Representation Theorem

The representation theorem states that the weight $w$ can be represented by a weighted combination of $\alpha_i\phi(x_i)$.

$$ w = \sum_i\alpha_i \phi(x_i) $$

#### Kernelized Least Squares

$$ f(x) = w^T\phi(x) = \sum_{i=1}^N \alpha_i K(x, x_i) $$

$$ \min_\alpha (y-K\alpha)^T(y-K\alpha) + \lambda \alpha^TK\alpha $$

$$ \hat{\alpha} = (K + \lambda I)^{-1} y$$

The fitted value is given by

$$ \hat{f}(x) = \sum_{i=1}^N \hat{\alpha_i}K(x, x_i) $$

#### Kernelized Logistic Regression

$$ \min_{\alpha_i, b} \sum_j \frac{1}{1 + \exp(y_j(\sum_i \alpha_i K(x_i,x_j)+b))} $$
