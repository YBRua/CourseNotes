# Support Vector Machines

## Linear SVM

Assume a set of training data $(x_1,y_1),\dots,(x_N,y_N)$ with $x_i \in \mathbb{R}^p$ and $y_i \in \{ 1, -1 \}$. Assume for the moment that the data are linearly separable.

Consider the following classification rule

$$ G(x) = \mathrm{sign}(w^T + b) $$

The goal is to find a set of appropriate parameters $w, b$.

### Maximum Margin Classifier

Define the **margin** of a linear classifier as the width that the boundary could be increased by, before hitting a data point.

The **maximum margin classifier** is the linear classifier with the maximum margin. This leads to the simplest form of SVM (Linear SVM).

- Intuitively the maximum margin design feels safe.
  - If a small error is made near the boundary, the maximum margin gives the least chance of causing a misclassification.
- Some learning theories (e.g., using VC dimension) are related to (but not exactly the same as) the proposition that maximum margin can be beneficial.
- Empirically it also works well.

### Formulation of Linear SVM

Consider the decision hyperplane

$$ \{ x | w^Tx + b = 0 \} $$

Define

- "Plus-Plane" $\{ x | w^Tx + b = 1 \}$
- "Minus-Plane" $\{ x | w^Tx + b = -1 \}$

Let $x^-$ be any point on the minus-plane, and let $x^+$ be the point on the plus-plane that is closest to $x^-$. Note that $w$ is perpendicular to the plus and minus planes, and we have

$$ x^+ = x^- + \lambda w $$

for some $\lambda$.

We can thus formulate

$$\begin{align*}
    &w^Tx^- + b = -1 &\quad(1)\\
    &w^Tx^+ + b = 1 &\quad(2)\\
    &x^+ = x^- + \lambda w &\quad(3)\\
    &\| x^+ - x^- \| = M &\quad(4)
\end{align*}$$

where $M$ is the margin width.

We can then interpret $M$ with $w$ and $b$.

Plug (3) into (2),

$$ w^T(x^- + \lambda w) + b = 1 \Longrightarrow (w^Tx^- + b) + \lambda w^Tw = 1 $$

Therefore,

$$ \lambda w^Tw = 2 \Longrightarrow \lambda = \frac{2}{w^Tw} $$ (5)

Plug (3) into (4) and using (5),

$$ M = \|x^+ - x^-\| = \lambda \|w\| = \frac{2}{\|w\|} $$

We should therefore maximize the margin $M = \frac{2}{\|w\|}$,

$$ \max_w \frac{2}{\|w\|} $$

under the constraint that all data points are correctly classified.

Note that maximizing $1/\|w\|$ is equivalent to minimizing $\|w\|$, and we thus have

$$ \begin{align*}
    \min_{w} &\quad \frac{1}{2}\|w\|^2 \\
    \mathrm{s.t.} &\quad y_i(w^Tx_i + b) \ge 1
\end{align*} $$

#### Dual Formulation of Linear SVM

Introduce Lagrangian multipliers $\alpha_i \ge 0$ and the Lagrangian function is given by

$$ \mathcal{L}(w, b, \alpha) = \frac{1}{2}\|w\|^2 + \sum_{i=1}^N \alpha_i (1 - y_i(w^Tx_i + b)) $$

Set $\nabla\mathcal{L} = 0$,

$$ \nabla_w\mathcal{L} = w - \sum_{i=1}^N \alpha_i y_i x_i = 0 \Longrightarrow w = \sum_{i=1}^N \alpha_i y_i x_i $$

$$ \nabla_b\mathcal{L} = \sum_{i=1}^N \alpha_i y_i = 0 $$

Substitute $w$, the dual function is given by

$$ \begin{align*}
    g(\alpha) &= \frac{1}{2}w^Tw + \sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i y_iw^Tx_i - \sum_{i=1}^N \alpha_i y_i b\\
    &= \sum_{i=1}^N\alpha_i + \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j y_iy_j x_i^T x_j\\
    &= \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j
\end{align*} $$

The dual form of Linear SVM is

$$\begin{align*}
    \max_{\alpha} &\quad g(\alpha)\\
    \mathrm{s.t.} &\quad \alpha_i \ge 0 \\
    &\quad \sum_{i=1}^N \alpha_i y_i = 0
\end{align*}$$

- A standard quadratic programming w.r.t. $\alpha$.
- $w$ can be recovered by $w = \sum_{i=1}^N \alpha_i y_i x_i$.

#### Remarks

By complementary slackness,

$$ \alpha_i (1- y_i(w^Tx_i + b)) = 0 $$

- If $\alpha_i > 0$, then $y_i(w^Tx_i + b) = 1$, and $x_i$ lies on the boundary.
- Otherwise, $x_i$ is not on the boundary, and $\alpha_i = 0$.
- Many of the $\alpha_i$'s would be zero.
- $x_i$ with non-zero $\alpha_i$ (i.e., lie on the boundary) are known as the **support vectors**.
- Let $\mathcal{S}$ be the index set of support vectors,
  - $w = \sum_{i \in \mathcal{S}} \alpha_i y_i x_i$
  - $b = \frac{1}{|\mathcal{S}|} \sum_{i\in\mathcal{S}} (y_s - w^Tx_s)$
- When testing with a new data $z$,
  - $w^Tz+b = \sum_{i\in\mathcal{S}} \alpha_iy_i(x_i^Tz)+b$
  - Changes in the interior points do not affect the decision boundary.

## Non-Linearly Separable Problems

### Error Penalty

Allow "error" $\xi_i$ in classification, with penalty $C$

$$\begin{align*}
    \min &\quad \frac{1}{2}\|w\|^2 + C\sum_{i=1}^N \xi_i\\
    \mathrm{s.t.} &\quad y_i(w^Tx_i + b) \ge 1 - \xi_i\\
    &\quad \xi_i \ge 0
\end{align*}$$
