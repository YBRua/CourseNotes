# Unsupervised Learning

## Introduction

- Learning without a teacher.
- The system aims to exploit data structure from a set of $N$ observations $\{ x_1,\dots,x_N \}$ where $x_i \in \mathbb{R}^p$.

## Clustering

Divides the samples into multiple regions (typically convex regions).

Group or segment a collection of objects into subsets or **clusters**, such that those within each cluster are more closely related to each other than objects in other clusters.

### Cluster Analysis

- Central to all of the goals of cluster analysis is the notion of the degree of similarity (or dissimilarity) between the individual objects.

#### Proximity Matrices

- Dissimilarities can be computed by averaging over the collection of such judgements.
- Dissimilarity represented by a $N \times N$ matrix $D$.
- Each $d_{ij}$ records the proximity between sample $i$ and $j$.
- Most algorithms presume a matrix with non-negative dissimilarities and zero diagonal elements.

#### Dissimilarities based on Variables

- Dissimilarity between two variables

$$ D(x_i, x_j) = \sum_{k=1}^p d(x_{i,k}, x_{j,k}) $$

##### Quantitative Variables

$$ d(x_{i,k}, x_{j,k}) = (x_{i,k} - x_{j,k})^2 $$

##### Categorical Variables

For categorial variables, assume the variable has $M$ possible values. The values can be arranged in a symmetric $M \times M$ matrix

$$ L_{ij} = L_{ji}, \quad L_{ii} = 0, \quad L_{ij} \ge 0 $$

The most common choice is

$$ L_{ij} = 1, \quad \forall i \neq j $$

##### Observation Dissimilarities

The dissimilarity between two samples $D(x_i, x_j)$ can be computed by the weighted sum of each dimension

$$ D(x_i, x_j) = \sum_{k=1}^p w_k d(x_{ik}, x_{jk}), \quad \sum_{k=1}^p w_k = 1 $$

The average dissimilarity over all pairs of samples is given by

$$ \bar{D} = \frac{1}{N^2} \sum_{i=1}^N\sum_{j=1}^N D(x_i, x_j) = \sum_{k=1}^p w_j \bar{d}_j $$

$$ \bar{d}_k = \frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N d_j (x_{ik}, x_{jk}) $$

#### Combinatorial Algorithms

Assign each sample $x_i$ to a cluster $C(i) = 1,\dots, K$.

The **within-cluster scatter** is defined as

$$ W(C) = \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k, C(j) = k} d(x_i, x_j) $$

Note that we should minimize $W(C)$ since samples in the same class should be close to each other.

The **total point scatter** is given by

$$ \begin{align*}
    T &= \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N d(x_i, x_j) \\
    &= \frac{1}{2}\sum_{k=1}^K \sum_{C(i) = k} (\sum_{C(j) = k} d(x_i, x_j) + \sum_{C(j)\neq k}d(x_i, x_j))\\
    &= W(C) + B(C)
\end{align*} $$

where $B(C)$ is the **between-class scatter**

$$ B(C) = \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k} \sum_{C(j)\neq k}d(x_i, x_j) $$

Note that the total point scatter $T$ is a constant, and minimizing $W(C)$ is equivalent to maximizing $B(C)$.

### K-Means

The within-class scatter $W(C)$ can be written as

$$\begin{align*}
    W(C) &= \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k} \sum_{C(j)=k} d(x_i, x_j) \\
    &= \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k} \sum_{C(j)=k} \|x_i - x_j\|^2 \\
    &= \sum_{k=1}^K N_k \sum_{C(i) = k} \| x_i - \bar{x}_k \|^2
\end{align*}$$

where $\bar{x}_k$ is the center of cluster $k$.

#### Algorithm

- **Expectation Step.** Estimate the hidden variable
  - $ C(i) = \arg\min_k \| x_i - \bar{x}_k \|^2 $
- **Maximization Step.** Maximum likelihood estimation
  - $ \bar{x}_k = \frac{1}{N_k} \sum_{C(i) = k} x_k $

### Vector Quantization (VQ)

An extension of the K-Means algorithm that reduces $N$ points to $K$ centroids.

1. Pick a sample point $x_i$ at random.
2. Find the centroid $m_k$ for which $d(x_i, m_k)$ is minimized.
3. Move $m_k$ towards $x_i$ by a small fraction of distance.
4. Repeat until convergence.

### Self-Organizing Map (SOM)

An constrained version of the K-Means algorithm with prototypes on a topological map.

- Tranditional K-Means updates centroids independently.
- SOM updates related centroids accordigly.

1. Find $K$ prototypes $m_j$ placed on a given map.
2. Find closest $m_j$ of a sample $x_i$.
3. Move $m_j$ and its neighbors $m_k$ toward $x_i$.
   - $ m_k \leftarrow m_k + \alpha (x_i - m_k) $

- Less sensitive to initialization

## Dimensionality Reduction

Identifiers low-dimensional manifolds that preserves the relation or association of the original data points.
