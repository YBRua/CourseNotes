# Basis Expansion

*Augment/Replace the vector of inputs $X$ with additional variables, which are transformations of $X$, and then use linear models in this new space of features.*

$f(X)$ can be modeled by

$$ f(X) = \sum_{m=1}^M \beta_m h_m(X), $$

which is a **linear basis expansion** in $X$. Once $h_m$ are fixed, the problem becomes a linear regression problem.

Some common choices for $h_m$ are

- Identity: $h_m(x) = x$
- Polynomial: $h_m(x) = x^p$
- Logarithm: $h_m(x) = \log(x)$
- Sinusoidal: $h_m(x) = \sin(x)$ or $h_m(x) = \cos(x)$
- Indicator: $h_m(x) = \mathbb{I}(L \le x \le U)$

## Piece-wise Polynomials and Splines

Assume $x$ is a one-dimensional feature.

### Piecewise Multinomial

An **order-M spline** with knots $\xi_j, j=1,\dots,K$ is a piecewise-polynomial of order $M$, and has continuous derivatives up to order $M-2$ at each knot.

The general form of the truncated-power basis set would be

$$ h_j(X) = x^{j-1}, j = 1,\dots,M \quad h_{M+l} = (X-\xi_l)^{M-1}_+, l = 1,\dots,K, $$

where $(\cdot)_+$ denotes the positive part.

- A **piecewise constant** has an order $M=1$.
- A **piecewise linear** has an order $M=2$.
- A **cubic spline** has an order $M=4$.

$M=1,2,4$ are the most commonly used orders, and there is usually no reason to go beyond cubic splines unless one is interested in smooth derivatives.

### Natural Cubic Spline

- The behavior of polynomials fit to data tends to be erratic near the boundaries and beyond the boundary knots.
- **Natural cubic splines** adds an additional constraint that the function $f(x)$ is linear beyond the boundary knots.

### B-Spline

## Smoothing Splines

*A spline basis method that avoids the knot selection problem, by using a maximal set of knots*.

Consider: among all functions $f(x)$ with continuous second-order derivatives, find the one that minimizes the penalized residual sum of squares

$$ RSS(f, \lambda) = \sum_{i=1}^N \{ y_i - f(x_i) \}^2 + \lambda \int f''(t)^2 \mathrm{d}t, $$

where $\lambda$ is a fixed nonnegative hyperparameter.

- $\lambda = 0$: no constraint on smoothness, $f$ can be any function that interpolates the data.
- $\lambda = +\infty$: no second-order derivative is tolerated, $f$ is a linear function (ordinary least squares).

It can be shown that the solution to the optimization problem is a *natural spline* with knots at the unique values at the $x_i, i=1,\dots,N$. It can be written as

$$ f(x) = \sum_{j=1}^N N_j(x)\theta_j $$

## Multidimensional Splines

## Wavelet Smoothing

### Wavelet Functions

Wavelet bases are generated by *translations* and *dilations* of a single scaling function $\phi(x)$ (a.k.a. the *father*).

#### Translations

- Let $\phi(x) = \mathbb{I}[x \in [0, 1]]$, then $\phi_{0,k}(x) = \phi(x-k)$ generates an orthonormal basis for functions with jumps at integers.
- The generated functions forms a space called the **reference space** $V_0$.

#### Dilations

The dilations $\phi_{1,k}(x) = \sqrt{2}\phi(2x-k)$ forms an orthonormal basis for a space $V_1 \supset V_0$.

More generally, we have

$$ \phi_{j,k}(x) = 2^{j/2} \phi(2^jx-k), $$

and $V_0 \subset V_1 \subset \cdots \subset V_{j}$. As $j$ increases, the space $V_j$ becomes more fine-grained (contains more details).

### Mother Wavelets

> Although the formulations are not very interesting, the names are interesting.

$$ \psi(x) = \phi(2x) - \phi(2x-1), $$

$$ \psi_{0,k}(x) = \psi(x-k), \quad \psi_{j,k}(x) = 2^{j/2} \psi(2^j x -k) $$

The spaces spanned by $\psi_{j,k}$ are called denoted by $W_j$.

#### Relations between Father and Mother Wavelets

- $V_0 \quad \phi_{0,k}(x) = \phi(x-k)$
- $V_1 \quad \phi_{1,k}(x) = \sqrt{2}\phi(2x-k)$
- $V_0 \quad \psi_{0,k}(x) = \psi(x-k)$

$$ V_1 = V_0 \oplus W_0, \quad V_{j+1} = V_j \oplus W_j $$

By expanding $V_j$, we have

$$ V_{j+1} = V_0 \oplus W_0 \oplus W_1 \oplus \cdots \oplus W_{j} $$

It can be interpreted as adding details $W_j$ to a sketch $V_0$.

### Wavelet Noise Reduction

- Set the details to zero if they are very small.
