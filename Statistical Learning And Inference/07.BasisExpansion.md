# Basis Expansion

*Augment/Replace the vector of inputs $X$ with additional variables, which are transformations of $X$, and then use linear models in this new space of features.*

$f(X)$ can be modeled by

$$ f(X) = \sum_{m=1}^M \beta_m h_m(X), $$

where $h_m$ denotes the $m$-th transform on $X$, $m=1,\dots,M$. This is known as a **linear basis expansion** in $X$. Once $h_m$ are fixed, the problem becomes a linear regression problem (on the transformed new variables).

Some common choices for $h_m$ are

- Identity: $h_m(x) = x$
- Polynomial: $h_m(x) = x^p$
- Logarithm: $h_m(x) = \log(x)$
- Sinusoidal: $h_m(x) = \sin(x)$ or $h_m(x) = \cos(x)$
- Indicator: $h_m(x) = \mathbb{I}(L \le x \le U)$

## Piece-wise Polynomials and Splines

Assume $x$ is a one-dimensional feature.

### Piecewise Multinomial

- We consider the family of piecewise polynomials and splines that allow for local polynomial representation of data.
- Preferrably we want functions that are *continuous at the knots*
  - e.g., $f(\xi_i^-) = f(\xi_i^+)$, which is enforced by an equality constraint

An **order-M spline** with knots $\xi_j, j=1,\dots,K$ is a piecewise-polynomial of order $M$, and has continuous derivatives up to order $M-2$ at each knot[^knot].

[^knot]: A knot can be think of as the point at which two piecewise functions intersect.

The general form of the truncated-power basis set would be

$$ h_j(X) = x^{j-1}, j = 1,\dots,M \quad h_{M+l} = (X-\xi_l)^{M-1}_+, l = 1,\dots,K $$

where $(\cdot)_+$ denotes the positive part.

#### Examples

$M=1,2,4$ are the most commonly used orders, and there is usually no reason to go beyond cubic splines unless one is interested in smooth derivatives.

##### Cubic Spline

A **cubic spline** has an order $M=4$.

$$ h_1(X) = 1, h_2(X) = X, h_3(X) = X^2, h_4(X) = X^3, h_5(X) = (X-\xi_1)^3_+, h_6(X) = (X-\xi_2)^3_+ $$

There are 6 basis functions, corresponding to a six-dimensional linear space of functions.

##### Piecewise Constant

A **piecewise constant** has an order $M=1$.

$$ f(x) = \sum_{d=1}^D \beta_d h_d(x) $$

$$ h_1(x) = \mathbb{I}(x \le \xi_1), h_2(x) = \mathbb{I}(\xi_1 \le x < \xi_2), h_3(x) = \mathbb{I}(\xi_2 \le x) $$

##### Piecewise Linear

A **piecewise linear** has an order $M=2$.

$$ f(x) = \beta_1 + \beta_2 x + \beta_3 (x-\xi_1)_+ + \beta_4 (x-\xi_2)_+ $$

### Natural Cubic Spline

- The behavior of polynomials fit to data tends to be erratic near the boundaries and beyond the boundary knots.
- **Natural cubic splines** adds an additional constraint that the function $f(x)$ is *linear beyond the boundary knots*.

A **natural cubic spline** with $K$ knots is represented by $K$ basis functions. Starting from the truncated power series basis and applying the boundary constraints, we can derive the form

$$ N_1(X) = 1, N_2(X) = X, N_{k+2}(X) = d_k(X) - d_{K-1}(X) $$

where 

$$ d_k(X) = \frac{(X - \xi_k)^3_+ - (X-\xi_K)^3_+}{\xi_K - \xi_k} $$

When $K=2$

$$ N_3(X) = d_1(x) - d_1(x) = 0 $$

When $K=3$,

$$ N_3(X) = d_1(X) - d_2(X) = \frac{(x-\xi_1)^3_+ - (x-\xi_3)^3_+}{\xi_3 - \xi_1} - \frac{(x-\xi_2)^3_+ - (x-\xi_3)^3_+}{\xi_3 - \xi_2} $$

$$ N_4(X) = 0 $$

$$ f(X) = \sum_{i=1}^K \beta_iN_i(X) $$

and we assume linearity beyond boundary knots $\xi_1, \xi_3$.

### B-Spline

The truncated power series basis is intuitive mathematically, but it could be problematic in implementation due to the rounding issues caused by the powers. In this section, we introduce the B-Splines, which can be used for representing polynomial splines, and are more numerically friendly.

We start by augmenting the sequence of knots with $\xi_0 < \xi_1$ and $\xi_K < \xi_{K+1}$. The additional $\xi_0$ and $\xi_{K+1}$ are usually selected to mark the domain over which we wish to evaluate the spline.

Define an **augmented knot sequence** such that

- $\tau_1 \le \tau_2 \le \dots \le \tau_M \le \xi_0$
- $\tau_j + M = \xi_j, j=1,\dots,K$
- $\xi_{K+1} \le \tau_{K+M+1} \le \dots \le \tau_{K+2M}$

The choice of $\tau$ beyond the boundaries are arbitrary, and it is customary to make them all the same and equal to $\xi_0$ and $\xi_{K+1}$.

Denote by $B_{i,m}(x)$ the $i$-th B-Spline basis function of order $m$ for the knot sequence $\tau$, with $m \le M$.

$$ B_{i,1}(x) = \begin{cases}
    1 &\quad \tau_i \le x \le \tau_{i+1}\\
    0 &\quad o.w.
\end{cases} $$

for $i=1,\dots,K+2M-m$, which are also known as the Haar basis functions.

$$ B_{i,m}(x) = \frac{x-\tau_i}{\tau_{i+m-1}-\tau_i}B_{i,m-1}(x) + \frac{\tau_{i+m} - x}{\tau_{i+m} - \tau_{i+1}} B_{i+1,m-1}(x) $$

The spline functions of order $m$ on a given set of knots can be expressed as a linear combination of B-Splines, $\sum_i \alpha_i B_{i,m}(x)$.

## Smoothing Splines

*A spline basis method that avoids the knot selection problem, by using a maximal set of knots*.

Consider: among all functions $f(x)$ with continuous second-order derivatives, find the one that minimizes the penalized residual sum of squares

$$ RSS(f, \lambda) = \sum_{i=1}^N \{ y_i - f(x_i) \}^2 + \lambda \int f''(t)^2 \mathrm{d}t, $$

where $\lambda$ is a fixed nonnegative hyperparameter.

- $\lambda = 0$: no constraint on smoothness, $f$ can be any function that interpolates the data.
- $\lambda = +\infty$: no second-order derivative is tolerated, $f$ is a linear function (ordinary least squares).

It can be shown that the solution to the optimization problem is a *natural spline* with knots at the unique values of the $x_i, i=1,\dots,N$. It can be written as

$$ f(x) = \sum_{j=1}^N N_j(x)\theta_j $$

The RSS thus reduces to

$$ RSS(\theta, \lambda) = (y-N\theta)^T(y-N\theta) + \lambda\theta^T\Omega_N\theta $$

where $N_{ij} = N_j(x_i)$, $\Omega_{N_{ij}} = \int N_i''(t)N_j''(t)\mathrm{d}t$

The solution is therefore

$$ \hat{\theta} = (N^TN +\lambda\Omega_N)^{-1}N^Ty $$

The fitted smoothing spline is given by

$$ \hat{f}(x) = \sum_{j=1}^N N_j(x)\hat{\theta}_j $$

### Smoother Matrix

We discuss intuitive ways of specifying the amount of smoothing (i.e., choosing $\lambda$).

Let $\hat{\mathbf{f}}$ be the N-dim vector of fitted values $\hat{f}(x_i)$ at the training predictors $x_i$. Then

$$ \hat{\mathbf{f}} = N(N^TN + \lambda\Omega_N)^{-1}N^Ty = S_\lambda y $$

The fit is linear in $y$, and $S_\lambda$ is known as the **smoother matrix**. The matrix is symmetric and positive semidefinite, and has a real eigendecomposition. We can rewrite $S_\lambda$ into the Reinsch form

$$ \begin{align*}
    S_\lambda &= (N^{-T} (N^TN+\lambda\Omega_N)N^{-1})^{-1}\\
    &= (N^{-T}N^TNN^{-1} + \lambda N^{-T}\Omega_NN^{-1})^{-1}\\
    &= (I + \lambda N^{-T}\Omega_NN^{-1})^{-1}\\
    &= (I + \lambda K)^{-1}
\end{align*} $$

where $K = N^{-T}\Omega_NN^{-1}$

Since $\hat{\mathbf{f}} = S_\lambda y$ solves

$$ \min_{\mathbf{f}} \|(y-\mathbf{f})\|^2  + \mathbf{f}^TK\mathbf{f} $$

The eigen-decomposition of $S_\lambda$ is

$$ S_\lambda = \sum_{k=1}^N \rho_k(\lambda)u_ku_k^T, \quad \rho_k(\lambda) = \frac{1}{1 + \lambda d_k} $$

where $d_k$ is the corresponding eigenvalue of $K$.

The smoother matrix for a smoothing spline is nearly *banded*, indicating an equivalent kernel with local support.

### Bias-Variance Tradeoff

Denote the degree of freedom

$$ df_{\lambda}  = \sum_{k=1}^N \rho_k(\lambda) = \sum_{k=1}^N \frac{1}{1 + \lambda d_k} $$

It can be shown that

$$ \mathbf{cov}(\hat{\mathbf{f}}) = S_{\lambda}S_\lambda^T, \quad \mathbf{var}(\hat{\mathbf{f}}) = \mathbf{diag}(S_{\lambda}S_\lambda^T), \quad \mathbf{bias}(\hat{\mathbf{f}}) = y -\mathbb{E}[\hat{\mathbf{f}}] = y -S_\lambda y = (I-S_\lambda)y $$

As $\lambda$ increases, $df_\lambda$ decreases, the variance decreases but the bias increases.

## Multidimensional Splines

We now extended the splines to multi-dimensions. Assume $X \in \mathbb{R}^2$, and we have a basis of functions $h_{1k}(X_1), k=1,\dots,M_1$ for coordinate $X_1$, and similarly $h_{2k}(X_2), k=1,\dots,M_2$ for $X_2$.

The $M_1 \times M_2$ **tensor product matrix** is defined by

$$ g_{jk}(X) = h_{1j}(X_1)h_{2k}(X_2) $$

which can be used for representing a two-dimensional function

$$ g(X) = \sum_{j=1}^{M_1}\sum_{k=1}^{M_2} \theta_{jk}g_{jk}(X) $$

### Multidimensional Smoothing Splines

The smoothing spline generalizes to high dimensionality as well

$$ \min_f \sum_{i=1}^N (y_i - f(x_i))^2 + \lambda J[f] $$

where $J[f]$ is a penalty function, such as

$$ J[f] = \iint_{\mathbb{R}^2} \left[ \left( \frac{\partial^2 f}{\partial x_1^2} \right)^2 + \left( \frac{\partial^2 f}{\partial x_1x_2} \right)^2 + \left( \frac{\partial^2 f}{\partial x_2^2} \right)^2 \right] \mathrm{d}x_1\mathrm{d}x_2 $$

The solution has the form

$$ f(x) = \beta_0 + \beta^Tx + \sum_{j=1}^N \alpha_j h_j(x) \quad h_j(x) = \|x - x_j\|^2\log \|x - x_j\| $$

## Wavelet Smoothing

### Wavelet Functions

Wavelet bases are generated by *translations* and *dilations* of a single scaling function $\phi(x)$ (a.k.a. the *father*).

#### Translations

- Let $\phi(x) = \mathbb{I}[x \in [0, 1]]$, then $\phi_{0,k}(x) = \phi(x-k)$ generates an orthonormal basis for functions with jumps at integers.
- The generated functions forms a space called the **reference space** $V_0$.

#### Dilations

The dilations $\phi_{1,k}(x) = \sqrt{2}\phi(2x-k)$ forms an orthonormal basis for a space $V_1 \supset V_0$.

More generally, we have

$$ \phi_{j,k}(x) = 2^{j/2} \phi(2^jx-k), $$

and $V_0 \subset V_1 \subset \cdots \subset V_{j}$. As $j$ increases, the space $V_j$ becomes more fine-grained (contains more details). Each $V_j$ is spanned by $\phi_{j,k}(x)$.

### Mother Wavelets

> Although the formulations are not very interesting, the names are interesting.

$$ \psi(x) = \phi(2x) - \phi(2x-1), $$

$$ \psi_{0,k}(x) = \psi(x-k), \quad \psi_{j,k}(x) = 2^{j/2} \psi(2^j x -k) $$

The spaces spanned by $\psi_{j,k}$ are called denoted by $W_j$.

#### Relations between Father and Mother Wavelets

- $V_0 \quad \phi_{0,k}(x) = \phi(x-k)$
- $V_1 \quad \phi_{1,k}(x) = \sqrt{2}\phi(2x-k)$
- $V_0 \quad \psi_{0,k}(x) = \psi(x-k)$

$$ V_1 = V_0 \oplus W_0, \quad V_{j+1} = V_j \oplus W_j $$

By expanding $V_j$, we have

$$ V_{j+1} = V_0 \oplus W_0 \oplus W_1 \oplus \cdots \oplus W_{j} $$

It can be interpreted as adding details $W_j$ to a sketch $V_0$.

### Wavelet Noise Reduction

- Set the details to zero if they are very small.
