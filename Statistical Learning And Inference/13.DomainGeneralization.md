# Domain Generalization

## Domain Adaptation vs. Domain Generalization

- **Domain adaptation.** unlabeled target domain is *seen* in the training stage.
- **Domain generalization.** unlabeled target domain is *unseen* in the trianing stage.

The goal of domain generalization is to learn *domain-invariant* features for the classifier.

The training set might contain images from multiple **latent domains**.

## Methods

### Latent Domain Labels are Known

#### Domain-Invariant Component Analysis (DICA)

#### Domain Generalization with Adversarial Feature Learning

Assume we have images from $K$ latent domains and their labels $(X_k, Y_k)$.

Train an auto encoder with encoder $Q$ and decoder $P$ on the images to extract latent domain features.

$$ H_k = Q(X_k), \quad \hat{X}_k = P(Q(X_k)) $$

The model is trained with three loss functions

$$ \min_{Q,P} \max_{D} L_{rec} + L_{MMD} + L_{GAN} $$

- Reconstruction loss. $L_{rec} = \|\hat{X}_k - X_k\|_F^2$
- MMD loss. $L_{MMD} = \sum_{k_1 \neq k_2} \| H_{k_1} - H_{k_2} \|_F^2$
- GAN loss. $\mathbb{E}_{h \sim p(h)}[\log(D(h))] + \mathbb{E}_{x\sim p(x)}[\log(1 - D(Q(x)))]$
  - where $h$ is a random vector drawn from a prior distribution.
  - the discriminator $D$ forces the extracted features and $h$ to have similar distributions.

### Latent Domain Labels are Unknown

When the latent domain labels are unknown, we try to *discover* the latent domains in the training data.
