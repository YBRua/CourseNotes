# Domain Generalization

## Domain Adaptation vs. Domain Generalization

- **Domain adaptation.** unlabeled target domain is *seen* in the training stage.
- **Domain generalization.** unlabeled target domain is *unseen* in the trianing stage.

The goal of domain generalization is to learn *domain-invariant* features for the classifier.

The training set might contain images from multiple **latent domains**.

## Methods

### Latent Domain Labels are Known

#### Domain-Invariant Component Analysis (DICA)

$$ \max_{B} \frac{\frac{1}{n}\mathrm{Tr}\left( B^TL\left( L +n\epsilon I_n \right)^{-1} K^2 B \right)}{\mathrm{Tr}(B^TKQKB+BKB)} $$

where the numerator preserves the functional relationship $\mathbb{E}[X|Y]$, and the denominator minimizes the differences across domains $\mathbb{E}[X]$.

After obtaining $B$, use $\tilde{K} = KBB^TK$

#### Domain Generalization with Adversarial Feature Learning

Assume we have images from $K$ latent domains and their labels $(X_k, Y_k)$.

Train an auto encoder with encoder $Q$ and decoder $P$ on the images to extract latent domain features.

$$ H_k = Q(X_k), \quad \hat{X}_k = P(Q(X_k)) $$

The model is trained with three loss functions

$$ \min_{Q,P} \max_{D} L_{rec} + L_{MMD} + L_{GAN} $$

- Reconstruction loss. $L_{rec} = \|\hat{X}_k - X_k\|_F^2$
- MMD loss. $L_{MMD} = \sum_{k_1 \neq k_2} \| H_{k_1} - H_{k_2} \|_F^2$
- GAN loss. $\mathbb{E}_{h \sim p(h)}[\log(D(h))] + \mathbb{E}_{x\sim p(x)}[\log(1 - D(Q(x)))]$
  - where $h$ is a random vector drawn from a prior distribution.
  - the discriminator $D$ forces the extracted features and $h$ to have similar distributions.

### Latent Domain Labels are Unknown

When the latent domain labels are unknown, we try to *discover* the latent domains in the training data.

#### Discovering latent domains for multisource domain adaptation

If there are $C$ categories and $K$ domains, group training samples into $CK$ local clusters.

Cluster the means of the local cluster to find domains, in which each domain contains only a local cluster from each category.

#### Reshaping visual datasets for domain adaptation

Denote by $z_{mk}$ the probability that the $m$-th sample belongs to the $k$-th latent domain.

$$\begin{align*}
  \max_{z_{mk}} &\quad \sum_{k\neq k'}\left\| \frac{1}{M_k}\sum_m \phi(x_m)z_{mk} - \frac{1}{M_{k'}} \sum_m \phi(x_m)z_{mk'}\right\|^2\\
  \mathrm{s.t.} &\quad \sum_{k=1}^K z_{mk} = 1\\
  &\quad \frac{1}{M_k} \sum_{m=1}^M z_{mk}y_{mc} = \frac{1}{M}\sum_{m=1}^M y_{mc}
\end{align*}$$
