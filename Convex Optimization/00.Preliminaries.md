# Preliminaries

## Norms

### Inner Product, Euclidean Norm, Angle

### Vectors

**Inner Product.** The **standard inner product** on $\mathbb{R}^n$ is given by

$$ \langle x, y \rangle = x^T y = \sum_{i=1}^n x_i y_i $$

**Euclidean Norm.** The **Euclidean norm**, or $L_2$-norm, of a vector $x \in \mathbb{R}^n$ is defined as

$$ \| x \|_2 = (x^Tx)^{1/2} = (x_1^2 + \cdots + x_n^2)^{1/2} $$

**The Cauchy-Schwartz Inequality.** The **Cauchy-Schwartz inequality** states that $\forall x, y \in \mathbb{R}^n$

$$ | x^Ty | \le \|x\|_2\|y\|_2 $$

**Angle.** The **(unsigned) angle** between nonzero vectors $x, y \in \mathbb{R}^n$ is defined as

$$ \angle(x, y) = \cos^{-1}\frac{x^Ty}{\|x\|_2\|y\|_2} $$

We say $x$ and $y$ are **orthogonal** if $x^Ty = 0$.

### Matrices

**Inner Product of Matrices.** The **standard inner product** on $\mathbb{R}^{m \times n}$ is given by

$$ \langle X, Y \rangle = \mathrm{tr}(X^TY) = \sum_{i=1}^m \sum_{j=1}^n X_{ij}Y_{ij} $$

If $X, Y$ are symmetric $n \times n$ matrices, then the inner product can be written as

$$ \mathrm{tr}(X^TY) = \sum_{i=1}^m \sum_{j=1}^n X_{ij}Y_{ij} = \sum_{i=1}^n X_{ii}Y_{ii} + 2 \sum_{i < j} X_{ij}Y_{ij} $$

**Frobenius Norm.** The **Frobenius norm** of a matrix $X \in \mathbb{R}^{m \times n}$ is

$$ \| X \|_F = \left( \mathrm{tr}(X^TX) \right)^{1/2} = \left( \sum_{i=1}^m \sum_{j=1}^n X_{ij}^2 \right)^{1/2} $$

The Forbenius norm is the Euclidean norm of the vector obtained by "flattening" $X$.

### Norms, Distance, Unit Ball

## Numerical Complexity

- Floating point operations (flops)
  - Addition, subtraction, multiplication, division, square root, etc.
  - Only consider dominant terms
  - Not very accurate

### Complexity of Multiplications

- $x^Ty$, where $x, y \in \mathbb{R}^n$: $O(n)$ flops, $2n-1$.
- $Ax$, where $A \in \mathbb{R}^{m\times n}, y \in \mathbb{R}^n$: $O(mn)$ flops, $m(2n-1)$.
- $AB$, where $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}$: $O(mnp)$.

## Solving Linear Equations with Factored Matrices

### Linear Equations that are Easy to Solve

- **Diagonal matrix.** $Ax = b \Leftrightarrow x = A^{-1}b = \frac{1}{a_{ii}}b$.
  - $O(n)$ flops.
- **Lower triangular matrix.** $Ax = b \Leftrightarrow x = A^{-1}b = \frac{1}{a_{ii}}(b - \sum_{j=1}^{i-1}a_{ij}x_j)$.
  - $O(n^2)$ flops.

### LU Factorization

#### LU Factorization and Gaussian Elimination

Every nonsingular matrix $A \in \mathbb{R}^{n \times n}$ can be factored as

$$ A = PLU $$

where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular and nonsingular.

The compexity for LU factorization is about $2/3n^3$ flops, if no structure of $A$ is exploited.
