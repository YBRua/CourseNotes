# Introduction

> -- “我认识你和我不认识你在改卷的时候还是很有区别的”
> 
> -- “我没有威胁大家的意思”

## Overview of the Course

> “按照我们班之前的水平，考30分，有点难度”

1. Introduction.
2. Convex sets.
   - Line segment, convex set, cone.
   - Operations that preserves convexity.
3. Convex functions.
   - Definition, first-order & second-order conditions.
   - Epigraph, sublevel set.
   - Operations that preserves convexity.
   - Conjugate function.
4. Convex optimization problem.
   - Linear programming.
5. Duality.
   - Lagrangian.
   - Strong and weak duality, qualification constraints.
   - KKT conditions.
6. Approximation (Approximation / Fitting / Estimation / Geometric problems).
7. Unconstrained optimization.
   - Descent method.
   - Gradient descent.
   - Steepest descent method.
   - Newton's method.
8. Equality constrained optimization.
   - Equality constrained Newton's method.
   - Infeasible start Newton's method.
9. Inequality constrained optimization.
   - Barrier method.

## Mathematical Optimization

A **mathematical optimization problem** (a.k.a. **optimization problem**) has the form

$$ \begin{align*}
 \min \quad & f_0(x) \\
 \mathrm{s.t.} \quad & f_i(x) \le b_i \quad i=1,\dots,m
\end{align*} $$

where

- $x$ is the **optimization variable**,
- $f_0$ is the **objective function**,
- $f_i$'s are the **constraint functions**.

## Linear Regression and Least-Squares

### Linear Regression

- Dataset $\mathcal{D} = \{ (x_i, y_i) \}_{i=1}^n$ where $x_i \in \mathbb{R}$ and $y_i \in \mathbb{R}$.
- Objective (intuitively): Find a line $f_{\theta}(x) = ax + b$ that "best" fits the dataset.
  - Let $h_i = f_{\theta}(x_i)$
  - The term "best" is measured by a **loss function**
    - e.g. Mean Square Error $L = \sum_i (h_i - y_i)^2$

$$ \min_{a,b} \sum_{i=0}^{n} (h_i - y_i)^2 = \sum_{i=0}^{n} (ax_i + b - y_i)^2 $$

#### Solving Linear Regression

1. Closed-form solution
   - $ \partial L / \partial a = 0 $
   - $ \partial L / \partial b = 0 $
2. Descent method (e.g. Gradient Descent)
   - $a_{t+1} = a_t - \eta \frac{\partial L}{\partial a}$
   - $b_{t+1} = b_t - \eta \frac{\partial L}{\partial b}$
3. Least Squares
   - *“看到等号就换成减号，然后外面套一个平方”*

#### Least-Squares

$$ \begin{bmatrix}
  y_1\\ \vdots \\ y_n
\end{bmatrix} = \begin{bmatrix}
  x_1, & 1 \\
  \vdots & \vdots \\
  x_n, & 1
\end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} $$

$$ \Rightarrow Y = X \beta $$

## Linear Programming

In a **linear programming** problem, the objective and all constraint functions are *linear*.

$$ \begin{align*}
    \min \quad & c^Tx \\
    \mathrm{s.t.} \quad &a_i^Tx \le b_i
\end{align*} $$

### Chebyshev Approximation

> 打地鼠

## Convex Optimization

> 为什么要研究凸优化？简单。
> 为什么不研究非凸优化？困难。
> 非凸优化怎么做？瞎做。

A **convex optimization** problem is one of the form

$$ \begin{align*}
  \min \quad & f_0(x) \\
  \mathrm{s.t.} \quad & f_i(x) \le b_i \quad i=1,\dots,m
\end{align*} $$

where the functions $f_0, \dots, f_m$ are *convex functions*.

- The local optima of a convex optimization problem is also its global optima.
