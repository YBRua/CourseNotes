# Equality Constrained Minimization

## Equality Constrained Minimization Problem

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

Assume $f$ is convex and twice continuously differentiable; $A \in \mathbb{R}^{p \times n}$ with $\mathrm{rank}(A) = p < n$. This implies that there are fewer contraints than variables, and that the equality constraints are independent. Further assume an optimal solution $x^*$ exists, with optimal value $p^*$.

The Lagrangian is

$$ \mathcal{L}(x, \nu) = f(x) + \nu^T (Ax - b) $$

The KKT conditions are

$$ Ax = b, \quad \nabla f(x) + A^T\nu = 0 $$

Denote the primal and dual optimal variables by $x^*, \nu^*$, we then have

$$ Ax^* = b, \quad \nabla f(x^*) + A^T\nu^* = 0 $$

- The solution of the equality constrained problem is given by the solution of the $n + p$ equations above.
  - $Ax = b$ is called the **primal feasibility equations**.
  - $\nabla f(x) + A^T\nu = 0$ is called the **dual feasibility equations**.

### Eliminating Equality Constraints

Since $\mathrm{rank}(A) = p < n$, the system $Ax = 0$ should have $n - p$ independent solutions. Denote the solutions by $u_1,\dots,u_{n-p}$, such that $Au_i = 0$.

We can then parametrize the feasible set

$$ \{ x | Ax = b \} = \{ Fz + \hat{x} | z \in \mathbb{R}^{n-p} \} $$

where $\hat{x}$ can be chosen as any particular solution of $Ax = b$, and $F = (u_1,\dots,u_{n-p})$ is any matrix whose range is the null space of $A$.

The original problem is therefore equivalent to

$$ \min_z f (Fz + \hat{x}) $$

From its solution $z^*$, the solution for $x$ is given by $x^* = Fz^* + \hat{x}$.

With $x^*$ we can then construct the optimal dual solution $\nu^*$. Consider the KKT condition,

$$ \nabla f(x^*) + A^T\nu^* = 0, \quad A^T\nu^* = -\nabla f(x^*) $$

Note that it is a system of $n$ equations with $p$ variables, where $n > p$, which is a typical ordinary least squares problem. The analytical solution is given by

$$ v^* = -(AA^T)^{-1}A\nabla f(x^*) $$

Note that the solution of least squares not always satisfies the equations (i.e., the line we fit usually does not pass all the data points). However, it holds in this specific case.

To show that this expression is correct, we must verify that it satisfies the dual feasibility condition,

$$ \nabla f(x^*) + A^T(-(AA^T)^{-1}A\nabla f(x^*)) = 0 $$

This can be proved by 证明它和一个满秩矩阵相乘的结果为 0. To construct such a matrix, consider

$$ \begin{bmatrix}
    A \\ F^T
\end{bmatrix} \nabla f(x^*) - A^T(AA^T)^{-1}A\nabla f(x^*) = 0 $$

It can be easily verified that the top block equals to 0. For the bottom block,

$$ F^T \nabla f(x^*) + F^T A^T \nu^* = F^T \nabla f(x^*) + (AF^T)\nu^* = F^T \nabla f(x^*) $$

Note that $\nabla f(x^*) + A^T \nu = 0$, 

$$ F^T \nabla f(x^*) = F^T (-A^T \nu) = 0 $$

Therefore the expression is correct.

## Newton's Method with Equality Constraints

### The Newton Step

#### Second-Order Approximation Perspective

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

Consider the second-order Taylor expansion at some *feasible* $x$.

$$ \begin{align*}
    \min &\quad \hat{f}(x + v) = f(x) + \nabla f(x)^T v +  (1/2) v^T \nabla^2 f(x) v \\
    \mathrm{s.t.} &\quad A(x + v) = b
\end{align*} $$

We assume $x$ to be feasible, so $Ax = b$ holds,

$$ \begin{align*}
    \min &\quad \hat{f}(x + v) = f(x) + \nabla f(x)^T v +  (1/2) v^T \nabla^2 f(x) v \\
    \mathrm{s.t.} &\quad Av = 0
\end{align*} $$

We define the **Newton step** $\Delta x_{nt}$ as the solution to the convex quadratic problem.

The KKT system for this problem is

$$ \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} \begin{bmatrix}
    v \\ \lambda
\end{bmatrix} = \begin{bmatrix}
    -\nabla f(x) \\ 0
\end{bmatrix} $$

The **Newton direction** $v$ is given by the solution to this KKT system.

#### KKT Perspective

Consider from the perspective of KKT systems

$$ Ax^* = b, \quad \nabla f(x^*) + A^T \nu^* $$

We want $x^* = x + \Delta x_{nt}$, and therefore

$$ A(x + \Delta x_{nt}) = b, \quad \nabla f(x + \Delta x_{nt}) + A^T\nu^* = 0 $$

Consider the first-order approximation for $ \nabla f(x + \Delta x_{nt})$

$$ \nabla^2 f(x) \Delta x_{nt} + \nabla f(x) + A^T \nu^* = 0 $$

Again assume $x$ is feasible, and $A\Delta x_{nt} = 0$, from which we can derive the same set of linear equations

$$ \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} \begin{bmatrix}
    v \\ \lambda
\end{bmatrix} = \begin{bmatrix}
    -\nabla f(x) \\ 0
\end{bmatrix} $$

### Feasible Start Newton's Method

This method starts from some feasible point $x_0$ s.t. $Ax_0 = b$.

- Repeat
  - Solve for Newton's direction $\Delta x_{nt}$.
  - Compute Newton's decrement $\lambda (x)$.
  - Break if $ \frac{1}{2} \lambda (x)^2 \le \epsilon$.
  - Backtracking line search to find a $t$, starting from $t = 1$.
    - While $f(x + t\Delta x_{nt}) > f(x) + \alpha t \nabla f(x)^T \Delta x_{nt}$
      - $t = \beta t$
  - $x + x + t \Delta x_{nt}$

#### Remarks

- The algorithm is a descent method. I.e., $f(x^{(k+1)}) \le f(x^{k})$.
  - $f(x + v) \approx f(x) + \nabla f(x)^T v + \frac{1}{2} v^T \nabla^2 f(x) v = f(x) = \frac{1}{2}\lambda(x)^2 \le f(x)$

### Infeasible Start Newton's Method

Assume we start from some infeasible $Ax \neq b$.

The KKT system now becomes

$$ \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} \begin{bmatrix}
    v \\ \lambda
\end{bmatrix} = \begin{bmatrix}
    -\nabla f(x) \\ Ax - b
\end{bmatrix} $$

#### Primal-Dual Interpretation

A **primal-dual method** means one where we update both the primal and dual variable.

We express the optimality conditions as

$$ r(x, \nu) = (r_{dual} (x, \nu), r_{pri}(x, \nu)) $$

where

$$ r_{dual}(x, \nu) = \nabla f(x) + A^T\nu, \quad r_{pri}(x, \nu) = Ax - b $$

are the **dual residual** and **primal residual**.

Consider the first-order Taylor approximation of $r$,

$$ r(y + z) = \hat{r}(y+z) = r(y) + D_{r(y)}z $$

where $y = (x, \nu)$ and $D_{r(y)} \in \mathbb{R}^{(n + p) \times (n + p)}$ is the derivative of $r$ evaluated at $y$.

$$ D_{r(y)} = \begin{pmatrix}
    \frac{\partial r_d}{\partial x} & \frac{\partial r_d}{\partial \nu}\\
    \frac{\partial r_p}{\partial x} & \frac{\partial r_p}{\partial \nu}
\end{pmatrix} = \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} $$

#### Residual Norm Reduction Property

The Newton direction at an infeasible point is *not necessarily* a descent direction for $f$.

$$ \frac{\partial f(x + t \Delta x)}{\partial t} \mid_{t=0} = \nabla_x f(x)^T \Delta x $$

$$ \nabla_x f(x) = - \nabla^2f(x)\Delta x - A^T \lambda $$

$$ \Delta x^T \nabla_x f(x) = - \Delta x^T \nabla^2 f(x) \Delta x - \Delta x^T A^T \lambda $$

which is not necessarily negative.

However, the norm of the residual decreases in the Newton direction.