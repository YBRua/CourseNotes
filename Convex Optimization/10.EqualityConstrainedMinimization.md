# Equality Constrained Minimization

## Equality Constrained Minimization Problem

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

Assume $f$ is convex and twice continuously differentiable; $A \in \mathbb{R}^{p \times n}$ with $\mathrm{rank}(A) = p < n$. This implies that there are fewer contraints than variables, and that the equality constraints are independent. Further assume an optimal solution $x^*$ exists, with optimal value $p^*$.

The Lagrangian is

$$ \mathcal{L}(x, \nu) = f(x) + \nu^T (Ax - b) $$

The KKT conditions are

$$ Ax = b, \quad \nabla f(x) + A^T\nu = 0 $$

Denote the primal and dual optimal variables by $x^*, \nu^*$, we then have

$$ Ax^* = b, \quad \nabla f(x^*) + A^T\nu^* = 0 $$

- The solution of the equality constrained problem is given by the solution of the $n + p$ equations above.
  - $Ax = b$ is called the **primal feasibility equations**.
  - $\nabla f(x) + A^T\nu = 0$ is called the **dual feasibility equations**.

### Equality Constrained Convex Quadratic Minimization

Consider the problem

$$\begin{align*}
    \min &\quad \frac{1}{2}x^TQx + q^Tx + r\\
    &\quad Ax = b
\end{align*}$$

where $P$ is positive semidefinite. The optimality conditions are

$$ Ax^* = b \quad Px^* + q + A^T\nu^* = 0 $$

which can be rewritten as

$$ \begin{bmatrix}
    P & A^T\\
    A & 0
\end{bmatrix}\begin{bmatrix}
    x^* \\ \nu^*
\end{bmatrix} = \begin{bmatrix}
    -q \\ b
\end{bmatrix} $$

which is a set of $n+p$ linear equations of $x^*, \nu^*$. This is called the **KKT system** for the equality constrained convex quadratic optimization problem. The coefficient matrix is also called the **KKT matrix**.

### Eliminating Equality Constraints

Since $\mathrm{rank}(A) = p < n$, the system $Ax = 0$ should have $n - p$ independent solutions. Denote the solutions by $u_1,\dots,u_{n-p}$, such that $Au_i = 0$.

We can then parametrize the feasible set

$$ \{ x | Ax = b \} = \{ Fz + \hat{x} | z \in \mathbb{R}^{n-p} \} $$

where $\hat{x}$ can be chosen as any particular solution of $Ax = b$, and $F = (u_1,\dots,u_{n-p})$ is any matrix whose range is the null space of $A$.

We can represent $x$ by $x = Fz + \hat{x}$ The original problem is therefore equivalent to

$$ \min_z f (Fz + \hat{x}) $$

From its solution $z^*$, the solution for $x$ is given by $x^* = Fz^* + \hat{x}$.

With $x^*$ we can then construct the optimal dual solution $\nu^*$. Consider the KKT condition,

$$ \nabla f(x^*) + A^T\nu^* = 0, \quad A^T\nu^* = -\nabla f(x^*) $$

Note that it is a system of $n$ equations with $p$ variables, where $n > p$, which is a typical ordinary least squares problem. The analytical solution is given by

$$ v^* = -(AA^T)^{-1}A\nabla f(x^*) $$

Note that the solution of least squares not always satisfies the equations (intuitively, the line we fit usually does not pass all the data points). However, it holds in this specific case.

To show that this expression is correct, we must verify that it satisfies the dual feasibility condition,

$$ \nabla f(x^*) + A^T(-(AA^T)^{-1}A\nabla f(x^*)) = 0 $$

This can be proved by 证明它和一个满秩矩阵相乘的结果为 0. To construct such a matrix, consider

$$ \begin{bmatrix}
    A \\ F^T
\end{bmatrix} (\nabla f(x^*) - A^T(AA^T)^{-1}A\nabla f(x^*)) = 0 $$

It can be easily verified that the top block equals to 0. For the bottom block,

$$ F^T \nabla f(x^*) + F^T A^T \nu^* = F^T \nabla f(x^*) + (AF^T)\nu^* = F^T \nabla f(x^*) $$

Note that $\nabla f(x^*) + A^T \nu = 0$ and $AF = 0$,

$$ F^T \nabla f(x^*) = F^T (-A^T \nu) = 0 $$

Therefore the expression is correct.

## Newton's Method with Equality Constraints

### The Newton Step

#### Second-Order Approximation Perspective

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

Consider the second-order Taylor expansion at some *feasible* $x$.

$$ \begin{align*}
    \min &\quad \hat{f}(x + v) = f(x) + \nabla f(x)^T v +  (1/2) v^T \nabla^2 f(x) v \\
    \mathrm{s.t.} &\quad A(x + v) = b
\end{align*} $$

We assume $x$ to be feasible, so $Ax = b$ holds,

$$ \begin{align*}
    \min &\quad \hat{f}(x + v) = f(x) + \nabla f(x)^T v +  (1/2) v^T \nabla^2 f(x) v \\
    \mathrm{s.t.} &\quad Av = 0
\end{align*} $$

We define the **Newton step** $\Delta x_{nt} = v^*$ as the solution to this convex quadratic problem.

Using the results from [previous analysis](#equality-constrained-convex-quadratic-minimization), the KKT system for this problem is

$$ \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} \begin{bmatrix}
    \Delta x_{nt} \\ w
\end{bmatrix} = \begin{bmatrix}
    -\nabla f(x) \\ 0
\end{bmatrix} $$

where $w$ is the Lagrangian multipliers associated with the equality constraints. The **Newton direction** $\Delta x_{nt}$ is given by the solution to this KKT system.

#### KKT Perspective

Consider from the perspective of KKT systems

$$ Ax^* = b, \quad \nabla f(x^*) + A^T \nu^* = 0 $$

We want $x^* = x + \Delta x_{nt}$, and therefore

$$ A(x + \Delta x_{nt}) = b, \quad \nabla f(x + \Delta x_{nt}) + A^T\nu^* = 0 $$

Consider the first-order approximation for $ \nabla f(x + \Delta x_{nt})$

$$ \nabla^2 f(x) \Delta x_{nt} + \nabla f(x) + A^T \nu^* = 0 $$

Again assume $x$ is feasible, and $A\Delta x_{nt} = A(x^* - x) = 0$, from which we can derive the same set of linear equations

$$ \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} \begin{bmatrix}
    \Delta x_{nt} \\ \nu^*
\end{bmatrix} = \begin{bmatrix}
    -\nabla f(x) \\ 0
\end{bmatrix} $$

### Feasible Start Newton's Method

This method starts from some feasible point $x_0$ s.t. $Ax_0 = b$.

---

**Algorithm.** Newton's method for equality constrained minimization

1. Given a feasible starting point $x \in \mathbf{dom}(f)$ with $Ax = b$, tolerance $\epsilon>0$
2. Repeat
   1. Solve for Newton's direction $\Delta x_{nt}$.
   2. Compute Newton's decrement $\lambda (x)$.
   3. Quit if $ \frac{1}{2} \lambda (x)^2 \le \epsilon$.
   4. Backtracking line search to find a $t$, starting from $t = 1$.
      1. While $f(x + t\Delta x_{nt}) > f(x) + \alpha t \nabla f(x)^T \Delta x_{nt}$
      2. $t = \beta t$
   5. Update $x + x + t \Delta x_{nt}$

---

- The algorithm is a descent method. I.e., $f(x^{(k+1)}) \le f(x^{k})$.
  - $f(x + v) \approx f(x) + \nabla f(x)^T v + \frac{1}{2} v^T \nabla^2 f(x) v = f(x) - \frac{1}{2}\lambda(x)^2 \le f(x)$

### Infeasible Start Newton's Method

Assume we start from some infeasible $x \in \mathbf{dom}(f)$ with $Ax \neq b$.

We want to find some $\Delta x_{nt}$ that at least approximately satisfies the optimality conditions, $x+ \Delta x_{nt} \approx x^*$. We substitute $x+\Delta x_{nt}$ for $x^*$ and $w$ for $\nu^*$ in the optimality conditions, and use the first-order approximation

$$ \nabla{}f(x + \Delta x_{nt}) \approx \nabla{}f(x) + \nabla^2{}f(x)\Delta x_{nt} $$

for the gradient to obtain

$$ A(x + \Delta x_{nt}) = b \quad \nabla{}f(x) + \nabla^2{}f(x) \Delta x_{nt} + A^T w = 0 $$

The KKT system now becomes

$$ \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} \begin{bmatrix}
    \Delta x_{nt} \\ w
\end{bmatrix} = \begin{bmatrix}
    -\nabla f(x) \\ b-Ax
\end{bmatrix} $$

#### Primal-Dual Interpretation

A **primal-dual method** means one where we update both the primal and dual variable.

We express the optimality conditions as

$$ r(x, \nu) = (r_{dual} (x, \nu), r_{pri}(x, \nu)), \quad r(x^*, \nu^*) = 0 $$

where

$$ r_{dual}(x, \nu) = \nabla f(x) + A^T\nu, \quad r_{pri}(x, \nu) = Ax - b $$

are the **dual residual** and **primal residual**.

Consider the first-order Taylor approximation of $r$,

$$ r(y + z) \approx \hat{r}(y+z) = r(y) + D_{r(y)}z $$

where $y = (x, \nu)$ and $D_{r(y)} \in \mathbb{R}^{(n + p) \times (n + p)}$ is the derivative of $r$ evaluated at $y$.

$$ D_{r(y)} = \begin{pmatrix}
    \frac{\partial r_d}{\partial x} & \frac{\partial r_d}{\partial \nu}\\
    \frac{\partial r_p}{\partial x} & \frac{\partial r_p}{\partial \nu}
\end{pmatrix} = \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} $$

We define the primal-dual Newton step $\Delta y_{pd}$ as the step $z$ for which the Taylor approximation $\hat{r}(y+z)$ vanishes,

$$ D_{r(y)}z = -r(y) $$

which expands into

$$
\begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix}
\begin{bmatrix}
    \Delta x_{pd}\\
    \Delta \nu_{pd}
\end{bmatrix}
=-
\begin{bmatrix}
    \nabla f(x) + A^T\nu \\
    Ax -b
\end{bmatrix}
$$

If we write $v + \Delta\nu_{pd}$ as $v^+$, the linear system becomes

$$ \begin{bmatrix}
    \nabla^2 f(x) & A^T \\
    A & 0
\end{bmatrix} \begin{bmatrix}
    \Delta x_{pd} \\ \nu^+
\end{bmatrix} = \begin{bmatrix}
    -\nabla f(x) \\ b-Ax
\end{bmatrix} \tag{*}$$

which is exactly the same set of equations for infeasible start Newton's method.

$$ \Delta x_{nt} = \Delta x_{pd}, \quad w = \nu^+ = \nu + \Delta\nu_{pd} $$

#### Residual Norm Reduction Property

The Newton direction at an infeasible point is *not necessarily* a descent direction for $f$.

$$ \left.\frac{\partial f(x + t \Delta x)}{\partial t} \right|_{t=0} = \nabla_x f(x)^T \Delta x $$

If this derivative is negative, then the direction is a descent direction.

By the optimality condition $\nabla{}f(x) + \nabla^2{}f(x) \Delta x_{nt} + A^T w = 0$,

$$ \nabla_x f(x) = - \nabla^2f(x)\Delta x - A^T w $$

Therefore,

$$ \Delta x^T \nabla_x f(x) = - \Delta x^T \nabla^2 f(x) \Delta x - \Delta x^T A^T w $$

which is not necessarily negative.

However, the norm of the residual $\|r\|_2$ decreases in the Newton direction. To see this,

$$\left. \frac{\mathrm{d}\|r(y + t\Delta y)\|^2}{\mathrm{d}t} \right|_{t=0} = -\|r(y)\|_2 \le 0$$

#### Full-Step Feasibility Property

The Newton step $\Delta x_{nt}$ defined by (*) has the property that

$$ A(x + \Delta x_{nt}) = b $$

- If a step length of 1 is taken using $\Delta x_{nt}$, then the following iterates will be feasible.
  - Once $x$ is feasible, the Newton step becomes a feasible direction, so all future steps will be feasible.

With a step $t \in [0,1]$, the next iterate is $x^+ = x + t\Delta x_{nt}$, we can analyze the effect on the equality constraint residual $r_{pri}$

$$ \begin{align*}
    r_{pri}^+ &= A(x +\Delta x_{nt}) - b \\
    &= Ax + tA\Delta x_{nt} - b\\
    &= Ax - b + t(b-Ax)\\
    &= (1-t) (Ax-b)\\
    &= (1-t)r_{pri}
\end{align*} $$

Therefore

$$ r_{pri}^+ = (1-t) r_{pri} $$

- Indicates a linear convergence of $r_{pri}$.
- The primal residual at each step is in the direction of the initial primal residual.
- Once a full step $t=1$ is taken, all future steps are primal feasible.
