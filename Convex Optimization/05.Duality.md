# Duality

## Lagrange Dual Function

### The Lagrangian

Consider an optimization problem (not necessarily convex) in standard form

$$ \begin{align*}
    \min &\quad f_0(x) \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1,\dots,p
\end{align*} $$

We assume the domain $\mathcal{D} = \bigcap \mathbf{dom} f_i \cap \bigcap \mathbf{dom} h_i$ to be nonempty.

The basic idea of Lagrangian duality is to augment the objective function with the weighted sum of the constraints.

Define the **Lagrangian** $L: \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R}^{p} \mapsto \mathbb{R}$ with domain $\mathbf{dom} L = \mathcal{D} \times \mathbb{R}^{m} \times \mathbb{R}^{p}$,

$$ L(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{i=1}^p \nu_i h_i(x) $$

$\lambda_i$ and $\nu_i$ are referred to as the **Lagrangian multipliers** associated with the $i$-th inequality/equality constraints. The vectors $\lambda, \nu$ are referred to as the **dual variables** or **Lagrangian multiplier vectors**.

### The Lagrange Dual Function

Define the **Lagrange dual function** (or simply **dual function**) $g: \mathbb{R}^{m} \times \mathbb{R}^{p} \mapsto \mathbb{R}$ as the minimum value of Lagrangian over $x$,

$$ g(\lambda, \nu) = \inf_{x \in \mathcal{D}} L(x, \lambda, \nu) = \inf_{x \in \mathcal{D}} \left( f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{i=1}^p \nu_ih_i(x) \right) $$

- **Concavity of the Lagrange Dual Function.** The dual function is the pointwise infimum (preserves concavity) of a family of affine functions of $(\lambda, \nu)$, and hence is concave in $\lambda$ and $\nu$ (even if the original $f$ is not convex).

### Lower Bounds on Optimal Value

The dual function $g$ yields a lower bound on the optimal value $p^*$. For any $\lambda \succeq 0$ and any $\nu$,

$$ g(\lambda, \nu) \le p^*. $$

!!!quote Proof
    Suppose $\tilde{x}$ is a feasible point and $\lambda \succeq 0$,

    $$ g(\lambda, \nu) \le L(\tilde{x}, \lambda, \nu) = f_0(\tilde{x}) + \sum_{i=1}^m \lambda_i f_i(\tilde{x}) + \sum_{i=0}^p \nu_i h_i(\tilde{x}) \le f_0(\tilde{x}) $$

    Note that $\lambda_i \ge 0$ and $f_i(\tilde{x}) \le 0, h_i(\tilde{x}) = 0$ by the feasibility of $\tilde{x}$.

Therefore, the function gives a nontrivial lower bound when $\lambda \succeq 0$ and $(\lambda, \nu) \in \mathbf{dom}(g)$. Such pairs of $(\lambda, \nu)$ are referred to as **dual feasible**.

### Examples

#### Least-squares Solution of Linear Equations

$$\begin{align*}
    \min &\quad x^Tx \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*}
$$

The Lagrangian is

$$ L(x, \nu) = x^Tx + \nu^T (Ax - b) $$

To derive the dual function $g(\nu) = \inf_x L$, since $L$ is a convex quadratic function w.r.t. $x$, we can take the gradient and set it to zero.

$$ \nabla_x L = 2x + A^T\nu \coloneqq 0 $$

Therefore $x = -(1/2)A^T\nu$. Plug this minimizing $x$ back into $L$

$$ g(\nu) = -\frac{1}{4} \nu^T AA^T \nu - \nu^T b $$

Note that $g$ is a concave quadratic in $\nu$.

#### Standard Form LP

Consider an LP in standard form

$$\begin{align*}
    \min &\quad c^Tx \\
    \mathrm{s.t.} &\quad Ax = b\\
    &\quad x \succeq 0
\end{align*}$$

The Lagrangian is

$$ L(x, \lambda, \nu) = c^Tx - \lambda^T x + \nu^T(Ax - b). $$

Note that in the equation we have $-\lambda^T$ because the inequality in standard LP is "$\ge$" but the Lagrangian requires "$\le$".

$$ L(x, \lambda, \nu) = -b^T\nu + (c + A^T\nu -\lambda)^T x. $$

$$g(\lambda, \nu) = \inf_x L = \begin{cases}
    -b^T\nu &\quad c + A^T \nu - \lambda = 0 \\
    -\infty &\quad \text{otherwise}
\end{cases}$$

The lower bound property is nontrivial only when $\lambda \succeq 0$ and $A^T\nu - \lambda + c = 0$.

#### Two-way Partition

Consider a nonconvex problem

$$ \begin{align*}
    \min &\quad x^TWx \\
    \mathrm{s.t.} &\quad x_i^2 = 1
\end{align*} $$

Note that the feasible set is $\{1, -1\}^n$, which is discrete and nonconvex.

$$ L(x, \nu) = x^TWx + \sum_{i=1}^n \nu_i^T (x_i^2 - 1) $$

$$ g(\nu) = \inf_x L(x, \nu) = \inf_x \left( x^TWx + \sum_{i=1}^n \nu_i (x_i^2 - 1) \right) $$

We can rewrite $\sum_i \nu_i x_i^2$ as $x^T \mathrm{diag}(\nu)x $,

$$ g(\nu) = \inf_x \left( x^T\left( W + \mathrm{diag}(\nu) \right) x - \sum_{i}\nu_i \right) $$

$$ g(\nu) = \begin{cases}
    -\mathbf{1}^T \nu &\quad W + \mathrm{diag}(\nu) \ge 0\\
    -\infty &\quad \text{otherwise}
\end{cases} $$

### Langrangian Dual Function and Conjugate Functions

!!!note Recap. Conjugate Function
    The conjugate function $f^*$ of a function $f$ is given by

    $$ f^*(y) = \sup_{x \in \mathbf{dom}(f)} (y^Tx - f(x)) $$

Consider

$$ \begin{align*}
    \min &\quad f_0(x) \\
    \mathrm{s.t.} &\quad Ax \le b\\
    &\quad Cx = d
\end{align*} $$

Dual function

$$ g(\lambda, \nu) = \inf_x (f_0(x) + \lambda^T(Ax -b) + \nu^T (Cx - d)) $$

$$\begin{align*}
    g(\lambda, \nu) &= \inf_x \left( f_0(x) + (A^T\lambda + C^T\nu)^Tx \right) - b^T\lambda - d^T\nu\\
    &= \inf_x \left( -y^Tx + f_0(x) \right) - b^T\lambda -d^T \nu \quad \text{let } y = -(A^T\lambda + C^T\nu) \\
    &= -\sup_x \left( y^Tx - f_0(x) \right) - b^T\lambda -d^T \nu\\
    &= -f_0^*(-(A^T\lambda + C^T\nu)) - b^T\lambda - d^T\nu
\end{align*}$$

#### Equality Constrained Norm Minimization

...

#### Entropy Maximization

$$\begin{align*}
    \min &\quad f_0(x) = \sum_{i=1}^n x_i \log x_i \\
    \mathrm{s.t.} &\quad Ax \preceq b\\
    &\quad \mathbf{1}^T x = 1
\end{align*}$$

This problem only contains affine constraints, we can directly use the results derived above.

The conjugate of $f(x) = x \log x$ is $f^*(y) = e^{y-1}$, and therefore

$$ f^*_0(y) = \sum_{i=1}^n e^{y_i - 1} $$

Plug everything in

$$ g(\lambda, \nu) = -b^T\lambda -\nu -e^{-\nu-1}\sum_{i=1}^n e^{-a_i^T\lambda} $$

## Lagrange Dual Problem

The Lagrange dual function gives a lower bound on the optimal value $p^*$ of the original optimization problem. We then consider what is the *best* lower bound that can be obtained.

$$ \begin{align*}
    \max &\quad g(\lambda, \nu) \\
    \mathrm{s.t.} &\quad \nu \ge 0
\end{align*} $$

This is called the **Lagrangian dual problem** associated with the original optimization problem (sometimes called the **primal problem**).

The dual problem is a *convex* problem, because it is a maximization of concave problem, under convex constraints.

### Weak Duality

Denote the optimal value of the Lagrange dual problem by $d^*$. It is the best lower bound of $p^*$.

$$ d^* \le p^* $$

### Strong Duality and Slater's Constraint Qualification
