# Approximation and Fitting

## Norm Approximation

### Norms

A function $f: \mathbb{R}^n \mapsto \mathbb{R}$ is called a **norm** if

- $f$ is non-negative: $f(x) \ge 0$ for all $x \in \mathbb{R}^n$
- $f$ is definite: $f(x) = 0$ iff $x=0$
- $f$ is homogeneous: $f(tx) = |t|f(x)$ for all $x \in \mathbb{R}^n$ and $t \in \mathbb{R}$
- $f$ satisfies the triangle inequality: $f(x+y) \le f(x) + f(y)$ for all $x, y \in \mathbb{R}^n $

#### Lp-Norm

$$ \| v \|_p = \left( \sum_{i=1}^n |v_i|^p \right)^{1/p} $$

##### L2-Norm (Euclidean Distance)

$$ \| v \|_2 = \left( \sum_{i=1}^n v_i^2 \right)^{1/2} $$

##### L1-Norm (Manhattan Distance)

$$ \| v \|_1 = \sum_{i=1}^n |v_i| $$

##### L-$\infty$ Norms

$$ \|v\|_{\infty} = \max_{i} |v_i| $$

##### L0-Norm

$$ \|v\|_0 = \#\text{non-zero elements} $$

- **Note.** $L_0$-norm is not a norm. It is not homogenous.

### Norm Approximation

$$ \min \| Ax - b \| $$

- $A \in \mathbb{R}^{m \times n}$ with $m \ge n$
- $b \in \mathbb{R}^m$
- $\|\cdot\|$ is a norm on $\mathbb{R}^m$

#### Interpretations of Norm Approximation

##### Estimation Interpretation

Consider a linear measurement model

$$ y = Ax + v $$

where $v$ is some measurement error that is unknown, but persumed to be small and follows a certain distribution (typically a Gaussian distribution)

##### Geometric Interpretation

Consider a subspace $\mathcal{A} = \mathcal{R}(A) \subseteq \mathbb{R}^m$ and a point $b \in \mathbb{R}^m$. A **projection** of $b$ onto $\mathcal{A}$ in the norm $\|\cdot\|$ is any point in $\mathcal{A}$ that is closest to $b$.

Parametrize any point in $\mathcal{A}$ by $u =Ax$, solving the norm approximation is equivalent to computing a projection of $b$ onto $A$.

##### Design Interpretation

- $x_1,\dots,x_n$ are design variables.
- $y = Ax$ gives a vector of $m$ results.
- $b$ is a vector of targets or desired results.
- The goal is to choose a vector of design variables that achieves the desired results as closely as possible.

#### Examples

##### Least-Squares Approximation

$$ \min \| Ax - b \|^2 $$

- $f(x) = x^TA^TAx - 2b^TAx +b^Tb$
- $\nabla f(x) = 2A^TAx - 2A^Tb = 0$
- $ x = (A^TA)^{-1} A^Tb $

##### Chebyshev / Minimax Approximation

$$ \min \| Ax - b \|_{\infty} $$

The Chebyshev Approximation can be cast as an LP.

$$ \begin{align*}
    \min \quad & t\\
    \mathrm{s.t.} \quad & -t\bm{1} \preceq Ax - b \preceq t\bm{1}
\end{align*} $$

##### Sum of Absolute Residuals Approximation

$$ \min \| Ax - b \|_1 $$

Like the Chebyshev Approximation, the $L_1$-norm approximation can be cast as an LP.

$$ \begin{align*}
    \min \quad & \bm{1}^T t\\
    \mathrm{s.t.} \quad & -t \preceq Ax - b \preceq t
\end{align*} $$

## Penalty Function Approximation

The **Penalty Function Approximation** is a generalization of the $L_p$-norm approximation, where the objective depends only on the amplitude distribution of the residuals.

$$ \begin{align*}
    \min \quad & \phi(r_1) + \cdots + \phi(r_m) \\
    \mathrm{s.t.} \quad & r = Ax - b
\end{align*} $$

where $\phi: \mathbb{R} \mapsto \mathbb{R}$ is called the **penalty function**. We assume $\phi$ is convex.

#### Common Penalty Functions

- $\phi(u) = \|u\|^p$ with $p \ge 1$. Equivalent to norm approximation.
- **Deadzone-Linear.**
  $$ \phi(u) = \begin{cases}
    0 \quad & |u| \le a \\
    |u| - a \quad & |u| > a
  \end{cases} $$ The deadzone-linear penalty assesses no penalty for residuals smaller than $a$.
- **Log Barrier.** Log barrier with limit $a > 0$
  $$ \phi(u) = \begin{cases}
    -a^2\log(1-(u/a)^2) & \quad |u|<a \\
    \infty & \quad |u| \ge a
  \end{cases} $$
- **Huber Penalty Function.**
  $$ \phi(u) = \begin{cases}
    u^2 &\quad |u| \le M \\
    M(2|u| - M) |u| \ge M &\quad |u| > M
  \end{cases} $$ The Huber penalty is more robust to outliers

## Over-parametrization

Consider cases when $m < n$

### Least-Norm Problems

$$ \begin{align*}
    \min &\quad \| x \| \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

The least-norm problem is only interesting when $m < n$.

## Regularized Approximation

The goal is to find a vector $x$ that is small and also makes $Ax - b$ small. This is naturally described as a convex optimization with two objectives

$$ \min (\mathrm{w.r.t.} \mathbb{R}_{+}^{2}) \quad (\|Ax-b\|, \|x\|) $$

### Regularization

**Regularization** is a common scalarization method used to solve the bi-criterion problem.

One form of regularization is to minimize the weighted sum

$$ \min \quad \| Ax - b \| + \gamma \| x\| $$

where $\gamma > 0$ is a parameter.

#### Tikhonov Regularization

Another common method is to minimize the weighted sum of squared norms

$$ \min \quad \| Ax - b \|^2 + \delta \| x \|^2 $$

This problem has an analytical solution

$$ x = (A^TA + \delta I)^{-1}A^Tb $$

Since $(A^TA + \delta I) \succ 0$ for any $\delta > 0$, the Tikhonov Regularization requires no rank (or dimension) assumptions on $A$.
