# Convex Optimization Problems

## Optimization Problems

### Terminologies

#### Optimization Problem

$$ \begin{align*}
    \min &\quad f_0(x) \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, p \\
    &\quad h_i(x) = 0, \quad i = 1,\dots,p
\end{align*} $$

- $x \in \mathbb{R}^n$ is the **optimization variable**.
- The function $f_0: \mathbb{R}^n \mapsto \mathbb{R}$ is the **objective function** or **cost function**.
- The inequalities $f_i(x) \le 0$ are called the **inequality constraints** and $f_i: \mathbb{R}^n \mapsto \mathbb{R}$ are the **inequality constraint functions**.
- The equalities $h_i(x) = 0$ are called the **equality constraints** and $h_i: \mathbb{R}^n \mapsto \mathbb{R}$ are called the **equality constraint functions**.

#### Optimal Value and Optimal Point

##### Optimal Value

The **optimal value** $p^*$ of an optimization problem is defined as

$$ p^* = \inf \{ f_0(x) | f_i(x) \le 0, h_i(x) = 0 \} $$

- $p^*$ is allowed to take on the extended values $\pm \infty$.
  - If the problem is **infeasible**, $p^* = \infty$.
  - If there are feasible points $x_k$ with $f_0(x_k) \to -\infty$ as $x_k \to -\infty$, the problem is **unbounded below** and $p^* = -\infty$.

##### Feasible Point

A point $x$ is feasible if $x \in \mathbf{dom}f_0$ and it satisfies all constraints $f_i(x) \le 0$ and $h_i(x) = 0$.

##### Optimal Points

$x^*$ is said to be an **optimal point**, or it solves the problem, if $x^*$ is feasible and $f_0(x^*) = p^*$.

The set of all optimal points is the **optimal set**,

$$ X_{opt} = \{ x | f_i(x) \le 0, h_i(x) = 0, f_0(x) =p^* \} $$

A feasible point $x$ is **locally optimal** if there is an $R > 0$ s.t.

$$ f_0(x) = \inf\{ f_0(z) | f_i(z) \le 0, h_i(z) = 0, \|z - x\|_2 \le R \} $$

i.e., $x$ minimizes $f_0$ over nearby points in the feasible set.

#### Feasibility Problem

$$ \begin{align*}
    \min &\quad 0 \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1, \dots, p
\end{align*} $$

The objective function is either $0$ or $\infty$ (if the feasible set is empty). This problem is called a **feasibility problem**. It is sometimes written as

$$ \begin{align*}
    \mathrm{find} &\quad x \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1, \dots, p
\end{align*} $$

## Convex Optimization

A **convex optimization** is one of the form

$$ \begin{align*}
    \min &f_0(x) \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad a_i^T x = b_i, \quad i = 1, \dots, p
\end{align*} $$

where $f_0,\dots,f_m$ are convex functions.

Compared with a standard form optimization problem, a convex optimization problem has three additional constraints

1. The objective function must be convex.
2. The inequality constraint functions must be convex.
3. The equality constraint functions $h_i(x) =a_i^Tx - b_i$ must be affine.

!!!note ""
    **Property.** The feasible set of a convex optimization problem is convex.

    - The feasible set is the intersection of (1) a convex set (domain of $f_0$); (2) $m$ convex sublevel sets ($f_i(x) \le 0$); (3) $p$ hyperplanes ($a_i^Tx = b_i$).

### Local and Global Optima of Convex Functions

!!!success ""
    **Theorem.** Any locally minimal point of a convex optimization problem is also globally minimal.

*Proof.* Prove by contradiction. Assume $x$ is a local minimum, but there exists some $y$ such that $f_0(y) < f_0(x)$.

By definition of local minimum, there exists an $R > 0$ s.t.

$$ f_0(x) = \inf \{ f_0(z) | z \in \mathrm{feasible}, \|z - x\|_2 \le R \} $$

Evidently $\|y - x\|_2 >R$. Now consider a point $z$ given by

$$ z = \theta y + (1-\theta) x $$

where $\theta \le \frac{R}{2\|y-x\|_2}$, then $\|z-x\| \le R$.

$$ f_0(z) = f_0(\theta y + (1-\theta)x) \le \theta f_0(y) + (1-\theta)f_0(x) < \theta f_0(x) + (1-\theta) f_0(x) = f_0(x) $$

Thus we have $f_0(z) < f_0(x)$, which is contradictory to that $x$ is locally minimal.

### First-order Optimality Condition

Suppose the objective function $f_0$ in a convex optimization problem is differentiable. A point $x$ is optimal if and only if it is feasible and

$$ \nabla f_0(x)^T(y-x) \ge 0 \quad \forall y \in X $$

where $X$ denotes the feasible set.

- Geometrically, the criterion states that the angle between the negative gradient and any other vector in the feasible set is no less than $90\degree$. I.e., $-\nabla f_0(x)^T(y-x) \le 0$.
  - Therefore moving along any direction in the feasible set will increase the function value.

#### Unconstrained Problems

For an unconstrained problem, the condition reduces to

$$ \nabla f_0(x) = 0 $$

for $x$ to be optimal.

#### Equality-constrained Problems

For an equality-constrained problem, a feasible point $x$ is optimal if there exists a vector $v$ such that 

$$ \nabla f_0(x) + A^Tv = 0 $$

Together with $Ax = b$, this is the Lagrangian multiplier optimality condition.

#### Inequality-constrained Problems

Consider optimization over the nonnegative orthant.

$$ \begin{align*}
    \min &\quad f_0(x) \\
    \mathrm{s.t.} &\quad x \succeq 0
\end{align*} $$

By the optimality condition,

$$ \nabla f_0(x)^T (y - x) \ge 0$$

The term $\nabla f_0(x)^T y$ is a linear function of $y$, which is unbounded belowe unless $\nabla f_0(x)^T$

### Equivalent Convex Problems

#### Eliminating Equality Constraints

For a convex problem, the equality constraints must be linear ($Ax = b$).
