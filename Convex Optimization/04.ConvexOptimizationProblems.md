# Convex Optimization Problems

## Optimization Problems

### Terminologies

#### Optimization Problem

$$ \begin{align*}
    \min &\quad f_0(x) \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, p \\
    &\quad h_i(x) = 0, \quad i = 1,\dots,p
\end{align*} $$

- $x \in \mathbb{R}^n$ is the **optimization variable**.
- The function $f_0: \mathbb{R}^n \mapsto \mathbb{R}$ is the **objective function** or **cost function**.
- The inequalities $f_i(x) \le 0$ are called the **inequality constraints** and $f_i: \mathbb{R}^n \mapsto \mathbb{R}$ are the **inequality constraint functions**.
- The equalities $h_i(x) = 0$ are called the **equality constraints** and $h_i: \mathbb{R}^n \mapsto \mathbb{R}$ are called the **equality constraint functions**.

#### Optimal Value and Optimal Point

##### Optimal Value

The **optimal value** $p^*$ of an optimization problem is defined as

$$ p^* = \inf \{ f_0(x) | f_i(x) \le 0, h_i(x) = 0 \} $$

- $p^*$ is allowed to take on the extended values $\pm \infty$.
  - If the problem is **infeasible**, $p^* = \infty$.
  - If there are feasible points $x_k$ with $f_0(x_k) \to -\infty$ as $x_k \to -\infty$, the problem is **unbounded below** and $p^* = -\infty$.

##### Feasible Point

A point $x$ is feasible if $x \in \mathbf{dom}f_0$ and it satisfies all constraints $f_i(x) \le 0$ and $h_i(x) = 0$.

##### Optimal Points

$x^*$ is said to be an **optimal point**, or it solves the problem, if $x^*$ is feasible and $f_0(x^*) = p^*$.

The set of all optimal points is the **optimal set**,

$$ X_{opt} = \{ x | f_i(x) \le 0, h_i(x) = 0, f_0(x) =p^* \} $$

A feasible point $x$ is **locally optimal** if there is an $R > 0$ s.t.

$$ f_0(x) = \inf\{ f_0(z) | f_i(z) \le 0, h_i(z) = 0, \|z - x\|_2 \le R \} $$

i.e., $x$ minimizes $f_0$ over nearby points in the feasible set.

#### Feasibility Problem

$$ \begin{align*}
    \min &\quad 0 \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1, \dots, p
\end{align*} $$

The objective function is either $0$ or $\infty$ (if the feasible set is empty). This problem is called a **feasibility problem**. It is sometimes written as

$$ \begin{align*}
    \mathrm{find} &\quad x \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1, \dots, p
\end{align*} $$

## Convex Optimization

A **convex optimization** is one of the form

$$ \begin{align*}
    \min &f_0(x) \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad a_i^T x = b_i, \quad i = 1, \dots, p
\end{align*} $$

where $f_0,\dots,f_m$ are convex functions.

Compared with a standard form optimization problem, a convex optimization problem has three additional constraints

1. The objective function must be convex.
2. The inequality constraint functions must be convex.
3. The equality constraint functions $h_i(x) =a_i^Tx - b_i$ must be affine.

!!!note ""
    **Property.** The feasible set of a convex optimization problem is convex.

    - The feasible set is the intersection of (1) a convex set (domain of $f_0$); (2) $m$ convex sublevel sets ($f_i(x) \le 0$); (3) $p$ hyperplanes ($a_i^Tx = b_i$).

### Local and Global Optima of Convex Functions

!!!success ""
    **Theorem.** Any locally minimal point of a convex optimization problem is also globally minimal.

*Proof.* Prove by contradiction. Assume $x$ is a local minimum, but there exists some $y$ such that $f_0(y) < f_0(x)$.

By definition of local minimum, there exists an $R > 0$ s.t.

$$ f_0(x) = \inf \{ f_0(z) | z \in \mathrm{feasible}, \|z - x\|_2 \le R \} $$

Evidently $\|y - x\|_2 >R$. Now consider a point $z$ given by

$$ z = \theta y + (1-\theta) x $$

where $\theta \le \frac{R}{2\|y-x\|_2}$, then $\|z-x\| \le R$.

$$ f_0(z) = f_0(\theta y + (1-\theta)x) \le \theta f_0(y) + (1-\theta)f_0(x) < \theta f_0(x) + (1-\theta) f_0(x) = f_0(x) $$

Thus we have $f_0(z) < f_0(x)$, which is contradictory to that $x$ is locally minimal.

### First-order Optimality Condition

Suppose the objective function $f_0$ in a convex optimization problem is differentiable. A point $x$ is optimal if and only if it is feasible and

$$ \nabla f_0(x)^T(y-x) \ge 0 \quad \forall y \in X $$

where $X$ denotes the feasible set.

- Geometrically, the criterion states that the angle between the negative gradient and any other vector in the feasible set is no less than $90\degree$. I.e., $-\nabla f_0(x)^T(y-x) \le 0$.
  - Therefore moving along any direction in the feasible set will increase the function value.

#### Unconstrained Problems

For an unconstrained problem, the condition reduces to

$$ \nabla f_0(x) = 0 $$

for $x$ to be optimal.

#### Equality-constrained Problems

For an equality-constrained problem, a feasible point $x$ is optimal if there exists a vector $v$ such that 

$$ \nabla f_0(x) + A^Tv = 0 $$

Together with $Ax = b$, this is the Lagrangian multiplier optimality condition.

#### Inequality-constrained Problems

Consider optimization over the nonnegative orthant.

$$ \begin{align*}
    \min &\quad f_0(x) \\
    \mathrm{s.t.} &\quad x \succeq 0
\end{align*} $$

By the optimality condition,

$$ \nabla f_0(x)^T (y - x) \ge 0$$

The term $\nabla f_0(x)^T y$ is a linear function of $y$, which is unbounded belowe unless $\nabla f_0(x)^T$

### Equivalent Convex Problems

#### Eliminating Equality Constraints

For a convex problem, the equality constraints must be linear ($Ax = b$).

#### Epigraph Problem Form

The epigraph form of the convex optimization problem is

$$ \begin{align*}
    \min &\quad t\\
    \mathrm{s.t.} &\quad f_0(x) - t \le 0\\
    &\quad f_i(x) \le 0\\
    &\quad Ax = b
\end{align*} $$

#### Minimizing over Some Variables

Minimizing a convex function over some of its variables preserves convexity.

## Linear Optimization Problems

If the objective and contraints are all affine, the problem is called a **linear programming (LP)**.

A general LP has the form

$$ \begin{align*}
    \min &\quad c^Tx + d\\
    \mathrm{s.t.} &\quad Gx \preceq h\\
    &\quad Ax = b
\end{align*} $$

- The feasible set of the LP is a polyhedron.
- If an optimal solution exist, the minimizer must be on the vertices.
  - The gradient $c$ is a nonzero constant. The function could always move along the gradient descent direction unless it is on the border of the feasible set.

### Examples

#### Diet Problem (Cost-Limited Adaptive Healthy Eating)

- $m$ different neutrients in quantity at least equal to $b_1,\dots,b_m$.
- $n$ different foods $x_1,\dots,x_n$ to compose a healthy diet.
- Each food $i$ contributes $a_{ij}$ to the neutrient $j$, and has a cost $c_j$.

#### Chebyshev Center of a Polyhedron

The problem is to find the largest Euclidean ball that lies in the polyhedron

$$ \mathcal{P} = \{ x \in \mathbb{R}^n | a_i^T x \le b_i, i = 1,\dots,m \} $$

We represent that ball as

$$ \mathcal{B} = \{ x_c + u | \| u \|_2 \le r \} $$

The center of the optimal ball is known as the **Chebyshev center** of the polyhedron, which is the *deepest point* inside the polyhedron (i.e., farthest from boundaries).

The Chebyshev center can be determined by

$$ \begin{align*}
    \max &\quad r \\
    \mathrm{s.t.} &\quad a_i^Tx +r \|a_i\|_2 \le b_i
\end{align*} $$

## Quadratic Optimization Problems

### Quadratic Programming (LP)

A convex optimization problem is called a **quadratic programming (QP)** if the objective function is quadratic, and the constraint functions are affine. I.e., we optimize a convex quadratic function over a polyhedron.

$$ \begin{align*}
    \min &\quad \frac{1}{2} x^TPx + q^Tx + r \\
    \mathrm{s.t.} &\quad Gx \preceq h \\
    &\quad Ax = b,
\end{align*} $$

where $P \in \mathbb{S}_{+}^n $.

- LP can be viewed as a special case of QP, where $P = 0$.

### Quadratically Constrained Quadratic Programming (QCQP)

$$ \begin{align*}
    \min &\quad \frac{1}{2} x^TP_0x + q_0^Tx + r_0 \\
    \mathrm{s.t.} &\quad \frac{1}{2} x^TP_ix + q_i^Tx + r_i \\
    &\quad Ax = b,
\end{align*} $$

where $P_i \in \mathbb{S}_{+}^n$. In QCQP, we minimize a convex quadratic function over the intersection of some ellipsoids.

- QP can be viewed as a special case of QCQP, where $P_i = 0$.

### Second-Order Cone Programming (SOCP)

The **second-order cone programming (SOCP)** is closely related to QP. It has the form

$$ \begin{align*}
    \min &\quad f^Tx \\
    \mathrm{s.t.} &\quad \| A_i x + b_i \|_2 \le c_i^Tx + d_i \\
    &\quad Fx = g,
\end{align*} $$

### Examples

#### Least Squares

#### Distance between Polyhedra

#### Robust Linear Programming

## Geometric Programming

## Pareto Optimal Points and Values


