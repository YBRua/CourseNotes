# Convex Optimization Problems

## Optimization Problems

### Terminologies

#### Optimization Problem

The **standard form** of an optimization problem is

$$ \begin{align*}
    \min &\quad f_0(x) \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1,\dots,p
\end{align*} $$

where

- $x \in \mathbb{R}^n$ is the **optimization variable**.
- The function $f_0: \mathbb{R}^n \mapsto \mathbb{R}$ is the **objective function** or **cost function**.
- The inequalities $f_i(x) \le 0$ are called the **inequality constraints** and $f_i: \mathbb{R}^n \mapsto \mathbb{R}$ are the **inequality constraint functions**.
- The equalities $h_i(x) = 0$ are called the **equality constraints** and $h_i: \mathbb{R}^n \mapsto \mathbb{R}$ are called the **equality constraint functions**.

Note that for consistency (with CS257/CS2601), in this note we use a set of slightly different notations.

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad g_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1,\dots,p
\end{align*}\tag{OP} $$

#### Domain and Feasibility

##### Domain

The set of points for which the objective and all constraint functions are defined,

$$ \mathcal{D} = \mathbf{dom}(f) \cap \bigcap_{i=0}^m \mathbf{dom}(g_i) \bigcap_{i=1}^p \mathbf{dom}(h_i) $$

is called the **domain** of the optimization problem (OP).

##### Feasibility

A point $x$ is **feasible** if $x \in \mathcal{D}$ and it satisfies all constraints $g_i(x) \le 0$ and $h_i(x) = 0$.

The set of all feasible points is called the **feasible set** or the **constraint set**.

The problem is **feasible** if the feasible set is nonempty.

#### Optimal Value and Optimal Point

##### Optimal Value

The **optimal value** $p^*$ of an optimization problem is defined as

$$ p^* = \inf \{ f(x) | g_i(x) \le 0, h_i(x) = 0 \} $$

- $p^*$ is allowed to take on the extended values $\pm \infty$.
  - If the problem is **infeasible**, $p^* = \infty$.
  - If there are feasible points $x_k$ with $f(x_k) \to -\infty$ as $x_k \to -\infty$, the problem is **unbounded below** and $p^* = -\infty$.

##### (Global) Optimal Points

$x^*$ is said to be an **optimal point**, or it solves the problem (OP), if $x^*$ is feasible and $f(x^*) = p^*$.

The set of all optimal points is the **optimal set**,

$$ X_{opt} = \{ x | f(x) =p^*, g_i(x) \le 0, h_i(x) = 0 \} $$

If there exists an optimal point, we say the optimal value is **attained** or **achieved**, and the problem is **solvable**.

##### $\epsilon$-Suboptimal Points

A feasible point $x_0$ with $f(x_0) \le p^* + \epsilon$ ($\epsilon > 0$) is called **$\epsilon$-suboptimal**.

##### Local Optimal Points

A feasible point $x$ is **locally optimal** if there is an $R > 0$ s.t.

$$ f(x) = \inf\{ f(z) | g_i(z) \le 0, h_i(z) = 0, \|z - x\|_2 \le R \} $$

i.e., $x$ minimizes $f_0$ over nearby points in the feasible set.

#### Feasibility Problem

$$ \begin{align*}
    \min &\quad 0 \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1, \dots, p
\end{align*} $$

The objective function is either $0$ or $\infty$ (if the feasible set is empty). This problem is called a **feasibility problem**. It is sometimes written as

$$ \begin{align*}
    \mathrm{find} &\quad x \\
    \mathrm{s.t.} &\quad f_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad h_i(x) = 0, \quad i = 1, \dots, p
\end{align*} $$

## Convex Optimization

A convex optimization problem has the standard form

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad g_i(x) \le 0, \quad i = 1, \dots, m \\
    &\quad a_i^T x = b_i, \quad i = 1, \dots, p
\end{align*} \tag{CP}$$

where $f, g_1,\dots,g_m$ are convex functions.

Compared with a standard form optimization problem, a convex optimization problem has three additional constraints

1. The objective function must be convex.
2. The inequality constraint functions must be convex.
3. The equality constraint functions $h_i(x) =a_i^Tx - b_i$ must be affine.

!!!note ""
    The feasible set of a convex optimization problem is convex.

    - The feasible set is the intersection of
      1. A convex set (intersection of $\mathbf{dom}(f)$ and $\mathbf{dom}(g_i)$)
      2. $m$ convex sublevel sets ($g_i(x) \le 0$)
      3. $p$ hyperplanes ($a_i^Tx = b_i$).

!!!note ""
    The set of solutions (global minima) $X_{opt}$ is convex.

### Local and Global Optima of Convex Functions

**Proposition.** Any locally minimal point of a convex optimization problem is also globally minimal.

!!!quote ""
    *Proof.* Prove by contradiction. Assume $x$ is a local minimum, but there exists some $y$ such that $f_0(y) < f_0(x)$.

    By definition of local minimum, there exists an $R > 0$ s.t.

    $$ f(x) = \inf \{ f(z) | z \in \mathrm{feasible}, \|z - x\|_2 \le R \} $$

    Evidently $\|y - x\|_2 >R$. Now consider a point $z$ given by

    $$ z = \theta y + (1-\theta) x $$

    where $\theta \le \frac{R}{2\|y-x\|_2}$, then $\|z-x\| \le R$.

    $$ f(z) = f(\theta y + (1-\theta)x) \le \theta f(y) + (1-\theta)f(x) < \theta f(x) + (1-\theta) f(x) = f(x) $$

    Thus we have $f(z) < f(x)$, which is contradictory to that $x$ is locally minimal.

### First-order Optimality Condition

Suppose the objective function $f$ in a convex optimization problem is differentiable. A point $x$ is optimal *if and only if* it is feasible and

$$ \nabla f(x)^T(y-x) \ge 0 \quad \forall y \in X $$

where $X$ denotes the feasible set.

- Geometrically, if $\nabla f(x) \neq 0$, then $-\nabla f(x)$ defines a supporting hyperplane to the feasible set at $x$.
  - The criterion states that the angle between the negative gradient at $x$ and any other vector in the feasible set is no less than $90\degree$. I.e., $-\nabla f_0(x)^T(y-x) \le 0$.
  - Therefore moving along any direction in the feasible set will increase the function value.

!!!quote ""
    *Proof.* $\Leftarrow$ Assume the optimality condition holds. For any $y \in X$, by the first-order condition for convexity,

    $$ f(y) \ge f(x) + \nabla f(x)^T(y-x) $$

    Using the optimality condition, we have

    $$ f(y) \ge f(x) \quad \forall y \in X $$

    Therefore $x$ is an optimal point.

    $\Rightarrow$ Assume $x^*$ is optimal. Since $X$ is convex, for $x \in X, \alpha \in [0, 1]$,

    $$ x^* + \alpha(x - x^*) = \alpha x - \bar{\alpha} x^* \in X $$

    so $d = x - x^*$ is a feasible direction. We thus have

    $$ \nabla f(x^*)^T(x-x^*) = \nabla f(x^*)^Td \ge 0 $$

    where the inequality follows from the first-order necessary condition for constrained optimization problems.

#### Unconstrained Problems

For an unconstrained problem, the condition reduces to (the well-known necessary and sufficient condition)

$$ \nabla f(x) = 0 $$

for $x$ to be optimal.

!!!quote ""
    *Proof.* To see this, since $f$ is differentiable, its domain is open (by differentiability). Take $y = x - t\nabla f(x)$, for small and positive $t$,

    $$ \nabla f(x)^T(y-x) = -t\|\nabla f(x)\|^2 \ge 0 $$

    Therefore $\nabla f(x) = 0$.

#### Equality-constrained Problems

For an equality-constrained problem, a feasible point $x$ is optimal *if and only if* there exists a vector $v$ such that 

$$ \nabla f(x) + A^Tv = 0 $$

Together with $Ax = b$, this is the Lagrangian multiplier optimality condition.

!!!quote ""
    *Proof (sketch).* By optimality condition,

    $$ \nabla f(x)^T(y-x) \ge 0 $$

    for any feasible $y$ satisfying $Ay=b$. Since both $Ax=b$ and $Ay=b$,

    $$ A(y - x) = 0 $$

    Denote $v = y-x \in \mathcal{N}(A)$ (null space of $A$), the optimality condition can be expressed as

    $$ \nabla f(x)^T v \ge 0 \quad \forall v \in \mathcal{N}(A) $$

    Note that if a linear function is nonnegative on a subspace, it must be zero (since both $-v$ and $v$ are nonnegative). Therefore $\nabla f(x)^Tv = 0$ for all $v \in \mathcal{N}(A)$, which implies $\nabla f(x)^T \perp \mathcal{N}(A)$.

    Using the fact that $\mathcal{N}(A)^\perp = \mathcal{R}(A)$ (Range of $A$), $\nabla{f(x)} \in \mathcal{R}(A^T)$, i.e., there exists some $v \in \mathbb{R}^p$ s.t.

    $$ \nabla{f(x)} + A^T v = 0 $$

    Intuitively (Geometrically), when $x$ is a optimal point, $\nabla f(x)$ must  be orthogonal to the hyperplane $Ax = b$ (or otherwise we can still move along the hyperplane to attain a smaller value), and therefore $\nabla f(x)$ can be represented by a linear combination of $[a_1,\dots,a_n] = A$.

#### Inequality-constrained Problems

Consider optimization over the nonnegative orthant.

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad x \succeq 0
\end{align*} $$

By the optimality condition,

$$ \nabla f(x)^T (y - x) \ge 0 \quad \forall x, y \succeq 0$$

The term $\nabla f(x)^T y$ is a linear function of $y$, which is unbounded below on $y \succeq 0$ unless $\nabla f(x)^T \succeq 0$. The condition then reduces to $-\nabla{f(x)}^Tx \ge 0$. Since $\nabla{f(x)} \succeq 0, x \succeq 0$, it follows that

$$\nabla{f(x)}_i x_i = 0$$

which is known as *complementarity*.

### Equivalent Convex Problems

#### Eliminating Equality Constraints

For a convex problem, the equality constraints must be linear $Ax = b$. The equality constraint can be eliminated by finding a particular solution $x_0$ such that $Ax_0 = b$ and a matrix $F$ such that $x = Fz + x_0$.

$$ \begin{align*}
    \min &\quad f(Fz + x_0) \\
    \mathrm{s.t.} &\quad g_i(Fz + x_0) \le 0
\end{align*} $$

#### Epigraph Problem Form

The epigraph form of the convex optimization problem (CP) is

$$ \begin{align*}
    \min &\quad t\\
    \mathrm{s.t.} &\quad f(x) - t \le 0\\
    &\quad g_i(x) \le 0\\
    &\quad Ax = b
\end{align*} $$

A linear objective is *universal* for convex optimization, since any convex optimization problem can be transformed to one with linear objective in this way.

#### Minimizing over Some Variables

Minimizing a convex function over some of its variables preserves convexity.

Assume the following optimization problem is convex in $x_1, x_2$

$$ \begin{align*}
    \min &\quad f(x_1, x_2) \\
    \mathrm{s.t.} &\quad g_i (x_1) \le 0
\end{align*} $$

It is equivalent to

$$ \begin{align*}
    \min_{x_1} &\quad \tilde{f}(x_1) \\
    \mathrm{s.t.} &\quad g_i(x_1) \le 0
\end{align*} $$

where $\tilde{f}(x_1) = \inf_{x_2} f(x_1, x_2)$.

## Linear Optimization Problems

### Linear Programming

If the objective and contraints are all affine, the problem is called a **linear programming (LP)**.

A general LP has the form

$$ \begin{align*}
    \min &\quad c^Tx + d\\
    \mathrm{s.t.} &\quad Gx \preceq h\\
    &\quad Ax = b
\end{align*} \tag{LP}$$

- The constant $d$ in the objective function could be omitted, as it does not affect the optimal solution.
- Maximizing a linear function under linear constraint is also an LP, as linear functions are both convex and concave.
- If an optimal solution exist, the minimizer must be on the boundary of the feasible set.
  - The gradient $c$ is a nonzero constant. The function could always move along the gradient descent direction unless it is on the border of the feasible set.

### Standard and Inequality Forms of OP

#### Standard Form LP

The **standard form** of LP the only inequalities are componentwise nonnegativity constraints $x \succeq 0$.

$$\begin{align*}
    \min &\quad c^Tx \\
    \mathrm{s.t.} &\quad Ax = b \\
    &\quad x \succeq 0
\end{align*} $$

A general LP can be converted into standard form by the following steps

(1) Introduce slack variables $s_i$ for inequalities $Gx \preceq h$.

$$ \begin{align*}
    \min &\quad c^Tx\\
    \mathrm{s.t.} &\quad Gx + s = h\\
    &\quad Ax = b \\
    &\quad s \succeq 0
\end{align*}$$

And (2) Expresss $x$ as two nonnegative variables $x = x^+ - x^-$.

$$ \begin{align*}
    \min &\quad c^Tx^+ - c^Tx^-\\
    \mathrm{s.t.} &\quad Gx^+ - Gx^- + s = h\\
    &\quad Ax^+ - Ax^- = b \\
    &\quad s \succeq 0, x^+ \succeq 0, x^- \succeq 0
\end{align*}$$

which yields a standard form LP with variables $x^+, x^-, s$.

#### Inequality Form LP

If the LP has no equality constraints, it is called an **inequality form** LP

$$ \begin{align*}
    \min &\quad c^Tx \\
    \mathrm{s.t.} &\quad Ax \preceq b
\end{align*} $$

A general LP can be converted into inequality form by (1) variable substitution using the equality constraint, or (2) rewrite the equality constraint as two inequality constraints.

### Examples

#### Diet Problem (Cost-Limited Adaptive Healthy Eating)

- $m$ different neutrients in quantity at least equal to $b_1,\dots,b_m$.
- $n$ different foods $x_1,\dots,x_n$ to compose a healthy diet.
- Each food $i$ contributes $a_{ij}$ to the neutrient $j$, and has a cost $c_j$.

The LP is thus formulated as

$$\begin{align*}
    \min &\quad c^Tx \\
    \mathrm{s.t.} &\quad Ax \succeq b \\
    &\quad x \succeq 0
\end{align*}$$

#### Chebyshev Center of a Polyhedron

The problem is to find the largest Euclidean ball that lies in the polyhedron

$$ \mathcal{P} = \{ x \in \mathbb{R}^n | a_i^T x \le b_i, i = 1,\dots,m \} $$

We represent that ball as

$$ \mathcal{B} = \{ x_c + u | \| u \|_2 \le r \} $$

The center of the optimal ball is known as the **Chebyshev center** of the polyhedron, which is the *deepest point* inside the polyhedron (i.e., farthest from boundaries).

The variables are $x_c$ and $r$, we want to maximize $r$ subject to the constraint $\mathcal{B} \subseteq \mathcal{P}$.

Consider a simpler constraint that $\mathcal{B}$ lies in one halfspace $a_i^Tx \le b_i$,

$$ \| u \|_2 \le r \Longrightarrow a_i^T(x_c + u) \le b_i $$

Since $\sup\{ a_i^T u | \|u\|_2 \le r \} = r\|a\|_2^2$, we can rewrite the constraint as

$$ a_i^Tx_c + r\|a_i\|_2^2 \le b_i $$

which is linear in $x_c$ and $r$.

The Chebyshev center can be determined by

$$ \begin{align*}
    \max_{r, x} &\quad r \\
    \mathrm{s.t.} &\quad a_i^Tx +r \|a_i\|_2 \le b_i
\end{align*} $$

#### Piecewise-Lienar Minimization

Consider the problem of minimizing a piecewise-linear convex function

$$ f(x) = \max_i (a_i^Tx + b_i) $$

The problem can be transformed into the following LP

$$ \begin{align*}
    \min &\quad t \\
    \mathrm{s.t.} &\quad a_i^Tx+b_i \le t \quad i=1,\dots,m
\end{align*} $$

## Quadratic Optimization Problems

### Quadratic Programming (QP)

A convex optimization problem is called a **quadratic programming (QP)** if the objective function is quadratic, and the constraint functions are affine. I.e., we optimize a convex quadratic function over a polyhedron.

$$ \begin{align*}
    \min &\quad \frac{1}{2} x^TPx + q^Tx + r \\
    \mathrm{s.t.} &\quad Gx \preceq h \\
    &\quad Ax = b,
\end{align*} $$

where $P \in \mathbb{S}_{+}^n $.

- LP can be viewed as a special case of QP, where $P = 0$.

### Quadratically Constrained Quadratic Programming (QCQP)

If the objective and the inequality constraints are convex quadratic, the problem is called a **quadratically constrained quadratic programming (QCQP)**.

$$ \begin{align*}
    \min &\quad \frac{1}{2} x^TP_0x + q_0^Tx + r_0 \\
    \mathrm{s.t.} &\quad \frac{1}{2} x^TP_ix + q_i^Tx + r_i \le 0 \\
    &\quad Ax = b,
\end{align*} $$

where $P_i \in \mathbb{S}_{+}^n$. In QCQP, we minimize a convex quadratic function over the intersection of some ellipsoids.

- QP can be viewed as a special case of QCQP, where $P_i = 0$ for the inequality constraints.

### Second-Order Cone Programming (SOCP)

The **second-order cone programming (SOCP)** is closely related to QP. It has the form

$$ \begin{align*}
    \min &\quad f^Tx \\
    \mathrm{s.t.} &\quad \| A_i x + b_i \|_2 \le c_i^Tx + d_i \\
    &\quad Fx = g
\end{align*} $$

The constraint $\| A_i x + b_i \|_2 \le c_i^Tx + d_i$ is called a second-order cone constraint, since it is the same as requiring the affine function $Ax+b, c^Tx + d$ to lie in the second-order cone in $\mathbb{R}^{k+1}$.

- When $c_i=0$, the problem is equivalent to a QCQP.
- When $A_i=0$, the problem reduces to an LP.

### Examples

#### Least Squares

The problem of minimizing the convex quadratic function

$$ \| Ax - b \|_2^2 = x^TA^TAx - 2b^TAx + b^Tb $$

with $A \in \mathbb{R}^{n\times p}, x\in\mathbb{R}^p$ is an unconstrained QP.

By the first-order optimality condition, $x^*$ is optimal iff

$$ A^T(b - Ax) = 0 \Longleftrightarrow A^TAx = A^Tb $$

1. If $A$ has full column rank, $\mathbf{rank}(A) = p$.
   - $A^TA$ is positive definite.
   - A unique solution is given by $(A^TA)^{-1}A^Tb$.
2. $\mathbf{rank}(A) = r < p$.
   - WLOG assume the first $r$ columns are linearly independent, $A = (A_1, A_2)$ with $A_1 \in \mathbb{R}^{n \times r}$ and $\mathbf{rank}(A_1) = r$.
   - There is a solution $x^*$ with the last $p-r$ values being $0$.
   - If $x_1^*$ solves $\min \|b_1 - A_1x_1\|_2^2$, then $x^* = [x_1^*, 0]^T$ solves the original problem.
   - Further, for any $x_0 \neq 0$ s.t. $Ax_0 = 0$, $x^* + x_0$ is also a solution.

#### General Unconstrained QP

$$ \min \frac{1}{2}x^TQx + b^Tx + c $$

where $Q \in \mathbb{R}^{n \times n}$ s.t. $Q \succeq 0$.

By first-order condition, the solution is given by

$$ \nabla{f(x)} = Qx + b = 0 $$

1. $Q \succ 0$. There is a unique solution $x^* = -Q^{-1}b$.
2. $\det(Q) = 0$ and $b$ is in the column space of $Q$ (i.e., $Qx+b=0$ has a solution), there are infinitely many solutions.
3. $\det(Q) = 0$ and $b$ is not in the column space of $Q$, there is no solution, $p^* = \infty$.

#### Distance between Polyhedra

The (Euclidean) distance between two polyhedra $\mathcal{P}_1 = \{ x | A_1 x \preceq b_1 \}$ and $\mathcal{P}_2 = \{ x | A_2 x \preceq b_2 \}$ in $\mathbb{R}^n$ is defined as

$$ \mathbf{dist}(\mathcal{P}_1, \mathcal{P}_2) = \inf\{ \|x_1 - x_2\|_2 | x_1 \in \mathcal{P}_1, x_2 \in \mathcal{P}_2 \} $$

which can be found by solving the QP

$$\begin{align*}
    \min &\quad \|x_1 -x_2\|_2^2 \\
    \mathrm{s.t.} &\quad A_1x_1 \preceq b_1 \\
    &\quad A_2 x_2 \preceq b_2
\end{align*}$$

- The problem is infeasible iff one of the polyhedra is empty.
- The optimal value is 0 iff two polyhedra intersect.

#### Robust Linear Programming

Consider an LP

$$\begin{align*}
    \min &\quad c^Tx \\
    \mathrm{s.t.} &\quad a_i^Tx \le b_i
\end{align*}$$

with noise in the observation of $a_i$ that $a_i$ falls in given ellipsoids

$$ a_i \in \mathcal{E}_i = \{ \bar{a}_i + P_i u | \|u\|_2 \le 1 \} $$

We require the constriants be satisfied for all possible points of $a_i$ within $\mathcal{E}_i$, which gives the **robust linear programming**

$$\begin{align*}
    \min &\quad c^Tx \\
    \mathrm{s.t.} &\quad a_i^Tx \le b_i \quad \forall a_i \in \mathcal{E}_i
\end{align*}$$

The constraint $a_i^Tx \le b_i$ for all $a_i \in \mathcal{E}_i$ can be expressed as

$$ \sup\{ a_i^Tx | a_i \in \mathcal{E}_i \} \le b_i $$

The LHS can be expressed as

$$ \sup\{ a_i^Tx | a_i \in \mathcal{E}_i \} = \bar{a}_i^Tx + \sup\{ u^TP_i^Tx|\|u\|\le 1 \} = \bar{a}_i^Tx + \|P_ix\|_2 $$

Thus

$$ \bar{a}_i^Tx + \|P_i^Tx\| \le b_i \Longrightarrow \|P_i^Tx\|_2 \le -\bar{a}_i^Tx + b_i $$

which is a second-order cone constraint.

##### Probablistic Modeling

Assume $a_i$ follows a Gaussian distribution with mean $\bar{a}_i$ and covariance matrix $\Sigma_i$. We require that each constraint $a_i^T x \le b_i$ be satisfied with a probability over some threshold $\eta \ge 0.5$.

$$ \mathbf{prob}(a_i^Tx \le b_i) \ge \eta $$

Letting $u = a_i^Tx$, with variance $\sigma^2$,

$$ \mathbf{prob}\left( \frac{u-\bar{u}}{\sigma} \le \frac{b_i - \bar{u}}{\sigma} \right) $$

Note that $(u - \bar{u})/\sigma$ has a standard normal distribution, the probability above is simply $\Phi((b_i - \bar{u}) / \sigma)$ where $\Phi()$ is the cumulative distribution function for standard normal distribution. Thus

$$ \frac{b_i - \bar{u}}{\sigma} \ge \Phi^{-1}(\eta) \Longrightarrow \bar{u} + \Phi^{-1}(\eta)\sigma \le b_i $$

Using $\bar{u} = a_i^Tx$ and $\sigma = (x^T\Sigma_ix)^{1/2}$,

$$ \bar{a}_i^Tx + \Phi^{-1}(\eta)\| \Sigma_i^{1/2}x \|_2  \le b_i$$

Since we have assumed $\eta \ge 0.5$, $\Phi^{-1}(\eta) \ge 0$, so this is again a second-order cone constraint.

## Geometric Programming

TBD

## Pareto Optimal Points and Values

TBD
