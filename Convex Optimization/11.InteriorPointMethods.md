# Interior-Point Methods

We consider **interior-point methods** for solving convex optimization problems with inequality constraints.

$$ \begin{align*}
    \min &\quad f(x) \\
    \mathrm{s.t.} &\quad Ax = b\\
    &\quad g_i(x) \le 0
\end{align*} \tag{ICP}$$

Assume $f, g_i$ are twice continuously differentiable, $A \in \mathbb{R}^{p \times n}$ with $\mathrm{rank}(A) = p  < n$. We assume the problem is solvable.

Further assume the problem is strictly feasible, $\exists x \in \mathcal{D}$ s.t. $Ax = b$ and $g_i(x) < 0$. Therefore the Slater's condition holds, and there exists a set of multipliers $\lambda^*, \nu^*$ that satisfies the KKT conditions with $x^*$,

$$ \begin{cases}
    Ax^* = b \\
    g_i(x^*) \le 0\\
    \lambda^*_i \ge 0\\
    \lambda^*_i g_i(x^*) = 0\\
    \nabla \mathcal{L} = \nabla f(x^*) + \sum\lambda_i^* \nabla g_i(x^*) + A^T\nu^* = 0
\end{cases} $$

## Logarithm Barrier Function and Central Path

We aim to approximately formulate the inequality constrained problem (11.1) as an equality constrained problem (so that the Newton's method could be applied).

We can rewrite (ICP) with indicator functions

$$ \begin{align*}
    \min &\quad f(x) + \sum_{i=1}^m \mathbb{I}_{-}(g_i(x)) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

where

$$ \mathbb{I}_{-}(u) = \begin{cases}
    0 &\quad u \le 0\\
    \infty &\quad o.w.
\end{cases} $$

is the indicator function for non-positive reals.

The re-formulated problem has no inequality constraints, but the new objective function is not (in general) differentiable.

### Logarithmic Barrier

The basic idea of the logarithmic barrier is to approximate $\mathbb{I}_{-}(u)$ with a differentiable alternative.

$$ \hat{\mathbb{I}}_{-}(u) = -(1/t) \log(-u), \quad \mathbf{dom}(\hat{\mathbb{I}}_{-}) = -\mathbb{R}_{++} $$

where $t$ is a parameter that controls the accuracy of the approximation.

- $\hat{\mathbb{I}}_-$ is convex and nondecreasing.
  - This implies the scalar composition $\hat{\mathbb{I}}_{-}(f(x))$ is convex if $f$ is convex.
- $ \lim_{t \to \infty} \hat{\mathbb{I}}_{-}(u) = \mathbb{I}(u) $. The function eventually becomes the indicator function as we increase $t$.

With this approximation we formulate the problem as

$$ \begin{align*}
    \min &\quad f(x) + \sum_{i=1}^m -(1/t)\log(-g_i(x)) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

For simplicity, denote

$$ \phi(x) = -\sum_{i=1}^m \log(-g_i(x)) $$

$\phi(x)$ is called the **logarithmic barrier** (or **log barrier**) for the problem (ICP). Its domain is

$$ \mathbf{dom}(\phi) = \{ x \in \mathbb{R}^n | f_i(x) < 0 \} $$

which is the set of points that strictly satisfy the inequality constraints of (ICP).

Thus,

$$ \begin{align*}
    \min &\quad f(x) + \frac{1}{t}\phi(x) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} $$

Take gradient and Hessian of $\phi(x)$ (for future reference),

$$ \nabla \phi(x) = \sum_{i=1}^m \frac{-\nabla g_i(x)}{g_i(x)}, \quad \nabla^2 \phi(x) = -\sum_{i=1}^m \frac{\nabla^2 g_i(x)}{g_i^2(x)} + \sum_{i=1}^{m} \frac{\nabla g_i(x) \nabla g_i(x)^T}{g_i^2(x)} $$

#### Choice of $t$

- As $t$ increases, the approximation becomes more accurate.
- However, when $t$ is large, it is difficult to optimize $f + (1/t)\phi$, as the Hessian varies rapidly near the boudnary of feasible set.
- This can be circumvented by solving a sequence of problems.
  - Increase the value of $t$ at each iteration.
  - Starting Newton's method at the solution of the problem at previous step of $t$.

### Central Path

Again for simplicity, convert the problem to

$$ \begin{align*}
    \min &\quad tf(x) + \phi(x) \\
    \mathrm{s.t.} &\quad Ax = b
\end{align*} \tag{BM}$$

Assume the problem could be solved by Newton's method, and it has a unique solution $x^*(t)$ for each $t > 0$.

The **central path** associated with the optimization problem is defined as the set (curve) of points $x^*(t)$, which are called the **central points**.

## The Barrier Method

---

**Algorithm.** Barrier method (logarithmic barrier)

1. Given a strictly feasible point $x$, $t = t^{(0)} > 0, \mu>1, \epsilon > 0$.
2. Repeat
   1. **Centering Step.** 
      - Compute $x^*(t)$ by minimizing $tf(x) + \phi$ subject to $Ax = b$, starting at $x$.
   2. **Update.** $x = x^*(t)$.
   3. **Stopping Criterion.** quit if $m/t < \epsilon$, where $m$ are the number of inequality constraints.
   4. **Increase $t$.** $t = \mu t$.

---

- Use the $x$ solved at step $k$ as the initial point for step $k+1$

## Additional Notes on the Barrier Method

### Suboptimality of central points

The central point $x(t)$ is (no more than) $m/t$-suboptimal.

$$ f(x^*(t)) - p^* \le \frac{m}{t} $$

By the optimality condition for equality constrained problems, for $x^*(t)$ to be optimal, there exists $\nu^*(t)$ such that

$$ \nabla{}f(x^*(t)) + \frac{1}{t}\nabla{}\phi(x^*(t)) + A^T\nu^*(t) = 0 $$

Using the formula for $\nabla{}\phi(x)$,

$$ \nabla{}f(x^*(t)) + \sum_{j=1}^m \lambda_j^*(t) \nabla{}g_j(x^*(t)) + A^T\nu^*(t) $$

where we define $\lambda_j^*(t) = -\frac{1}{tg_j(x^*(t))}$. Note that $\lambda_j^*(t) > 0$ since $x^*(t)$ is strictly feasible.

Define

$$ \mathcal{L}(x) = L(x, \lambda^*(t), \nu^*(t)) = f(x) + \sum_{j=1}^m \lambda_j^*(t)g_j(x) + (Ax-b)^T\nu^*(t) $$

Then $x^*(t)$ minimizes $\mathcal{L}(x)$ since $\nabla{}\mathcal{L}(x) = 0$. Note that $\mathcal{L}(x)$ is the Lagrangian $L(x,\lambda,\nu)$ at $\lambda^*(t), \nu^*(t)$. This implies $\lambda^*(t)$ and $\nu^*(t)$ are dual feasible. Therefore the dual function $g(\lambda^*(t), \nu^*(t))$ is finite,

$$ p^* \ge g(\lambda^*(t), \nu^*(t)) = \inf_x(L(x, \lambda^*(t), \nu^*(t))) $$

Since $x^*(t)$ is a minimizer for $L(x, \lambda^*(t), \nu^*(t))$,

$$ p^* \ge f(x^*(t)) + \sum_{j=1}^m \underbrace{\lambda_j^*(t)g_j(x^*(t))}_{=1/t} + (\underbrace{Ax^*(t) - b}_{=0,\text{ by feasibility}})^T\nu^*(t) $$

We thus have

$$ p^* \ge f(x^*(t)) + \frac{m}{t} $$

which completes the proof.

- We can thus use $m/t$ as the indicator of error.

#### *Alternative (but essentially the same) Proof from CS257 Slides

Note

$$ \mathcal{L}(x^*(t)) = f(x^*(t)) + \sum_{j=1}^m \underbrace{\lambda_j^*(t)g_j(x^*(t))}_{=1/t} + (\underbrace{Ax^*(t) - b}_{=0,\text{ by feasibility}})^T\lambda^* = f(x^*(t)) - \frac{m}{t} $$

By feasibility of $x^*$,

$$ \mathcal{L}(x^*) = f(x^*) + \sum_{j=1}^m \lambda_j^* \underbrace{g_j(x^*)}_{\le 0} + (\underbrace{Ax^*-b}_{=0})^T\lambda^* \le f(x^*)$$

Therefore

$$ f(x^*(t)) - \frac{m}{t} = \mathcal{L}(x^*(t)) \le \mathcal{L}(x^*) \le f(x^*) = p^* $$

### Interpretation via Modified KKT Conditions

Recall that for $x^*(t)$ to be an optimal point for the barrier problem (BM), there should exist some $\mu^*$ such that

$$ t\nabla{}f(x^*(t)) + \sum_{i=1}^m \frac{-\nabla{}g_i(x)}{g_i(x)} + A^T\mu^*(t) = 0 \tag{1}$$

For the original problem (ICP), by KKT optimality conditions, the optimal $x$ should satisfy

$$ \nabla_x{}L = \nabla{}f(x) + \sum_{i=1}^m \lambda_i \nabla{}g_i(x) + A^T\nu = 0 \tag{2}$$

Comparing (1) and (2), we see that if we define

$$ \lambda_i = -\frac{1}{tg_i(x)}, \qquad \nu = \frac{1}{t}\mu $$

Therefore we derive a set of conditions for the barrier problem (BM)

$$\begin{align*}
    & \nabla f(x) + \sum_{i=1}^m \lambda_i g_i(x) + A^T\nu = 0\\
    & \lambda_i g_i(x) = \underline{-\frac{1}{t}}\\
    & Ax = b\\
    & g_i(x) \le 0 \\
    & \lambda_i \ge 0
\end{align*}$$

- A point $x$ is equal to $x^*(t)$ if and only if there exists some $\lambda, \nu$ that satisfies the conditions above.
- Notice the complementary slackness condition is now $\lambda_i g_i(x) = -1/t$ instead of $=0$.
- For large $t$, $x^*(t)$ *almost* satisfies the KKT optimality conditions for the original problem (ICP).

## Feasibility and Phase I Method

The barrier method requires a strictly feasible point $x^{(0)}$. When such a point is not known in advance, a preliminary stage, called **phase I**, is used to find a strictly feasible point to initialize the barrier method.

$$\begin{align*}
    \mathrm{find} &\quad x\\
    \mathrm{s.t.} &\quad g_i(x) \le 0\\
    &\quad Ax = b
\end{align*}$$

### Feasibility and Basic Phase I

We formulate the feasibility problem as

$$\begin{align*}
    \min_{x,s} &\quad s\\
    \mathrm{s.t.} &\quad g_i(x) \le s\\
    &\quad Ax = b
\end{align*}$$

- This can be solved by barrier method.
  - Finding an initial point for it is easy, by solving for $Ax_0 = b$ and setting $s_0 = \max_{i=1,m} g_i(x_0)$.
  - No need to find the *optimal* $s$. Once we reach some $(x,s)$ with $s < 0$, we can stop and use $x$ to initialize the original problem.
- If there exists $s < 0$, the original problem (ICP) is feasible.
- Otherwise the original problem is infeasible.
