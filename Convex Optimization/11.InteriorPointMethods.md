# Interior-Point Methods

We consider **interior-point methods** for solving convex optimization problems with inequality constraints.

$$ \begin{align*}
    \min &\quad f(x) \\
    \st &\quad Ax = b\\
    &\quad g_i(x) \le 0
\end{align*} $$ (11.1)

Assume $f, g_i$ are twice continuously differentiable, $A \in \mathbb{R}^{p \times n}$ with $\mathrm{rank}(A) = p  < n$. We assume the problem is solvable.

Further assume the problem is strictly feasible, $\exists x \in \mathcal{D}$ s.t. $Ax = b$ and $g_i(x) < 0$. Therefore the Slater's condition holds, and there exists a set of multipliers $\lambda^*, \nu^*$ that satisfies the KKT conditions with $x^*$,

$$ \begin{cases}
    Ax^* = b \\
    g_i(x^*) \le 0\\
    \lambda^*_i \ge 0\\
    \lambda^*_i g_i(x^*) = 0\\
    \nabla \mathcal{L} = \nabla f(x^*) + \sum\lambda_i^* \nabla g_i(x^*) + A^T\nu^* = 0
\end{cases} $$

## Logarithm Barrier Function and Central Path

We aim to approximately formulate the inequality constrained problem (11.1) as an equality constrained problem.

We can rewrite (11.1) with indicator functions

$$ \begin{align*}
    \min &\quad f(x) + \sum_{i=1}^m \mathbb{I}_{-}(g_i(x)) \\
    \st &\quad Ax = b
\end{align*} $$

where

$$ \mathbb{I}_{-}(u) = \begin{cases}
    0 &\quad u \le 0\\
    \infty &\quad o.w.
\end{cases} $$

is the indicator function for non-positive reals.

The re-formulated problem has no inequality constraints, but the new objective function is not (in general) differentiable.

### Logarithmic Barrier

The basic idea of the logarithmic barrier is to approximate $\mathbb{I}_{-}(u)$ with a differentiable alternative.

$$ \hat{\mathbb{I}}_{-}(u) = -(1/t) \log(-u), \quad \mathbf{dom}(\hat{\mathbb{I}}_{-}) = -\mathbb{R}_{++} $$

where $t$ is a parameter that controls the accuracy of the approximation. Larger $t$ means more accurate approximation.

$$ \begin{align*}
    \min &\quad f(x) + \sum_{i=1}^m -(1/t)\log(-g_i(x)) \\
    \st &\quad Ax = b
\end{align*} $$

For simplicity, denote

$$ \phi(x) = -\sum_{i=1}^m \log(-g_i(x)) $$

Thus,

$$ \begin{align*}
    \min &\quad f(x) + \frac{1}{t}\phi(x) \\
    \st &\quad Ax = b
\end{align*} $$

Take gradient and Hessian of $\phi(x)$ (for future reference),

$$ \nabla \phi(x) = \sum_{i=1}^m \frac{-\nabla g_i(x)}{g_i(x)}, \quad \nabla^2 \phi(x) = -\sum_{i=1}^m \frac{\nabla^2 g_i(x)}{g_i^2(x)} + \sum_{i=1}^{m} \frac{\nabla g_i(x) \nabla g_i(x)^T}{g_i^2(x)} $$

#### Choice of $t$

- As $t$ increases, the approximation becomes more accurate.
- However, when $t$ is large, it is difficult to optimize $f + (1/t)\phi$, as the Hessian varies rapidly near the boudnary of feasible set.
- This can be circumvented by solving a sequence of problems.
  - Increase the value of $t$ at each iteration.
  - Starting Newton's method at the solution of the problem at previous step of $t$.

### Central Path

Again for simplicity, convert the problem to

$$ \begin{align*}
    \min &\quad tf(x) + \phi(x) \\
    \st &\quad Ax = b
\end{align*} $$

Assume the problem could be solved by Newton's method, and it has a unique solution for each $t > 0$.

## The Barrier Method

We defer further discussions on the logarithm barrier and first give the procedure of the barrier method.

- Given a strictly feasible point $x$, $t = t^{(0)} > 0, \mu>1, \epsilon > 0$.
- Repeat
  - **Centering Step.**
    - Compute $x^*(t)$ by minimizing $tf(x) + \phi$ subject to $Ax = b$, starting at $x$.
  - **Update.** $x = x^*(t)$.
  - **Stopping Criterion.** quit if $m/t < \epsilon$, where $m$ are the number of inequality constraints.
  - **Increase $t$.** $t = \mu t$.
