# Unconstrained Minimization

## Unconstrained Minimization Problems

Consider an unconstrained optimization problem

$$ \min f(x) $$

where $f: \mathbb{R}^n \mapsto \mathbb{R}$ is convex and twice continuously differentiable. We further assume that the problem is solvable.

Since $f$ is differentiable and convex, a necessary and sufficient condition for $x^*$ to be a minimizer is that

$$ \nabla_x f(x^*) = 0 $$

### Examples

#### Quadratic Minimization

$$ \min \frac{1}{2}x^TPx + q^Tx + r $$

where $P \in \mathbb{S}_{+}^n$, $q \in \mathbb{R}$, $r \in \mathbb{R}$.

$$ \nabla_x f(x) = Px + q $$

- If $P \in \mathbb{S}_{++}^n$, then $\nabla_x f(x) = 0$ has a unique solution $x = -P^{-1}q$.
- If $\det P = 0$, assume eigenvalues $\lambda_{k+1}, \dots, \lambda_n = 0$.
  - If some of $b_{k+1}, \dots, b_n \neq 0$, the problem has no solution.
  - If all of $b_{k+1}, \dots, b_n = 0$, the problem has infinite solutions.

#### Unconstrained Geometric Programming

$$ \min f(x) = \log\left( \sum_{i=1}^m \exp\left( a^T_i x + b_i \right) \right) $$

$$ \nabla f(x) = \frac{\sum_{i=1}^m a_i \exp\left( a^T_i x + b_i \right)}{\sum_{i=1}^m \exp\left( a^T_i x + b_i \right)} $$

## Strong Convexity

A function is **strongly convex** if there exists an $m > 0$ such that

$$ \nabla^2 f(x) \ge mI $$

- **Remarks.** Strong convexity means
  - $(\nabla^2 f(x) - mI) \in \mathbb{S}_{++}^n$
  - The minimum eigenvalue of $\nabla^2 f(x)$ is at least $m$.

### First-Order Condition for Strong Convexity

A differentiable $f$ is $m$-strongly convex *if and only if*

$$ f(y) \ge f(x) + \nabla{f}(x)^T(y-x) + \frac{m}{2}\|x-y\|^2 $$

### Implications of Strong Convexity

#### A Better Lower Bound

For $x, y \in S$, we have

$$ f(y) = f(x) + \nabla f(x)^T (y-x) + \frac{1}{2}(y-x)^T\nabla^2f(z)(y-x) $$

for some $z$ on the line segment $[x,y]$. By the strongly convex assumption, the last term is at least $\frac{m}{2}\|y-x\|^2$, and therefore

$$ f(y) \ge f(x) + \nabla f(x)^T (y-x) + \frac{m}{2}\|y-x\|^2 $$

This establishes a relationship between the input ($y-x$) and output ($f(y) - f(x)$). It gives a better lower bound for $f(y)$ (than using convexity alone).

#### Bounding the Suboptimal Points

We start from the lower bound for $f(y)$

$$ f(y) \ge f(x) + \nabla f(x)^T (y-x) + \frac{m}{2}\|y-x\|^2 $$

The RHS is a convex function w.r.t. $y$, and hence the minimum is achieved at the stationary point

$$ \tilde{y} = x - (1/m)\nabla f(x) $$

Plug this back to the inequality, $f(y) \ge f(x) - \frac{1}{2m}\|\nabla f(x)\|^2$, and therefore

$$ f(x) - f(y) \le \frac{1}{2m}\|\nabla f(x)\|^2 $$

Since $y \in S$ is arbitrary, let $y$ be the minimizer,

$$ f(x) - p^* \le \frac{1}{2m}\|\nabla f(x)\|^2 $$

We thus essentially bound the suboptimality of $x$ by its gradient. If the gradient is small at some point $x$, then the value $f(x)$ is nearly optimal.

### Condition Number

Let $\nabla^2f(x) \in \mathbb{S}_{++}^n$ with eigenvalues $ \lambda_1 \le \cdots \le \lambda_n$.

The **condition number** is defined as

$$ \kappa(\nabla^2 f(x)) = \frac{\lambda_n}{\lambda_1} $$

## *Lipschitz Smoothness

### Lipschitz Continuity

A function $f$ is **Lipschitz continuous** with **Lipschitz constant** $L > 0$, or simply **$L$-Lipschitz** if

$$ \| f(x) - f(y) \| \le L \|x - y \| $$

- Can be defined with any norm, but we assume $L_2$-norm.
- Lipschitz continuity implies uniform continuity.

### $L$-Smoothness

A function is **$L$-Smooth** if it is differentiable and its gradient is $L$-Lipschitz,

$$ \| \nabla f(x) - \nabla f(y) \| \le L\| x - y \| $$

- $L$ upper bounds the change rate of $\nabla{f}$

#### Second-Order Condition for $L$-Smoothness

A twice continuously differentiable $f$ is $L$-smooth *if and only if* for any $x$, $-L I \preceq \nabla^2{f}(x) \preceq LI$, or equivalently, $|\lambda|\le L$ for all eigenvalues $\lambda$ of $\nabla^2{f}$.

#### Quadratic Upper Bound

If $f$ is $L$-smooth, then

$$ f(y) \le f(x) + \nabla{}f(x)^T(y-x) + \frac{L}{2}\| y - x\|^2 $$

## Descent Methods

### General Descent Methods

The descent methods produce a sequence of minimizing sequence $x^{(k)}$, where

$$ x^{(k+1)} = x^{(k)} + t^{(k)} \Delta x^{(k)} $$

where $t^{(k)}$ is the step size and $\Delta x^{(k)}$ is a search direction.

A **descent method** satisfies

$$ f\left( x^{(k+1)} \right)  \le f\left( x^{(k)} \right)$$

except when $x$ is optimal.

For a small enough $\Delta x$, 

$$ f(y) \approx f(x) + \nabla{f}(x)^T(y-x) $$

Therefore for $f(x^{(k+1)}) < f(x^{(k)})$ it should hold that

$$ \nabla f(x^{(k)})^T\Delta x < 0 $$

- The search direction must make an acute angle with the negative gradient.
- Such a direction is called a **descent direction**.
- For convex differentiable $f$, $\Delta x$ is a descent direction *if and only if* $\nabla{f}(x^{(k)})^T \Delta x < 0$.

#### General Descent Algorithm

---

**Algorithm.** Descent Method

1. Choose initial point $x_0 \in \mathbf{dom}(f)$
2. Repeat until convergence
   1. Choose descent direction $\Delta x$
   2. **Line search.** Choose a step size $t$
   3. **Update.** $x = x + t\Delta x$

---

### Line Search

#### Exact Line Search

In **exact line search**, the step size $t$ is chosen to minimize $f$ along the ray $\{ x + t\Delta x | t \ge 0 \}$.

$$ t^* = \arg\min_s f(x + s \Delta x) $$

Can be used if solving the minimization of $t$ is cheap.

#### Backtracking Line Search

- Most line search used in practice are *inexact*.
- The step $t$ is chosen to approximately minimize $f$ along the ray, or just to reduce $f(x)$ "enough".
- A simple and effective method is the **backtracking line search**.

---

**Algorithm.** Backtracking linesearch

1. Given a descent direction $\Delta x$ for $f$ at $x \in \mathbf{dom}(f)$, hyperparameters $\alpha \in (0, 0.5)$, $\beta \in (0, 1)$.
2. Initialize $t=1$.
3. while $f(x + t\Delta x) > f(x) + \alpha t\nabla f(x)^T \Delta x$
   1. $t = \beta t$

---

- Called *backtracking* because it starts with unit step $t = 1$ and reduces $t$ by a factor $\beta$.

For small enough $t$,

$$ f(x + t\Delta x) \approx f(x) + t\nabla{f}(x)^T \Delta x < f(x) + \alpha t \nabla{ f(x)^T\Delta x} $$

so the backtracking will eventually terminate.

## Gradient Descent

A natural choice for the search direction is the negative gradient $\Delta x = -\nabla f(x)$. The resulting algorithm is called **gradient descent**.

---

**Algorithm.** Gradient Descent (with backtracking line search)

1. Given a starting point $x \in \mathbf{dom}(f)$
2. While $\|\nabla f(x)\|_2 > \eta$
   1. $\Delta x = - \nabla f(x)$
   2. (Backtracking line search) $t = t_0$
   3. while $f(x - t\nabla{f}(x)) > f(x) - \alpha t \| \nabla{f}(x) \|_2^2$
      1. $t = \beta t$
   4. $x = x + t \Delta x$

---

- The stopping criterion is usually of the form $\| \nabla f(x) \|_2 \le \eta$, where $\eta$ is a small and positive constant.
- The condition is checked after the first step (gradient computation), rather than after the update.
- Backtracking line search in gradient descent means we want the actual decrement is at least a proportion $\alpha$ of the decrease in the first-order (linear) approximation of $f$.
  - When $t$ is small, the proportion is almost 1.
  - As $t$ becomes larger, the first-order approximation is less exact, and the proportion becomes smaller.

### Convergence Analysis

We assume $f$ is $m$-strongly convex and the condition number is $\kappa = \frac{M}{m}$, i.e.,

$$ mI \le \nabla^2 f(x) \le MI $$

Recall that [$m$-strong convexity gives a better lower bound for $f(\cdot)$](#a-better-lower-bound)

$$ f(y) \ge f(x) + \nabla{f}(x)^T(y-x) + \frac{m}{2}\|x-y\|^2 $$

Using a similar strategy and the upper bound $MI$ for the Hessian, we have

$$ f(y) \le f(x) + \nabla f(x)^T (y-x) + \frac{M}{2}\|y-x\|^2 $$

Minimizing RHS over $y$ yields

$$ f(x) - f(y) \ge \frac{1}{2m}\| \nabla{}f(x) \|^2 $$

$$ f(x) - f(y) \le \frac{1}{2M}\| \nabla f(x) \|^2 $$

Taking $y = x^*$,

$$ \frac{1}{2m}\| \nabla f(x) \|^2 \le f(x) - p^* \le \frac{1}{2M}\| \nabla f(x) \|^2 $$

We can thus bound the optimal gap with $m, M$ and gradient at $x$.

#### Analysis for Exact Line Search

$$ x^+ = x - t \nabla f(x), $$

where

$$ t = \arg\min_s f(x - s \nabla f(x)) $$

Using the upper bound given by $M$,

$$ \begin{align*}
    f(x^+) &= f(x - t \nabla f(x)) \\
    & \le f(x) - t\|\nabla f(x) \|^2 + \frac{Mt^2}{2}\|\nabla f(x) \|^2 \\
    & = f(x) + \|\nabla f(x)\|^2 (\frac{Mt^2}{2} - t)
\end{align*} $$

The RHS is a quadratic function of $t$, and is minimized by $t = 1/M$. Therefore,

$$ f(x^+) \le f(x) - \frac{1}{2M} \| \nabla f(x) \|^2 $$

Substracting $p^*$ on both sides

$$ f(x^+) - p^* \le f(x) - p^* - \frac{1}{2M} \| \nabla f(x) \|^2 $$

Recall that 

$$ f(x) - p^* \le \frac{1}{2m}\|\nabla f(x)\|^2 \Longrightarrow \| \nabla f(x) \|^2 \ge 2m(f(x) - p^*) $$

Therefore

$$ f(x^+) - p^* \le (1 - \frac{m}{M})(f(x) - p^*) $$

Apply this inequality recursively,

$$ f(x^{(k)}) - p^* \le c^k (f(x^{(0)}) - p^*) $$

where $c = 1-m/M < 1$, which implies that $f(x^{(k)})$ eventually converges to $p^*$ as $k \to \infty$.

##### Remarks

- Gradient descent achieves **linear convergence**. Every iteration yields a better solution by a constant $c$
  - The error converges to zero at least as fast as a geometric series, which lies below a line on a log-linear plot of error-iteration number.
  - The number of iterations required to reach $f(x^{(k)}) - p^* \le \epsilon$ is $O(\log\frac{1}{\epsilon})$, for $\epsilon = 10^{-p}, k=O(p)$, i.e., it is linear in the number of significant digits.
- The rate of convergence is related to $m/M$, which could be represented by the inverse of condition number $\kappa$.
  - If $\kappa$ is small, the problem is **well-conditioned**, and gradient descent converges relatively fast.
  - If $\kappa$ is large, the problem is **ill-conditioned**, and gradient descent converges relatively slow.

#### Analysis for Backtracking Line Search

Assume $\alpha = 1/2$ and $\beta \in (0, 1)$.

$$ \begin{align*}
    \tilde{f}(t) = f(x + t \Delta x) &\le f(x) - t\| \nabla f(x) \|^2 + \frac{Mt^2}{2} \| \nabla f(x) \|^2 \\
    &= f(x) + \| \nabla f(x) \|^2 (\frac{Mt^2}{2} - t)
\end{align*} $$

Note that $t \le 1/M$ satisfies the stopping criteria for the back tracking line search. Therefore, we have $t \ge \beta / M$ (or otherwise the line search would have stopped earlier). The back tracking stops either at $t = 1$ or some $t \ge \beta / M$. Therefore

$$ t \ge \min\{ 1, \frac{\beta}{M} \} $$

Since $t \le \frac{1}{M}$, $\frac{Mt}{2} \le \frac{1}{2}$, $\frac{Mt}{2} - 1 \le - \frac{1}{2}$, $(Mt/2 - 1)t \le -t/2$.

$$ \begin{align*}
    f(x + t \Delta x) &\le f(x) - \frac{t}{2} \| \nabla f(x) \|^2 \\
    &\le f(x) - \frac{1}{2} \min \left\{ 1, \frac{\beta}{M} \right\} \|\nabla f(x) \|^2
\end{align*} $$

Denote the $\min$ thingy by $c$. Recall (again) that $\| \nabla f(x) \|^2 \ge 2m(f(x) - p^*)$.

$$ f(x^+) \le f(x) - 2cm(f(x) - p^*) $$

Substract $p^*$ on both sides

$$ f(x^*) - p^* \le (1- 2cm)(f(x) - p^*) $$

Or more generally, if $\alpha \neq 1/2$

$$ f(x^*) - p^* \le (1- \min\{ 2m\alpha, 2\alpha\beta m/M \})(f(x) - p^*) $$

which is also a linear convergence.

## Steepest Descent Method

Consider the first order Taylor expansion

$$ f(x + v) \approx f(x) + \nabla f(x)^Tv $$

Define the **normalized steepest descent direction** (w.r.t. a norm $\|\cdot\|$) as

$$ \Delta x_{nsd} = \arg\min_v\{ \nabla f(x)^Tv | \|v\| \le 1 \} $$

- There can be multiple minimizers (i.e., multiple directions)
- $\Delta x_{nsd}$ is a step of unit norm that gives the largest decrease in the linear approximation of $f$.
- It essentially solves for the negative dual norm of $\nabla{}f(x)$, $\nabla{}f(x)^T\Delta x_{nsd} = -\|\nabla{}f(x)\|_*$

The **(unnormalized) descent direction** is defined by scaling $\Delta x_{nsd}$ with the dual norm

$$ \Delta x_{sd} = \|\nabla{}f(x)\|_* \Delta x_{nsd} $$

Note that for the steepest descent step, we have

$$ \nabla{}f(x)^T \Delta x_{sd} = -\| \nabla{}f(x) \|_*^2 $$

### Steepest Descent Method

The steepest descent method uses the steepest descent direction as the search direction

---

1. Given a starting point $x \in \mathbf{dom}(f)$
2. Repeat until convergence
   1. Compute the steepest descent direction $\Delta_{sd}$
   2. Choose $t$ by some line search
   3. Update $x = x + t \Delta x_{sd}$

---

### Steepest Descent for Euclidean Norm

$$ \nabla f(x)^T v = \|\nabla f(x)\|_2 \cdot \|v\|_2 \cos\theta \ge - \|\nabla f(x)^T \|_2^2 $$

The steepest descent method for Euclidean norm coincides with the gradient descent method.

### Steepest Descent for Quadratic Norm

Consider the quadratic norm

$$ \|z\|_P = (z^TPz)^{1/2} = \| P^{1/2}z \|_2 $$

where $P \in \mathbb{S}_{++}^n$.

The dual norm is given by

$$ (\|z\|_P)_* = \|z\|_{-P} = \|P^{-1/2}z\|_2 $$

So the normalized steepest descent direction is

$$ \Delta x_{nsd} = -(\nabla{}f(x)^T P^{-1} \nabla{}f(x))^{-1/2} P^{-1} \nabla{}f(x) $$

and the steepest descent direction is given by

$$ \Delta x_{sd} = -P^{-1}\nabla{}f(x) $$

#### Interpretation via Change of Coordinates

The steepest descent direction can be interpreted as the gradient search direction, after a change of coordinates.

Define $\bar{u} = P^{1/2}u$, so we have $\|u\|_P = \| \bar{u} \|_2$. Using this change of coordinates, the original problem is equivalent to minimizing $\bar{f}$

$$ \bar{f}(\bar{u}) = f(P^{-1/2}\bar{u}) = f(u) $$

Apply gradient method to $\bar{f}$, at point $\bar{x} = P^{1/2}x$

$$ \Delta\bar{x}_{grad} = -\nabla{}\bar{f}(\bar{x}) = -P^{-1/2}\nabla{}f(P^{-1/2}\bar{x}) = -P^{-1/2}\nabla{}f(x) $$

This gradient search direction corresponds to

$$ \Delta x = P^{-1/2}\Delta\bar{x}_{grad} = -P^{-1}\nabla{}f(x) $$

for the original variable $x$.

- The steepest descent method in quadratic norm $\|\cdot\|_P$ can be thought of as the gradient method applied to the problem, after the change of coordinates $\bar{x} = P^{1/2}x$.
- The steepest descent method converges rapidly when the Hessian (after cooridnate change) has a small condition number.
  - By choosing a proper $P$ that makes the new Hessian $\nabla^2{}\bar{f}(\bar{x}) = P^{-1/2}\nabla^2{}f(x)P^{-1/2}$ to be nearly identity, the algorithm could converge extremely fast.
  - A choice is then $P=\nabla^2{}f(x)$, which leads to one interpretation of the Newton's method.

### Steepest Descent for $L_1$-Norm

The dual norm of $L_1$-norm is the $L_\infty$ norm, and $\Delta x_{nsd}$ is given by

$$ \Delta x_{nsd} = -\mathrm{sign}\left( \frac{\partial f(x)}{\partial x_i} \right) e_i $$

where $e_i$ is the $i$-th standard basis vector, and $i$ is the index of the component with the largest absolute value ($\| \nabla f(x) \|_\infty = |(\nabla{}f(x))_i|$)

The unnormalized direction is

$$ \Delta x_{sd} = - \frac{\partial f(x)}{\partial x_i} e_i $$

- At each iteration, we choose the component of $\nabla{} f(x)$ with the maximum absolute value and then update the corresponding component according to its sign.
- Sometimes called the **coordinate descent** algorithm.

## Newton's Method

### The Newton Step

The **Newton step** for $f$ at point $x$ is given by

$$ \Delta x_{nt} = -\nabla^2 f(x)^{-1} \nabla f(x) $$

Positive definiteness of $\nabla^2{}f$ imples

$$ \nabla{}f(x)^T \Delta x_{nt} = -\nabla{}f(x)^T\nabla^2{}f(x)^{-1}\nabla{}f(x) < 0 $$

unless $\nabla{}f(x) = 0$, so $\Delta x_{nt}$ is a descent direction.

#### Minimizer of Second-order Approximation

The second-order Taylor expansion of $f$ at $x$ is

$$ \hat{f}(x+v) = f(x) + \nabla f(x)^Tv + \frac{1}{2} v^T\nabla^2f(x)v $$

which is a convex quadratic function of $v$. It is minimized when

$$ \nabla f(x) + \nabla^2 f(x) v = 0 \Rightarrow v = -\nabla^2 f(x)^{-1}\nabla f(x) $$

Essentially, we use the second-order Taylor expansion $\hat{f}(x)$ as a local approximation of $f(x)$, and minimizes $\hat{f}(x)$ instead.

#### Steepest Descent Direction in Hessian Norm

The Netwon step is the steepest descent direction at $x$, for the quadratic norm defined by the Hessian $\nabla^2{}f(x)$, i.e.,

$$ \|u\|_{\nabla^2{}f(x)} = (u^T\nabla^2{}f(x)u)^{1/2} $$

when $x$ is near $x^*$, we have $\nabla^2{}f(x) \approx \nabla^2{}f(x^*)$, so we can expect the transformed Hessian to have good condition number (nearly an identity matrix) and thus converges very fast.

#### Solution of Linearized Optimality Condition

The Newton's method is orignally used for solving roots of $g(x) = 0$.

By the first-order Taylor expansion

$$ g(x) \approx \hat{g}(x) = g(x^{(k)}) + \nabla{}g(x^{(k)})(x-x^{(k)}) $$

Use the root of $\hat{g}(x)$ as the next point for approximation

$$ x^{(k+1)} = x^{(k)} - \nabla{}g(x^{(k)})g(x^{(k)}) $$

Recall that the optimality condition of an unconstrained problem is

$$ \nabla{}f(x) = 0 $$

The Newton's method iteratively finds the solution to $\nabla{}f(x) = 0$.

$$ x^{(k+1)} = x^{(k)} - \nabla^2{}f(x^{(k)})^{-1}\nabla{}f(x^{(k)}) $$

#### Affine Invariance of the Newton Step

The Newton step is independent to linear (affine) coordinate changes. Suppose $A$ is a nonsingular square matrix and $\bar{f}(y) = f(Ay)$,

$$ \nabla{}\bar{f}(y) = A^T\nabla{}f(x) \quad \nabla^2{}\bar{}f(y) = A^T\nabla^2{}f(x)A $$

where $x = Ay$. The Newton step for $\bar{f}$ at $y$ is

$$ \Delta y_{nt} = -(A^T\nabla^2{}f(x)A)^{-1}(A^T\nabla{}f(x)) = A^{-1}\Delta x_{nt} $$

The Newton step for $f$ and $\bar{f}$ are related by the same transformation $A$

$$ x + \Delta x_{nt} = A(y + \Delta y_{nt}) $$

### The Newton Decrement

Plug the Newton direction into the update step of the descent method.

$$ f(x+v) = f(x) - \frac{1}{2} \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) $$

The quantity

$$ \lambda(x) = \left( \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) \right)^{1/2} $$

is called the **Netwon decrement** at $x$.

$$ \hat{f}(x + v) = f(x) - \frac{1}{2}\lambda^2(x) $$

$\frac{1}{2}\lambda^2(x)$ could be used as a stopping criterion

### Newton's Method

---

1. Given a starting point $x \in \mathbf{f}$, tolerance $\epsilon > 0$
2. Repeat
   1. Compute Newton step and decrement, $\Delta x_{nt} = -\nabla^2{}f(x)^{-1}\nabla{}f(x), \lambda(x)^2 = \nabla{}f(x)^T\nabla^2{}f(x)^{-1}\nabla{}f(x)$
   2. **Stopping criterion.** Quit if $\lambda^2 / 2 < \epsilon$
   3. **Line search.** Choose $t$ by exact or backtracking line search
   4. Update $x = x + t\Delta x_{nt}$

---

- Also known as the **damped Newton's method**, because we add a line search.
- Original Netwon's method use a fixed step size of $t=1$, but it has strict requirements on the initial point $x_0$.
  - $x_0$ must be close enough to $x^*$, or otherwise the Newton's method might not even converge.
  - Nonetheless, if it converges, it converges extremely fast (in a few steps)

### Convergence Analysis

- For dampled Newton's method, it guarantees global convergence, and the process could be divided into two phases,
  - **Damped Newton phase.** Backtracking line search returns some $t < 1$.
  - **Quadratically convergent phase.** Once we get close enough to the optimal, backtracking line search returns $t=1$. In this phase the Newton's method converges quadratically, and once $t^{(k)} = 1$ for some $k$, the step size will always be $t^{(k')} = 1$ for $k' \ge k$.

For simplicity we only consider the most trivial case, with scalar variables, but similar results extend to more complicated cases.

Assume we are at some point $x$ (which is close enough to the optimal point $x^*$), the actual zero-point of $\nabla{}f(x^*) = 0$ is $x^*$, and $x^+ = x - \nabla^2{}f(x)^{-1} \nabla f(x)$.

Denote $g(x) = \nabla{}f(x)$ Consider the (second-order) Taylor expansion of $f(x^*)$ at $x$,

$$ g(x^*) = 0 = g(x) + g'(x)(x^*-x) + \frac{1}{2}g''(z)(x^*-x)^2 $$

for some $z \in [x^*, x]$. Multiply $g'^{-1}(x)$ on both sides,

$$ g'^{-1}(x) g(x) + x^* - x + \frac{1}{2}g'^{-1}(x)g''(z)(x^*-x)^2 = 0 $$

Rearranging

$$ x - (g'(x))^{-1}g(x) - x^* = \frac{1}{2}(g'(x))^{-1} g''(z)(x - x^*)^2 $$

Notice that $x - (g'(x))^{-1}g(x) = x^+$, and

$$ \epsilon = x^+ - x^* = C(x - x^*)^2 $$

- **Quadratic convergence.** $\xi^{(k+1)} \le {\xi^{(k)}}^2$
- The significant digits *doubles* every iteration
  - E.g., 2 digits at round 1, 4 digits at round 2, 8 digits at round 3
- The number of iterations required to ensure error $\epsilon < 10^{-p}$ is logarithmic in $p$. Very few iterations are required.
