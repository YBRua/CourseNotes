# Statistical Estimation

## Parametric Distribution Estimation

### Maximum Likelihood Estimation

Consider a family of probability distributions on $\mathbb{R}^m$, indexed by a vector $x \in \mathbb{R}^n$, i.e., each $x$ corresponds to a distribution $p_x(\cdot)$.

The function $p_x(y)$ is called the **likelihood function**.

However, it is more convenient to work with the **log-likelihood**,

$$ l(x) = \log p_x(y) $$

To estimate the value of the parameter $x$, based on observing one sample $y$ from the distribution, a widely used method is the **maximum-likelihood estimation (MLE)**.

$$ \hat{x}_{ml} = \arg\max_x p_x(y) = \arg\max_x l(x) $$

#### Linear Measurements with IID Noise

Consider a linear measurement model

$$ y_i = a_i^T x + v_i $$

where $x_i \in \mathbb{R}^n$ is a vector of parameters to be estimated, $y \in \mathbb{R}$ are the measured quantities, and $v_i$ are some iid measurement noise, with density $p$ on $\mathbb{R}$.

The likelihood function is

$$ p_x(y) = \prod_{i=1}^m p(y_i - a_i^Tx) $$

And the log-likelihood is

$$ l(x) = \log p_x(y) = \sum_{i=1}^m \log p(y_i - a_i^Tx) $$

So the goal of MLE is

$$ \max_x \log p_x(y) $$

If the density $p$ is log-concave, then the optimization problem would be convex.

##### Gaussian Noise

If $v_i$ follows a Gaussian distribution with zero mean and variance $\sigma^2$,

$$ p_x(v) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( -\frac{1}{2\sigma^2}v^2 \right) $$

The log-likelihood function is

$$ l(x) = -(m/2)\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\| Ax-y \|_2^2 $$

Therefore maximizing $l(x)$ is equivalent to minimizing

$$ \min_x \|Ax - y\|_2^2 $$

which is the solution of a least-squares problem

##### Laplacian Noise

If $v_i$ follows a Laplacian distribution

$$ p(z) = \frac{1}{2a}\exp\left( -\frac{1}{a}|z| \right), \quad a > 0$$

Maximizing $l(x)$ is equivalent to minimizing

$$ \min_x \| Ax - y \|_1, $$

which is the solution of a $L_1$-norm approximation.

##### Uniform Noise

If $v_i \sim \mathcal{U}(-a, a)$, an ML estimate is any $x$ satisfying

$$ \| Ax - y \|_{\infty} \le a $$

#### Logistic Regression

Consider a random variable $y \in \{ 0, 1 \}$ with

$$ \mathrm{Pr}[y = 1] = p, \quad \mathrm{Pr}[y=0] = (1-p) $$

The **logistic model** has the form

$$ p = \frac{\exp(a^Tx + b)}{1 + \exp(a^Tx + b)}, $$

and we use $p$ to model the probability that the input $x$ has output $y$.

We can re-order the log-likelihood function so that for a first group $x_1,\dots,x_q$, the outcome is $y=1$, and for $x_{q+1},\dots,x_m$, the outcome is $y=0$.

$$ p(a, b) = \prod_{i=1}^q p_i \cdot \prod_{i=q+1}^m (1 - p_i). $$

The above equation can be rewritten as

$$ p(a, b) = \prod_{i=1}^m p_i^{y_i} (1-p_i)^{1-y_i} $$

The log-likelihood function is then

$$ l(a, b) = \sum_{i=1}^m \mathbb{I}[y_i = 1] p_i + \mathbb{I}[y_i = 0] (1-p_i) $$

!!!note ""
    The log-likelihood function for a logistic regression is concave. So the MLE problem can be converted into a convex optimization problem.

### Maximum a Posteriori Probability Estimation

The **Maximum a posteriori (MAP)** estimation can be viewed as a Bayesian version of maximum likelihood estimation.

!!!note Recap. The Bayes Theorem
    $$ p(y|x) = \frac{p(x, y)}{p(x)} = \frac{p(x|y)p(y)}{p(x)} $$

We assume the parameter $x$ and the observation $y$ are random variables with a joint probability $p(x,y)$. This is in contrast to the statistical estimation setup, where $x$ is a *parameter*, not a *random variable*.

In the MAP setup, we have $p(y|x)$, and we want to maximize $p(x|y)$. By the Bayes Theorem,

$$ \log p(y|x) = \log p(y|x) + \log p(x) - \log p(y) $$

Maximizing $\log p(y|x)$ is equivalent to

$$ \max_x \log p(x| y) \sim \max_x \log p(y|x) + \log p(x) $$

Ignoring the philosophical differences between MAP and MLE, the only difference between MAP and MLE is an additional prior term $p(x)$.

## Non-parametric Distribution Estimation

Consider a random variable $X$ taking values from a finite set $\{ a_1,\dots,a_n \}$. The distribution $X$ is characterized by $p \in \mathbb{R}^n$.

Note that for each $p \in \mathbb{R}^n$ such that $p_i \ge 0, \sum_i p_i = 1$, it defines a probability distribution on $\mathbb{R}^n$ for $X$.

We aim to estimate the distribution of $p$, i.e., the *distribution of a distribution*.
