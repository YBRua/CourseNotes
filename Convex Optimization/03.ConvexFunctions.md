# Convex Functions

## Convex Functions

### Convex Functions

A function $f: \mathbb{R}^n \mapsto \mathbb{R}$ is a **convex function** if $\mathbf{dom}f$ is convex and $\forall x, y \in \mathbf{dom} f, \forall \theta \in [0, 1]$, we have

$$ f( \theta x + (1-\theta)y ) \le \theta f(x) + (1-\theta) f(y) $$

- Geometrically, it means the line segment between $(x, f(x))$ and $(y, f(y))$ lies *above* the graph of $f$.
- A function $f$ is **strictly convex** if the inequality strictly holds for $x \neq y$ and $0 < \theta < 1$.
- A function $f$ is **concave** if $-f$ is convex, and is **strictly concave** if $-f$ is strictly convex.

!!!note A function $f$ is convex iff it is convex when restricted to any line that intersects its domain.
    $f$ is convex if and only if $\forall x \in \mathbf{dom}f$ and $\forall v$, the function $g(t) = f(x + tv)$ is convex (on its domain $\mathbf{dom}(g) = \{ t | x + tv \in \mathbf{dom}(f) \}$).

    *Proof.* $\Rightarrow$. Assume $f$ is convex. Fix an arbitrary $x \in \mathbf{dom}(f)$ and direction $d$. Need to show $g(t) = f(x + td)$ is convex.

    Let $t_1, t_2 \in \mathbf{dom}(g), \theta \in [0, 1]$. Let $x_i = x + t_i d, \bar{t} = \theta t_1 + \bar{\theta}t_2$ and $\bar{x} = x + \bar{t}d$.

    1. Note $\bar{x} = x + (\theta t_1 + \bar{\theta} t_2)d = \theta x_1 + \bar{\theta} x_2$.
    2. $t_i \in \mathbf{dom}(g) \Longrightarrow x_i \in \mathbf{dom}(f)$.
    3. $\mathbf{dom}(f)$ is convex $\Longrightarrow \bar{x} \in \mathbf{dom}(f) \Longrightarrow \bar{t} \in \mathbf{dom}(g) \Longrightarrow \mathbf{dom}(g)$ is covnex.
    4. Since $f$ is convex,
       $$ g(\bar{t}) = f(\bar{x}) \le \theta f(x_1) + \bar{\theta} f(x_2) = \theta g(t_1) + \bar{\theta} g(t_2) $$
       So $g$ is convex.

    $\Leftarrow$. Assume $g(t)$ is convex for any $x \in \mathbf{dom}(f)$ and any direction $d$. Need to show $f$ is convex.

    Fix $x,y \in \mathbf{dom}(f), \theta \in [0, 1]$. Let $d = x - y, g(t) = f(y + td)$.

    1. $x, y \in \mathbf{dom}(f) \Longrightarrow 0, 1 \in \mathbf{dom}(g)$.
    2. $\mathbf{dom}(g)$ is convex $\Longrightarrow \theta \in \mathbf{dom}(g) \Longrightarrow x + \theta d \in \mathbf{dom}(f)$.
    3. $\theta x + \bar{\theta}y = y + \theta d, \theta x + \bar{\theta}y \in \mathbf{dom}(f) \Longrightarrow \mathbf{dom}(f)$ is convex.
    4. Since $g$ is convex and $\theta = \theta 1 + \bar{\theta} 0$,
       $$ f(\theta x + \bar{\theta}y) = g(\theta) \le \theta g(1) + \bar{\theta} g(0) = \theta f(x) + \bar{\theta} f(y) $$
       so $f$ is convex.

### Extended-value Extensions

> *"Padding"*

If $f$ is convex, its **exteded-value extension** $\tilde{f}: \mathbb{R}^n \mapsto \mathbb{R} \cup \{ \infty \}$ is defined by

$$ \tilde{f}(x) = \begin{cases}
    f(x) &\quad x \in \mathbf{dom}f \\
    \infty &\quad x \notin \mathbf{dom}f
\end{cases} $$

This essentially assumes a convex function takes $\infty$ outside of its domain, and can thus simplify notations.

### Examples

- $f(x) = ax + b$
- $f(x) = e^ax$
- $f(x) = x^\alpha$ with $\mathbf{dom} f = \mathbb{R}_{++}$, $\alpha \ge 1$ or $\alpha \le 0$.
- $f(x) = \|x\|_p$
- $f(x) = x\log(x)$ with $\mathbf{dom} f = \mathbb{R}_{++}$

## First- and Second-order Conditions for Convexity

### First-order Conditions

#### Differentiability

##### First-order (gradient)

A function $f$ is differentiable if $\mathbf{dom}f$ is open and

$$ \nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n} \right) $$

exists for each $x \in \mathbf{dom}f$.

#### First-order Condition for Convexity

> 函数不（不一定）单调，但是导数单调。

**First-order condition for convexity.** Suppose $f$ is differentiable and $\mathbf{dom}f$ is convex and open. Then $f$ is convex *iff* $\mathbf{dom}(f)$ is convex and 

$$ f(y) \ge f(x) + \nabla f(x)^T(y-x), \quad \forall x,y \in \mathbf{dom}(f) $$

!!!note ""
    **Note.** Geometrically, the tagent line (First-order Taylor approximation) is a global underestimator of $f$. That is, the line is always below $f$.

!!!quote ""
    *Proof.* $\Rightarrow$ Assume $f$ is convex. Let $d = y - x$. Let $d = y - x, t \in (0, 1)$. By convexity,

    $$ f(x + t(y - x)) = f(x + td) = f(ty + \bar{t}x) \le tf(x) + \bar{t}f(y) $$

    Rearranging,

    $$ \frac{f(x + td) - f(x)}{t} \le f(y) - f(x) $$

    Letting $t \to 0$,

    $$ \nabla f(x)^T (y-x) = \nabla f(x)^Td \le f(y) - f(x) $$

    $\Leftarrow$ Assume the first-order condition holds. Let $z = \theta x + \bar{\theta}y$, the first-order condition implies

    $$ f(x) \ge f(z) + \nabla f(z)^T (x-z) \tag{1}$$

    $$ f(y) \ge f(z) + \nabla f(z)^T (y-z) \tag{2}$$

    $\theta(1) + \bar{\theta}(2)$ yields

    $$ \theta f(x) + \bar{\theta}f(y) \ge f(z) = f(\theta x + \bar{\theta}y) $$

#### First-order Condition for Strict Convexity

**First-order condition for strict convexity.** Suppose $f$ is differentiable and $\mathbf{dom}(f)$ is convex and open. $f$ is strictly convex iff

$$ f(y) > f(x) + \nabla f(x)^T (y-x) \quad \forall x \neq y \in \mathbf{dom}(f) $$

#### Optimality of Stationary Points

If $\nabla f(x^*) = 0$ for a convex function $f$, then $x^*$ is a global minimum. If $f$ is strictly convex, then $x^*$ is the unique global minimum.

!!!quote ""
    *Proof.* For any $x \in \mathbf{dom}(f)$, by the first-order condition,

    $$ \nabla f(x) \ge f(x^*) + \nabla f(x^*)^T (x^*-x) $$

    Note that $\nabla f(x^*)^T = 0$, therefore $x^*$ is a global minimum.

    If $f$ is strictly convex, for any $x \neq x^* \in \mathbf{dom}(f)$, a similar proof would show that $x^*$ is unique.

    Additionally, for concave functions, similar results hold with all inequalities reversed, and minimum replaced by maximum.

### Second-order Conditions

#### Second-order (Hessian)

$$ \nabla^2 f = \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2}
\end{bmatrix} $$

!!!note ""
    **Note.** $\frac{\partial^2 f}{\partial x_1 \partial x_2}$ 不总是等于 $ \frac{\partial^2 f}{\partial x_2 \partial x_1}$，但是一般来说是可以当他们相等，算下来不对再说。

#### Second-order Condition for Convexity

**Second-order condition for convexity.** Suppose $f$ is twice differentiable, $\mathbf{dom}f$ is open and Hessian $\nabla^2 f(x)$ exists at each point in $\mathbf{dom}f$. Then $f$ is convex *iff* $\mathbf{dom}f$ is convex and *its Hessian is positive semidefinite* at every point $x \in \mathbf{dom}(f)$.

$$ \forall x \in \mathbf{dom}f, \nabla^2 f(x) \succeq 0. $$

!!!quote ""
    *Proof.* $\Rightarrow$ Assume $f$ is convex, fix $x \in \mathbf{dom}(f)$ and $d \in \mathbb{R}^n$.

    Since $\mathbf{dom}(f)$ is open, $x + td \in \mathbf{dom}(f)$ for a small $t$. By the second-order Taylor expansion with Peano remainder,

    $$ f(x + td) = f(x) + t\nabla f(x)^Td + \frac{1}{2}t^2d^T\nabla^2 f(x)d + o(t^2) $$

    By the first-order condition for convexity,

    $$ f(x + td) \ge f(x) + t\nabla f(x)^T d \Longrightarrow \frac{1}{2} d^T\nabla^2f(x)d + o(t^2) > 0 $$

    Setting $t \to 0$, $d^T\nabla^2 f(x)d \ge 0$ and therefore $\nabla^2 f(x) \succeq 0$.

    $\Leftarrow$ Assume the second-order condition holds. Fix $x, y \in \mathbf{dom}(f)$ and let $d = y-x$.

    By the second-order Taylor expansion with Lagrange remainder,

    $$ f(y) = f(x) + \nabla f(x)^Td + \frac{1}{2}d^T \nabla^2f(x + sd)d, \quad s \in (0, 1) $$

    Since the domain is convex, $x + sd = sy + \bar{s}(x) \in \mathbf{dom}(f)$.

    By the second-order condition,

    $$ \nabla^2 f(x+sd) \succeq 0 \Longrightarrow f(y) \ge f(x) + \nabla f(x)^Td $$

    which is the first-order condition for convexity.

#### Second-order Condition for Strict Convexity

A twice differentiable $f$ with an open convex domain $\mathbf{dom}(f)$ is strictly convex *if* $\nabla^2 f(x)$ is positive definite at every $x\in \mathbf{dom}(f)$.

- **Note.** Sufficient but not necessary.
  - E.g., $f(x) = x^4$ is strictly convex, but $f''(x) = 0$ at $x = 0$.

## Additional Examples and Properties

### Examples for Convex Functions

#### Scalar Functions

- **Exponential.** $f(x) = e^{ax}$ is convex for all $a \in \mathbb{R}$.
  - $f''(x) = a^2 e^{ax} \ge 0$
- **Power.** $f(x) = x^a$ is convex on $\mathbb{R}_{++}$ when $a\ge 1$ or $a \le 0$ and concave for $0 \le a \le 1$.
- **Powers of absolute value.** $f(x) = |x|^p$ where $p \ge 1$ is convex on $\mathbb{R}$.
  - $f''(x) = a(a-1)x^{a-2}$, value depends on $a$.
- **Logarithm.** $f(x) = \log x$ is (strictly) concave on $\mathbb{R}_{++}$.
  - $f''(x) = -x^{-2} < 0$
- **Negative entropy.** $f(x) = x\log x$ is convex on $\mathbb{R}_{+}$ or $\mathbb{R}_{++}$.
  - $f'(x) = \log x + 1, f''(x) = x^{-1} > 0$

#### Multivariate Functions

- **Quadratic function.** $f(x) = \frac{1}{2}x^TQx + b^Tx + c$ with symmetric $Q$ is convex iff $Q \succeq 0$.
- **Norm.** Every norm on $\mathbb{R}^n$ is convex.
- **Max function.** $f(x) = \max \{ x_1,\dots,x_n \}$ is convex on $\mathbb{R}^n$.
- **Quadratic-over-linear.** The function $f(x, y) = x^2/y$ with domain $\mathbb{R}\times\mathbb{R}_{++}$ is convex.
- **Log-sum-exp.** The function $f(x) = \log \sum_i \exp(x_i)$ is convex on $\mathbb{R}^n$.

!!!quote ""
    *Proof of log-sum-exp.*
    $$ \frac{\partial f}{\partial x_k} = \frac{\exp(x_k)}{\sum_i \exp(x_i)} = \mathrm{Softmax}(x_k) $$

    Denote $z_k = \exp(x_k)$.

    $$ \frac{\partial^2 f}{\partial x_k^2} = \frac{z_k \sum_j z_j - z_k^2}{(\sum_j z_j)^2} $$

    $$ \frac{\partial^2 f}{\partial x_i \partial x_j} = - \frac{z_i z_j}{(\sum_k z_k)^2} $$

    Therefore,

    $$ \nabla^2 f(x) = \frac{\mathrm{diag}(\bm{z})}{\bm{1}^T \bm{z}} - \frac{1}{(\bm{1}^T\bm{z})^2}\bm{z}\bm{z}^T $$

    Prove $\nabla^2f(x)$ is positive semidefinite by definition. $\forall v \in \mathbb{R}^n$, we show $v^T\nabla^2f(x)v \ge 0$.

    $$ v^T \mathrm{diag}(z) v = \sum_i z_i v_i^2 $$

    $$ v^T \nabla^2f(x) v = \frac{(\sum_i z_i)(\sum_jv_i^2z_j) - (\sum_i z_iv_i)^2}{(\bm{1}^T z)^2} $$

    which follows from the Cauchy-Schwarz inequality $(a^Ta)(b^Tb) \ge (a^Tb)^2$ applied to vector components $a_i = v_i\sqrt{z_i}, b_i = \sqrt{z_i}$ 

### Sublevel Sets (下水平集)

The **$\alpha$-sublevel set** of a function $f: \mathbb{R}^n \mapsto \mathbb{R}$ is defined as

$$ C_{\alpha} = \{ x \in \mathbf{dom}f | f(x) \le \alpha \} $$

- The sublevel sets of a convex function are convex, for any $\alpha$.
- The converse is not true. A function can have all its sublevel sets convex, but it is not necessarily convex.
  - E.g., $f(x) = -e^x$.
- The sublevel set can be a good way of establishing convexity of a set.

Conversely, if $f$ is concave, then its $\alpha$-superlevel set

$$ \{ x \in \mathbf{dom}(f) | f(x) \ge \alpha \} $$

is a convex set.

### Epigraph (上图)

The **epigraph** of a function $f: \mathbb{R}^n \mapsto \mathbb{R}$ is defined as

$$ \mathbf{epi}(f) = \{ (x, t) | x \in \mathbf{dom}(f), f(x) \le t \} $$

- A function is convex if and only if its epigraph is a convex set.
- A function is concave if and only if its **hypograph** $\{ (x, t) | x \in \mathbf{dom}(f), f(x) \ge t \}$ is a convex set.

### Jensen's Inequality and Extensions

#### Jensen's Inequality

If $f$ is convex, then

$$ f(\theta x + (1 - \theta)y) \le \theta f(x) + (1 - \theta) f(y) $$

is sometimes called the **Jensen's inequality**.

#### Extensions

The Jensen's inequality can be extended to multiple variables.

If $f$ is convex, $x_1,\dots,x_k \in \mathbf{dom}f$, $\theta_1,\dots,\theta_k \in [0, 1]$ with $\sum\theta_i = 1$, then

$$ f(\theta_1 x_1, \dots, \theta_k x_k) \le \theta_1 f(x_1) + \cdots + \theta_k f(x_k) $$

The inequality also extends to infinite sums, integrals and expectations.

$$ f\left( \int_S p(x)x \mathrm{d}x \right) \le \int_S f(x)p(x)\mathrm{d}x $$

$$ f(\mathbb{E}x) = \mathbb{E}f(x) $$

#### Examples

The **arithmetic-geometric mean inequality**

$$ \sqrt{ab} \le (a + b ) / 2 $$

with $a, b > 0$ can be proved using the fact that $f(x) = -\log(x)$ is convex. Jensen's inequality with $\theta = 1/2$ yields

$$ -\log\left( \frac{a+b}{2} \right) \le \frac{1}{2}(-\log{a} - \log{b}) $$

Taking the exponential on both sides proves the inequality.

## Operations that Preserve Convexity

### Non-Negative Weighted Sums

- If a function $f$ is convex, then $\alpha f(x)$ for $\alpha \ge 0$ is covex.
- If both $f_1, f_2$ are convex, then $f = f_1 + f_2$ is convex.
- If $f_1,\dots,f_k$ are convex, $w_1,\dots,w_k \ge 0$, then $f = \sum w_if_i$ is convex.
- If $f(x,y)$ is convex in $x$ for each $y \in \mathcal{A}$ and $w(y) \ge 0$ for each $y \in \mathcal{A}$, then $g(x) = \int_{\mathcal{A}} w(y) f(x,y)\mathrm{d}y$ is convex.

Can be seen in terms of the associated epigraphs. If $w \ge 0$ and $f$ is convex,

$$ \mathbf{epi}(wf) = \begin{bmatrix} I & 0 \\ 0 & w \end{bmatrix} \mathbf{epi}(f)$$

which is convex, because the image of a convex set under a linear mapping is convex.

### Composition with an Affine Mapping

Suppose $f: \mathbb{R}^n \mapsto \mathbb{R}, A \in \mathbb{n \times m}$, define $g : \mathbb{R}^m \mapsto \mathbb{R}$ by $g(x) = f(Ax + b)$, with $\mathbf{dom}(g) = \{ x | Ax + b \in \mathbf{dom}(f) \}$

- If $f$ is convex, so is $g$;
- If $f$ is concave, so is $g$.

### Pointwise Maximum and Supremem

#### Pointwise Maximum

If $f_1, \dots, f_m$ are convex, then their pointwise maximum $f = \max\{ f_1(x), \dots, f_m(x) \}$ is also convex.

!!!quote ""
    *Proof.* We prove the case when $m=2$, by definition.

    $$ \begin{align*}
      f(\theta x  + \bar{\theta}y) &= \max\{ f_1(\theta x  + \bar{\theta}y), f_2(\theta x  + \bar{\theta}y) \}\\
      &\le \max\{ \theta f_1(x) + \bar{\theta}f_1(y), \theta f_2(x) + \bar{\theta}f_2(y) \} \\
      &\le \theta\max\{ f_1(x), f_2(x) \} + \bar{\theta}\max\{ f_1(y), f_2(y) \} \\
      &= \theta f(x) + \bar{\theta}f(y)
    \end{align*} $$

!!!example Sum of $r$ largest elements
    For $x \in \mathbb{R}^n$ we denote by $x_{[i]}$ the $i$-th largest element in $x$, sorted in nonincreasing order. Then the function

    $$ f(x) = \sum_{i=1}^r x_{[i]} $$

    i.e., the sum of the $r$ largest elements of $x$, is convex. This can be seen by writting $f(x)$ as

    $$ f(x) = \sum_{i=1}^r x_{[i]} = \max\{ x_{i_1} + \cdots + x_{i_r} | 1 \le i_i < \dots < i_r \le n \} $$

    which is the maximum of all possible sums of $r$ different elements of $x$. Since $f(x)$ is the pointwise maximum of $n!/(r!(n-r)!)$ linear functions, it is convex.

#### Pointwise Supremum

The pointwise maximum can be extended to the pointwise extremum over an infinite set of convex functions.

If for each $y$, $f(x,y)$ is convex in $x$, then $g$, defined as

$$g(x) =\sup_{y\in\mathcal{A}} f(x,y)$$

is convex in $x$. Here the domain of $g$ is

$$ \mathbf{dom}(g) = \{ x | \forall y \in \mathcal{A}, (x, y) \in \mathbf{dom}(f), \sup_{y\in\mathcal{A}} f(x,y) < \infty \} $$

In terms of epigraphs, the pointwise supremum of functions corresponds to the intersection of epigraphs

$$ \mathbf{epi}(g) = \bigcap_{y\in\mathcal{A}} \mathbf{epi}f(\cdot,y) $$

The result thus follows from the fact that the intersection of a family of convex sets is convex.

!!!example Support function of a set
    Let $C \subseteq \mathbb{R}^n$, with $C \neq \emptyset$. The **support function** $S_C$ associated with $C$ is defined as

    $$ S_C(x) = \sup\{ x^Ty | y \in C \} $$

    with $\mathbf{dom}(S_C) = \{ x | \sup_{y\in C}x^Ty < \infty \}$.

    For each $y \in C$, x^Ty is linear in $x$, so $S_C$ is the pointwise supremum of a family of linear functions, hence convex.

!!!example Distance to farthest point of a set
    Let $C \subseteq \mathbb{R}^n$. The distance (in any norm) to the farthest point in $C$,

    $$ f(x) = \sup_{y \in C}\| x - y \| $$

    is convex. For any $y$, $\| x - y\|$ is convex. $f$ is thus a pointwise supremum of a family of convex functions.

!!!example Maximum eigenvalue of a symmetric matrix
    The function

    $$ f(X) = \lambda_{max}(X) \quad \mathbf{dom}(f) = S^{m} $$

    is convex. We can express $f(X)$ as

    $$ f(X) = \sup\{ y^TXy | \|y\|_2 = 1 \} $$

    It is a general way of solving for the maximum eigenvalue (Rayleigh Quotient). It is the supremum of a set of linear functions of $X$.

!!!note ""
    The results can be extended to concavity, with maximum and supremum replaced by minimum and infimum.

### Minimization

If $f(x, y)$ is convex in $(x, y)$ and $C$ is convex (and nonempty), then

$$g(x) = \inf_{y \in C} f(x,y) $$

with $\mathbf{dom}(g) = \{ x | \exists y \in C, (x,y) \in \mathbf{dom}(f) \}$ is convex.

!!!quote ""
    Can be proved by definition. For $x_1,x_2 \in \mathbf{dom}(g)$, for any $\epsilon>0$, there are $y_1, y_2 \in C$ such that $ f(x_i, y_i) \le g(x_i) + \epsilon $ for $i=1,2$. Now let $\theta \in [0,1]$,

    $$ \begin{align*}
      g(\theta x_1 + \bar{\theta} x_2) &= \inf_{y \in C}f(\theta x_1 + \bar{\theta} x_2, y) \\
      &\le f(\theta x_1 + \bar{\theta}x_2, \theta y_1 + \bar{\theta} y_2)\\
      &\le \theta f(x_1, y_1) + \bar{\theta}f(x_2, y_2)\\
      &\le \theta g(x_1) + \bar{\theta} g(x_2) + \epsilon
    \end{align*} $$

    Since this holds for any $\epsilon > 0$, we have

    $$ g(\theta x_1 + \bar{\theta} x_2) \le \theta g(x_1) + \bar{\theta} g(x_2)$$

    The result can also be shown by epigraph. Assuming the infimum is attained for each $x$, we have

    $$ \mathbf{epi}(g) = \{ (x,t) | (x,y,t) \in \mathbf{epi}(f) \text{ for some } y \in C \} $$

    Thus $\mathbf{epi}(g)$ is convex, as it is the projection of a convex set on some of its coordinates.

!!!example Distance to a set
    The distance of a point $x$ to a set $S$ in norm $\|\cdot\|$ is defined as

    $$ \mathbf{dist}(x, S) = \inf_{y \in S}\| x - y \| $$

    which is convex, if the set $S$ is convex.

### Scalar Composition

- Let $g: \mathbb{R}^n \mapsto \mathbb{R}$, $h: \mathbb{R} \mapsto \mathbb{R}$, $f(x) = h(g(x))$.
  - $f$ is convex
    - if $g$ is convex, $h$ is convex, $h$ is nondecreasing.
    - if $g$ is concave, $h$ is convex, $h$ is nonincreasing.
  - $f$ is concave
    - if $g$ is concave, $h$ is concave, $h$ is nondecreasing.
    - if $g$ is convex, $h$ is concave, $h$ is nonincreasing.
  - Note that $h$ can be extended with its extended value extensions.

As some examples

- $\exp(g(x))$ is convex if $g(x)$ is convex.
- $1/g(x)$ is convex if $g(x)$ is concave (assuming $g(x) > 0$).

### Perpective of a Function

If $f: \mathbb{R}^n \mapsto \mathbb{R}$, then the **perspective** of $f$, $g: \mathbb{R}^{n+1} \mapsto \mathbb{R}$ is defined by

$$g(x, t) = t f(x/t)$$

with $\mathbf{dom}(g) = \{ (x,t) | x/t \in \mathbf{dom}(f), t>0 \}$.

- The perspective operation preserves convexity
  - If $f$ is convex, so is $g$
  - If $f$ is concave, so is $g$

## The Conjugate Function

Let $f: \mathbb{R}^n \mapsto \mathbb{R}$, the function $f^*: \mathbb{R} \mapsto \mathbb{R}$, defined as

$$ f^*(y) = \sup_{x \in \mathbf{dom}f} (y^Tx - f(x)) $$

is called the **conjugate** of $f$.

- The conjugate function represents the intercept of the tangent of $f(x)$ at point $y$.
  - The conjugate function can be think of as a representation of $f$ in terms of "intercepts".
  - To see this, assume $f$ is differentiable, the supremum is attained when $y = f'(x)$.
  - Therefore $f^*(y) = f'(x)^Tx - f(x)$, and $f^*(y)$ is the gap between the tangent line $y^Tx$ and the original function $f(x)$ at value $x$.
- $f^*(x)$ is convex, because it is the pointwise supremum of a function of convex (affine) functions (of $y$).
  - This holds regardless of whether $f$ itself is convex.

### Examples

!!!example Negative Logarithm
    Let $f(x) = -\log(x)$.

    $$ f^*(y) = \sup_x (y^Tx - f(x)) = \sup_x (y^Tx + \log(x)) $$

    - If $y \ge 0$, the function is unbounded above, and $f^*(y) = \infty$.
    - Otherwise it reaches its maximum at $x = -1/y$ and therefore $f^*(y) = -1 - \log(-y)$.

!!!example Strictly Convex Quadratic Function
    Let $f(x) = -\frac{1}{2}x^TQx$, with $Q \in \mathbb{S}^n_{++}$.

    $$ f^*(y) = \sup_x \left( y^Tx - \frac{1}{2}x^TQx \right)$$

    is bounded above for all $y$, and it attains its maximum at $x=Q^{-1}y$.

    Therefore $f^*(y) = \frac{1}{2} y^TQ^{-1}y$.

!!!example Norm
    Let $\|\cdot\|$ be a norm on $\mathbb{R}^n$ with dual norm $\|\cdot\|_*$. We will show that the conjugate of $f(x) = \|x\|$ is

    $$f^*(y) = \begin{cases}
      0 &\quad \|y\|_*\le 1\\
      \infty &\quad o.w.
    \end{cases} $$
    
    i.e., the conjugate of a norm is the indicator function of the dual norm unit ball.

    If $\|y\|_* \ge 1$, then by definition of the dual norm $\|y\|_* = \sup_{\|z\|\le 1} (y^Tz)$, there is a $z$ with $\|z\| \le 1$ such that $z^Ty \ge 1$. Then taking $x = tz$ and letting $t \to \infty$, we have

    $$ y^Tx - \|x\| = t(y^Tz - \|z\|) \to \infty $$

    If $\|y\|_* \le 1$, then by definition of dual norm, we have $\|y\|_* = z^Ty \le 1$ subject to $\|z\| \le 1$ for some $z$. Let $z = x/\|x\|$ and multiply $\|x\|$ on both sides, we have $y^Tx \le \|x\|\|y\|_*$ for all $x$. This implies $y^Tx - \|x\| \le 0$ for all $x$, and the equality is achieved when $x=0$. Therefore $x=0$ is the value that maximizes $f^*(y) = y^Tx - \|x\|$.

### Properties

#### Conjugate of the conjugate

If $f$ is convex and $f$ is closed (i.e., $\mathbf{epi}f$ is a closed set), then $f^{**} = f$.

Too see this,

$$ f^*(y) = \sup_{x} (y^Tx - f(x)) = y^Tx - f(x) \quad \text{if } y = f'(x) $$

$$ f^{**}(v) = \sup_y (v^Ty - f^*(y)) = v^Ty - f^*(y) \quad \text{if } v = {f^*}'(y) $$

Note that

$$ v = \frac{\partial f^*(y)}{\partial y} = x + y^T\frac{\partial x}{\partial y} - \frac{\partial f(x)}{\partial x} \frac{\partial x}{\partial y} = x + y^T\frac{\partial x}{\partial y} - y^T\frac{\partial x}{\partial y} = x $$

Therefore

$$ f^{**}(v) = x^Ty - f^*(y) \Longrightarrow f^*(y) = y^Tx - f^{**}(x) \tag{1}$$

Using (1) and $f^*(y) = y^Tx - f(x)$, we have $f(x) = f^{**}(x)$.

## *Quasiconvex Functions

A function $f: \mathbb{R}^n \mapsto \mathbb{R}$ is **quasiconvex** (or **unimodal**) if its domain and all its sublevel sets

$$ S_{\alpha} = \{ x \in \mathbf{dom} f | f(x) \le \alpha \} $$

for $\alpha \in \mathbb{R}$ are convex.

-  A function $f$ is **quasiconcave** if $-f$ is quasiconvex.
- A function $f$ is **quasilinear** if it is both quasiconvex and quasiconcave.
